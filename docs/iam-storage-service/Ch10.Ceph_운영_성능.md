# Ch10. Ceph ìš´ì˜ & ì„±ëŠ¥

## ğŸ“‹ ê°œìš” ë° í•™ìŠµ ëª©í‘œ

### ê°œìš”

Ceph í´ëŸ¬ìŠ¤í„°ë¥¼ **í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜**í•˜ê¸° ìœ„í•´ì„œëŠ” ì²´ê³„ì ì¸ ëª¨ë‹ˆí„°ë§, ì„±ëŠ¥ ìµœì í™”, ì¥ì•  ëŒ€ì‘ ëŠ¥ë ¥ì´ í•„ìˆ˜ì…ë‹ˆë‹¤. ì´ ì±•í„°ì—ì„œëŠ” **cephadm/Rookì„ í†µí•œ í´ëŸ¬ìŠ¤í„° ë°°í¬**, **Prometheus/Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ**, **FIO/rados benchë¥¼ í™œìš©í•œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**, **BlueStore íŠœë‹**, **OSD ì¥ì•  ì‹œ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜**, **ì¼ë°˜ì ì¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, CephëŠ” **cephadm orchestrator**ë¥¼ ê¸°ë³¸ ë°°í¬ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ë©°, Kubernetes í™˜ê²½ì—ì„œëŠ” **Rook operator**ë¥¼ í†µí•´ Cloud Native ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤. **mClock ìŠ¤ì¼€ì¤„ëŸ¬**ë¥¼ í†µí•œ ë³µêµ¬/ë°±í•„ ì‘ì—… ìŠ¤ë¡œí‹€ë§, **BlueStore ìµœì í™”**ë¡œ 20% ì„±ëŠ¥ í–¥ìƒ, **Prometheus ë„¤ì´í‹°ë¸Œ ë©”íŠ¸ë¦­** ì§€ì› ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ìš´ì˜ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### í•™ìŠµ ëª©í‘œ

ì´ ì±•í„°ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **í´ëŸ¬ìŠ¤í„° ë°°í¬**: cephadm/Rookì„ ì‚¬ìš©í•œ í”„ë¡œë•ì…˜ í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
2. **ëª¨ë‹ˆí„°ë§ êµ¬ì„±**: Prometheus + Grafana + Alertmanager ìŠ¤íƒ ë°°í¬ ë° ìš´ì˜
3. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**: FIO, rados benchë¡œ í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ì¸¡ì • ë° ë¶„ì„
4. **ì„±ëŠ¥ íŠœë‹**: BlueStore, ë„¤íŠ¸ì›Œí¬, PG ìˆ˜ ìµœì í™”
5. **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**: PG ìƒíƒœ ì´í•´, OSD ì¥ì•  ë³µêµ¬, ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

---

## ğŸ”‘ í•µì‹¬ ê°œë… ë° ì´ë¡ 

### 1. Ceph ë°°í¬ ë°©ë²• (2025)

#### 1.1 cephadm (ê¸°ë³¸ ë°°í¬ ë„êµ¬)

**cephadm**ì€ Ceph Octopus (15.x) ì´í›„ **ê³µì‹ ê¸°ë³¸ ë°°í¬ ë„êµ¬**ë¡œ, ëª¨ë“  Ceph ë°ëª¬ì„ **ì»¨í…Œì´ë„ˆ**ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "cephadm Orchestrator"
        CLI[ceph CLI / Dashboard]
        Orchestrator[Ceph Orchestrator Module]

        subgraph "ê° í˜¸ìŠ¤íŠ¸"
            Cephadm[cephadm Agent]

            subgraph "Containers (Podman/Docker)"
                MONContainer[MON Container]
                OSDContainer[OSD Container]
                MGRContainer[MGR Container]
            end
        end
    end

    CLI --> Orchestrator
    Orchestrator -.SSH.-> Cephadm
    Cephadm --> MONContainer
    Cephadm --> OSDContainer
    Cephadm --> MGRContainer
```

**íŠ¹ì§•**:

- **ì»¨í…Œì´ë„ˆ ê¸°ë°˜**: Podman(ê¸°ë³¸) ë˜ëŠ” Dockerë¡œ ëª¨ë“  ë°ëª¬ ì‹¤í–‰
- **ì„ ì–¸ì  ê´€ë¦¬**: `ceph orch apply` ëª…ë ¹ìœ¼ë¡œ desired state ì •ì˜
- **ìë™ ì—…ê·¸ë ˆì´ë“œ**: `ceph orch upgrade` ëª…ë ¹ìœ¼ë¡œ rolling upgrade
- **SSH ê¸°ë°˜**: cephadmì´ SSHë¡œ ê° ë…¸ë“œì— ì ‘ì†í•˜ì—¬ ì»¨í…Œì´ë„ˆ ê´€ë¦¬

**ì¥ì **:

- ìš´ì˜ ì²´ì œ íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ì—†ìŒ (ì»¨í…Œì´ë„ˆì— ëª¨ë“  ì˜ì¡´ì„± í¬í•¨)
- ê°„ë‹¨í•œ ë°°í¬ ë° ì—…ê·¸ë ˆì´ë“œ
- Bare-metal, VM í™˜ê²½ ëª¨ë‘ ì§€ì›

#### 1.2 Rook (Kubernetes Operator)

**Rook**ì€ Kubernetes í™˜ê²½ì—ì„œ Cephë¥¼ **ë„¤ì´í‹°ë¸Œ ë°©ì‹**ìœ¼ë¡œ ë°°í¬/ê´€ë¦¬í•˜ëŠ” operatorì…ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "Kubernetes Cluster"
        RookOperator[Rook Operator<br/>CephCluster CRD ê´€ë¦¬]

        subgraph "Namespace: rook-ceph"
            MONPod[MON Pod]
            OSDPod[OSD Pod]
            MGRPod[MGR Pod]
            MDSPod[MDS Pod]
            RGWPod[RGW Pod]
        end

        subgraph "Monitoring"
            PrometheusOp[Prometheus Operator]
            ServiceMonitor[ServiceMonitor CRD]
        end
    end

    RookOperator --> MONPod
    RookOperator --> OSDPod
    RookOperator --> MGRPod

    PrometheusOp --> ServiceMonitor
    ServiceMonitor -.scrape.-> MGRPod
```

**CephCluster CRD ì˜ˆì‹œ**:

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.0  # Reef
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sd[b-d]"  # sdb, sdc, sddë§Œ ì‚¬ìš©
  dashboard:
    enabled: true
  monitoring:
    enabled: true  # Prometheus ServiceMonitor ìë™ ìƒì„±
```

**ì¥ì **:

- Kubernetes ë„¤ì´í‹°ë¸Œ (kubectlë¡œ ê´€ë¦¬)
- ìë™ ëª¨ë‹ˆí„°ë§ í†µí•© (Prometheus Operator)
- Persistent Volume í”„ë¡œë¹„ì €ë‹ (StorageClass, CSI)
- GitOps ì¹œí™”ì  (ArgoCD, Flux)

### 2. ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

#### 2.1 ì•„í‚¤í…ì²˜

CephëŠ” **Prometheus ë„¤ì´í‹°ë¸Œ ë©”íŠ¸ë¦­**ì„ ì œê³µí•©ë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "Ceph Cluster"
        MGR[MGR Daemon<br/>Prometheus Module]
        NodeExporter[Node Exporter<br/>ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­]
        CephExporter[Ceph Exporter<br/>ìƒì„¸ ì„±ëŠ¥ ì¹´ìš´í„°]
    end

    subgraph "Monitoring Stack"
        Prometheus[Prometheus<br/>ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì €ì¥]
        Alertmanager[Alertmanager<br/>ì•Œë¦¼ ë¼ìš°íŒ…]
        Grafana[Grafana<br/>ì‹œê°í™”]
    end

    MGR -.9283:metrics.-> Prometheus
    NodeExporter -.9100.-> Prometheus
    CephExporter -.9926.-> Prometheus

    Prometheus --> Alertmanager
    Prometheus --> Grafana
```

**ì£¼ìš” ë©”íŠ¸ë¦­**:

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì˜ˆì‹œ |
|--------|------|------|
| **ceph_health_status** | í´ëŸ¬ìŠ¤í„° ìƒíƒœ (0=OK, 1=WARN, 2=ERR) | `ceph_health_status 0` |
| **ceph_osd_up** | OSD UP ìƒíƒœ (0=down, 1=up) | `ceph_osd_up{osd="0"} 1` |
| **ceph_osd_in** | OSD IN ìƒíƒœ (0=out, 1=in) | `ceph_osd_in{osd="0"} 1` |
| **ceph_pool_bytes_used** | Pool ì‚¬ìš©ëŸ‰ (bytes) | `ceph_pool_bytes_used{pool="rbd"} 1073741824` |
| **ceph_pool_objects** | Pool ê°ì²´ ìˆ˜ | `ceph_pool_objects{pool="rbd"} 100` |
| **ceph_osd_op_r_latency_sum** | OSD ì½ê¸° ë ˆì´í„´ì‹œ í•©ê³„ (ms) | `ceph_osd_op_r_latency_sum{osd="0"} 1234.56` |
| **ceph_osd_op_r_latency_count** | OSD ì½ê¸° ì‘ì—… ìˆ˜ | `ceph_osd_op_r_latency_count{osd="0"} 1000` |
| **ceph_osd_op_w_latency_sum** | OSD ì“°ê¸° ë ˆì´í„´ì‹œ í•©ê³„ (ms) | `ceph_osd_op_w_latency_sum{osd="0"} 5678.90` |

#### 2.2 ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬ (cephadm)

```bash
# Monitoring stack í™œì„±í™” (ê¸°ë³¸ê°’: í™œì„±í™”)
sudo ceph orch apply prometheus --placement="ceph1"
sudo ceph orch apply grafana --placement="ceph1"
sudo ceph orch apply alertmanager --placement="ceph1"
sudo ceph orch apply node-exporter --placement="*"  # ëª¨ë“  ë…¸ë“œ

# Prometheus Module í™œì„±í™”
sudo ceph mgr module enable prometheus

# Grafana ì ‘ì† ì •ë³´
sudo ceph dashboard ac-user-show admin
# Grafana: https://ceph1:3000 (admin / <password>)
```

**Rookì—ì„œëŠ” ìë™ ë°°í¬**:

- `monitoring.enabled: true` ì„¤ì • ì‹œ ServiceMonitor ìë™ ìƒì„±
- Prometheus Operatorê°€ ë©”íŠ¸ë¦­ ìë™ ìˆ˜ì§‘

### 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

#### 3.1 ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

| ë„êµ¬ | ìš©ë„ | ì¸¡ì • ëŒ€ìƒ |
|------|------|----------|
| **rados bench** | RADOS ë ˆì´ì–´ í…ŒìŠ¤íŠ¸ | Pool ì„±ëŠ¥ (ê°ì²´ ì“°ê¸°/ì½ê¸°) |
| **FIO** | ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ í…ŒìŠ¤íŠ¸ | RBD ì„±ëŠ¥ (IOPS, ëŒ€ì—­í­, ë ˆì´í„´ì‹œ) |
| **rbd bench** | RBD ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ | RBD ì´ë¯¸ì§€ ì„±ëŠ¥ |
| **COSBench/GOSBench** | ê°ì²´ ìŠ¤í† ë¦¬ì§€ í…ŒìŠ¤íŠ¸ | RGW S3 API ì„±ëŠ¥ |

#### 3.2 rados bench ì‚¬ìš©ë²•

**ìˆœì°¨ ì“°ê¸° í…ŒìŠ¤íŠ¸**:

```bash
# 10ì´ˆ ë™ì•ˆ 4MB ê°ì²´ ì“°ê¸° (í´ëŸ¬ìŠ¤í„° ë¶€í•˜ í™•ì¸ìš©)
sudo rados bench -p test-pool 10 write --no-cleanup

# ì¶œë ¥ ì˜ˆì‹œ:
# Total time run:         10.5 sec
# Total writes made:      250
# Write size:             4194304
# Object size:            4194304
# Bandwidth (MB/sec):     95.2
# Average IOPS:           23.8
# Average Latency(s):     0.42
```

**ìˆœì°¨ ì½ê¸° í…ŒìŠ¤íŠ¸**:

```bash
# --no-cleanupìœ¼ë¡œ ë‚¨ê²¨ì§„ ê°ì²´ ì½ê¸°
sudo rados bench -p test-pool 10 seq

# ì¶œë ¥ ì˜ˆì‹œ:
# Total time run:         5.2 sec
# Total reads made:       250
# Read size:              4194304
# Bandwidth (MB/sec):     192.3
# Average IOPS:           48.1
# Average Latency(s):     0.21
```

**ëœë¤ ì½ê¸° í…ŒìŠ¤íŠ¸**:

```bash
sudo rados bench -p test-pool 10 rand
```

#### 3.3 FIO ë²¤ì¹˜ë§ˆí‚¹ (RBD)

**4K ëœë¤ ì“°ê¸° (IOPS ì¸¡ì •)**:

```bash
fio --name=rbd-randwrite \
    --rw=randwrite \
    --direct=1 \
    --ioengine=libaio \
    --bs=4k \
    --iodepth=32 \
    --numjobs=4 \
    --size=5G \
    --runtime=60 \
    --group_reporting=1 \
    --filename=/dev/rbd0
```

**128K ìˆœì°¨ ì½ê¸° (ëŒ€ì—­í­ ì¸¡ì •)**:

```bash
fio --name=rbd-seqread \
    --rw=read \
    --direct=1 \
    --ioengine=libaio \
    --bs=128k \
    --iodepth=64 \
    --numjobs=4 \
    --size=10G \
    --runtime=60 \
    --group_reporting=1 \
    --filename=/dev/rbd0
```

**ë¸”ë¡ í¬ê¸°ë³„ í…ŒìŠ¤íŠ¸**:

```bash
# 4k, 8k, 16k, 32k, 64k, 128k, 256k, 512k, 1m, 4m
for bs in 4k 8k 16k 32k 64k 128k 256k 512k 1m 4m; do
    fio --name=rbd-test-$bs --rw=randwrite --bs=$bs --iodepth=32 \
        --numjobs=4 --size=1G --runtime=30 --filename=/dev/rbd0
done
```

### 4. ì„±ëŠ¥ íŠœë‹

#### 4.1 BlueStore íŠœë‹

**ë©”ëª¨ë¦¬ ì„¤ì •** (ê°€ì¥ ì¤‘ìš”):

```ini
# ceph.conf ë˜ëŠ” ëŸ°íƒ€ì„ ì„¤ì •
[osd]
osd_memory_target = 8589934592  # 8GB (ìµœì†Œê°’, 16GB ê¶Œì¥)
```

**RocksDB íŠœë‹**:

```ini
[osd]
bluestore_rocksdb_options = "compression=kNoCompression,max_open_files=1024"
```

**ë°ì´í„°ë² ì´ìŠ¤/WAL ë¶„ë¦¬** (NVMe ì‚¬ìš© ì‹œ):

```bash
# OSD ìƒì„± ì‹œ DB/WAL ë””ë°”ì´ìŠ¤ ì§€ì •
sudo ceph orch daemon add osd ceph1:\
    --data-devices=/dev/sdb \
    --db-devices=/dev/nvme0n1 \
    --wal-devices=/dev/nvme0n1
```

**íš¨ê³¼**:

- ë©”íƒ€ë°ì´í„°ë¥¼ NVMeì— ë°°ì¹˜ â†’ **ë ˆì´í„´ì‹œ 50% ê°ì†Œ**
- HDD ë°ì´í„° + NVMe ë©”íƒ€ë°ì´í„° ì¡°í•©ìœ¼ë¡œ ë¹„ìš© ìµœì í™”

#### 4.2 ë„¤íŠ¸ì›Œí¬ ìµœì í™”

**10GbE ì´ìƒ ê¶Œì¥**:

```bash
# Jumbo Frame í™œì„±í™” (MTU 9000)
sudo ip link set eth0 mtu 9000

# /etc/sysctl.conf
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_congestion_control = bbr  # BBR congestion control
```

**Public/Cluster ë„¤íŠ¸ì›Œí¬ ë¶„ë¦¬**:

```yaml
# ceph.conf
[global]
public_network = 192.168.1.0/24   # í´ë¼ì´ì–¸íŠ¸ íŠ¸ë˜í”½
cluster_network = 10.0.0.0/24     # OSD ê°„ ë³µì œ íŠ¸ë˜í”½
```

**íš¨ê³¼**:

- ë³µì œ íŠ¸ë˜í”½ê³¼ í´ë¼ì´ì–¸íŠ¸ íŠ¸ë˜í”½ ê²©ë¦¬ â†’ **ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ì œê±°**

#### 4.3 PG ìˆ˜ ìµœì í™”

**ì ì ˆí•œ PG ìˆ˜ ê³„ì‚°** (Ch9 ì°¸ì¡°):

```
PG ìˆ˜ = (Total_OSDs Ã— mon_target_pg_per_osd) / pool_size

ì˜ˆì‹œ: 30 OSD, size=3, mon_target_pg_per_osd=200
PG ìˆ˜ = (30 Ã— 200) / 3 = 2000 â†’ 2048 (2ì˜ ê±°ë“­ì œê³±)
```

**PG ìˆ˜ ì¡°ì •** (ì£¼ì˜: ë¶€í•˜ ë°œìƒ):

```bash
# PG ìˆ˜ ì¦ê°€
sudo ceph osd pool set my-pool pg_num 2048
sudo ceph osd pool set my-pool pgp_num 2048

# PG Autoscaler í™œì„±í™” (ìë™ ì¡°ì •)
sudo ceph osd pool set my-pool pg_autoscale_mode on
```

### 5. OSD ì¥ì•  ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

#### 5.1 OSD ì¥ì•  ì‹œ íë¦„

```mermaid
sequenceDiagram
    participant OSD1 as OSD 1 (Primary)
    participant OSD2 as OSD 2 (Replica)
    participant MON
    participant Client

    OSD2->>MON: Heartbeat ëˆ„ë½ (10ì´ˆ ì´ìƒ)
    MON->>MON: OSD 2ë¥¼ downìœ¼ë¡œ í‘œì‹œ
    MON-->>Client: OSD Map ì—…ë°ì´íŠ¸ (osd.2 down)

    Note over OSD2: OSD 2ê°€ 300ì´ˆ(mon_osd_down_out_interval) ë™ì•ˆ down

    MON->>MON: OSD 2ë¥¼ outìœ¼ë¡œ í‘œì‹œ
    MON-->>OSD1: PG ì¬ë§¤í•‘ (osd.2 â†’ osd.3)

    OSD1->>OSD1: Recovery ì‹œì‘ (degraded â†’ active+clean)
    OSD1->>OSD3: ë°ì´í„° ë³µì œ

    Note over OSD1,OSD3: Recovery ì™„ë£Œ
    OSD1-->>MON: PG active+clean
```

**ì£¼ìš” íƒ€ì´ë¨¸**:

- **mon_osd_heartbeat_grace**: 10ì´ˆ (OSDë¥¼ downìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì‹œê°„)
- **mon_osd_down_out_interval**: 300ì´ˆ (OSDë¥¼ outìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì‹œê°„, ë³µêµ¬ ì‹œì‘)
- **osd_recovery_max_active**: 3 (OSDë‹¹ ë™ì‹œ ë³µêµ¬ PG ìˆ˜)

#### 5.2 mClock ìŠ¤ë¡œí‹€ë§ (2025)

**mClock ìŠ¤ì¼€ì¤„ëŸ¬**ëŠ” ë³µêµ¬/ë°±í•„ ì‘ì—…ì„ **í´ë¼ì´ì–¸íŠ¸ I/Oë³´ë‹¤ ë‚®ì€ ìš°ì„ ìˆœìœ„**ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤:

```bash
# Recovery ìŠ¤ë¡œí‹€ë§ ì„¤ì •
sudo ceph config set osd osd_recovery_max_active 3
sudo ceph config set osd osd_recovery_max_single_start 1
sudo ceph config set osd osd_max_backfills 1

# mClock í”„ë¡œíŒŒì¼ (high_client_ops: í´ë¼ì´ì–¸íŠ¸ ìš°ì„ )
sudo ceph config set osd osd_mclock_profile high_client_ops
```

**íš¨ê³¼**:

- ë³µêµ¬ ì¤‘ì—ë„ **í´ë¼ì´ì–¸íŠ¸ I/O ì„±ëŠ¥ ìœ ì§€**
- ë³µêµ¬ ì‹œê°„ì€ ê¸¸ì–´ì§€ì§€ë§Œ ì„œë¹„ìŠ¤ ì˜í–¥ ìµœì†Œí™”

### 6. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### 6.1 PG ìƒíƒœ í•´ì„

| ìƒíƒœ | ì˜ë¯¸ | ì¡°ì¹˜ |
|------|------|------|
| **active+clean** | ì •ìƒ | ì—†ìŒ |
| **peering** | OSD ê°„ ë™ê¸°í™” ì¤‘ | ëŒ€ê¸° (ìˆ˜ ë¶„ ì´ë‚´ í•´ê²°) |
| **degraded** | ë³µì œë³¸ ë¶€ì¡± | OSD ë³µêµ¬ ëŒ€ê¸° |
| **recovering** | ë³µêµ¬ ì§„í–‰ ì¤‘ | ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ |
| **backfilling** | PG ë°ì´í„° ì´ë™ ì¤‘ | ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ |
| **backfill_toofull** | OSD ìš©ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ ë°±í•„ ë¶ˆê°€ | OSD ì¶”ê°€ ë˜ëŠ” ë°ì´í„° ì‚­ì œ |
| **incomplete** | PG ë³µêµ¬ ë¶ˆê°€ (ë°ì´í„° ì†ì‹¤) | **ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥**, ë¡œê·¸ í™•ì¸ |
| **stale** | MONì´ PG ìƒíƒœë¥¼ ë°›ì§€ ëª»í•¨ | OSD ì¬ì‹œì‘ |

#### 6.2 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°

**ë¬¸ì œ 1: OSD Full**

```bash
# ì¦ìƒ
$ sudo ceph health
HEALTH_ERR 1 full osd(s)

# ì›ì¸
$ sudo ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META   AVAIL    %USE
 0    ssd  1.00000   1.00000  100 GiB   96 GiB  90 GiB   0 B  6 GiB  4.0 GiB  96.00  # FULL!

# í•´ê²°
# 1. ì„ì‹œ: full ratio ì¦ê°€ (ë¹„ì¶”ì²œ)
sudo ceph osd set-full-ratio 0.97

# 2. ê·¼ë³¸ì : OSD ì¶”ê°€ ë˜ëŠ” ë°ì´í„° ì‚­ì œ
sudo ceph orch daemon add osd ceph4:/dev/sdb
```

**ë¬¸ì œ 2: Slow Ops**

```bash
# ì¦ìƒ
$ sudo ceph health detail
HEALTH_WARN 10 slow ops, oldest one blocked for 30 sec

# ì›ì¸ í™•ì¸
$ sudo ceph daemon osd.0 dump_historic_ops

# ì¼ë°˜ì ì¸ ì›ì¸:
# - ë””ìŠ¤í¬ ì„±ëŠ¥ ì €í•˜ (SMART ê²€ì‚¬)
# - ë„¤íŠ¸ì›Œí¬ ì§€ì—°
# - CPU/ë©”ëª¨ë¦¬ ë¶€ì¡±

# í•´ê²°
# SMART ê²€ì‚¬
sudo smartctl -a /dev/sdb

# OSD í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart ceph-osd@0
```

**ë¬¸ì œ 3: Clock Skew**

```bash
# ì¦ìƒ
HEALTH_WARN clock skew detected on mon.ceph2, mon.ceph3

# ì›ì¸: NTP ë™ê¸°í™” ì‹¤íŒ¨

# í•´ê²°
sudo systemctl restart chronyd  # ë˜ëŠ” ntpd
sudo chronyc sources
```

---

## ğŸ’» ì‹¤ìŠµ ê°€ì´ë“œ (Hands-on)

### Lab 1: Monitoring Stack ë°°í¬ (Prometheus + Grafana)

**ëª©í‘œ**: cephadmìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬ ë° Grafana ëŒ€ì‹œë³´ë“œ í™•ì¸

**ë‹¨ê³„**:

1. **Monitoring Stack ë°°í¬**:

```bash
# Prometheus ë°°í¬
sudo ceph orch apply prometheus --placement="ceph1"

# Grafana ë°°í¬
sudo ceph orch apply grafana --placement="ceph1"

# Alertmanager ë°°í¬
sudo ceph orch apply alertmanager --placement="ceph1"

# Node Exporter (ëª¨ë“  ë…¸ë“œ)
sudo ceph orch apply node-exporter --placement="*"

# ë°°í¬ í™•ì¸
sudo ceph orch ls
```

2. **Prometheus Module í™œì„±í™”**:

```bash
# Prometheus Module í™œì„±í™”
sudo ceph mgr module enable prometheus

# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
curl http://ceph1:9283/metrics | grep ceph_health_status
```

3. **Grafana ì ‘ì†**:

```bash
# Grafana ì´ˆê¸° ë¹„ë°€ë²ˆí˜¸ í™•ì¸
sudo ceph dashboard ac-user-show admin

# ë¸Œë¼ìš°ì €ë¡œ ì ‘ì†: https://ceph1:3000
# ë¡œê·¸ì¸: admin / <password>

# ëŒ€ì‹œë³´ë“œ í™•ì¸
# - Ceph Cluster
# - OSD Overview
# - Pool Overview
# - RBD Overview
```

4. **ì»¤ìŠ¤í…€ ì¿¼ë¦¬**:

Prometheusì—ì„œ PromQLë¡œ ë©”íŠ¸ë¦­ ì¡°íšŒ:

```promql
# í´ëŸ¬ìŠ¤í„° ì „ì²´ IOPS
sum(rate(ceph_osd_op_r_latency_count[5m])) + sum(rate(ceph_osd_op_w_latency_count[5m]))

# OSDë³„ í‰ê·  ì“°ê¸° ë ˆì´í„´ì‹œ (ms)
rate(ceph_osd_op_w_latency_sum[5m]) / rate(ceph_osd_op_w_latency_count[5m])

# Pool ì‚¬ìš©ë¥  (%)
(ceph_pool_bytes_used / ceph_pool_max_avail) * 100
```

### Lab 2: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ (FIO, rados bench)

**ëª©í‘œ**: í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ì¸¡ì • ë° ê²°ê³¼ ë¶„ì„

**ë‹¨ê³„**:

1. **rados bench ì‹¤í–‰**:

```bash
# í…ŒìŠ¤íŠ¸ Pool ìƒì„±
sudo ceph osd pool create bench-pool 128 replicated
sudo ceph osd pool application enable bench-pool rbd

# ìºì‹œ í´ë¦¬ì–´ (ì •í™•í•œ ì¸¡ì •)
echo 3 | sudo tee /proc/sys/vm/drop_caches && sudo sync

# ìˆœì°¨ ì“°ê¸° í…ŒìŠ¤íŠ¸ (60ì´ˆ)
sudo rados bench -p bench-pool 60 write --no-cleanup

# ìˆœì°¨ ì½ê¸° í…ŒìŠ¤íŠ¸
sudo rados bench -p bench-pool 60 seq

# ëœë¤ ì½ê¸° í…ŒìŠ¤íŠ¸
sudo rados bench -p bench-pool 60 rand

# ì •ë¦¬
sudo rados -p bench-pool cleanup
```

2. **RBD + FIO í…ŒìŠ¤íŠ¸**:

```bash
# RBD ì´ë¯¸ì§€ ìƒì„±
sudo rbd create --size 50G bench-pool/fio-test

# RBD ë§µí•‘
sudo rbd map bench-pool/fio-test
# /dev/rbd0

# 4K ëœë¤ ì“°ê¸° (IOPS)
fio --name=4k-randwrite --ioengine=libaio --direct=1 \
    --rw=randwrite --bs=4k --iodepth=32 --numjobs=4 \
    --size=10G --runtime=60 --group_reporting=1 \
    --filename=/dev/rbd0

# 128K ìˆœì°¨ ì½ê¸° (ëŒ€ì—­í­)
fio --name=128k-seqread --ioengine=libaio --direct=1 \
    --rw=read --bs=128k --iodepth=64 --numjobs=4 \
    --size=10G --runtime=60 --group_reporting=1 \
    --filename=/dev/rbd0

# Mixed workload (70% read, 30% write)
fio --name=mixed-workload --ioengine=libaio --direct=1 \
    --rw=randrw --rwmixread=70 --bs=8k --iodepth=32 --numjobs=4 \
    --size=10G --runtime=60 --group_reporting=1 \
    --filename=/dev/rbd0
```

3. **ê²°ê³¼ ë¶„ì„**:

FIO ì¶œë ¥ ì˜ˆì‹œ:
```
IOPS=12345, BW=48.2MiB/s (50.5MB/s)
lat (usec): min=100, max=5000, avg=1042.35, stdev=234.56
```

**í•´ì„**:

- **IOPS**: ì´ˆë‹¹ I/O ì‘ì—… ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **BW (Bandwidth)**: ëŒ€ì—­í­ (MiB/s ë˜ëŠ” MB/s)
- **lat (Latency)**: ë ˆì´í„´ì‹œ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, avg/stdev í™•ì¸)

### Lab 3: BlueStore íŠœë‹

**ëª©í‘œ**: BlueStore íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

**ë‹¨ê³„**:

1. **í˜„ì¬ ì„¤ì • í™•ì¸**:

```bash
# OSD ë©”ëª¨ë¦¬ íƒ€ê²Ÿ í™•ì¸
sudo ceph config show osd.0 | grep osd_memory_target

# BlueStore ì„¤ì • í™•ì¸
sudo ceph config show osd.0 | grep bluestore
```

2. **ë©”ëª¨ë¦¬ íƒ€ê²Ÿ ì¦ê°€** (ê°€ì¥ í° ì˜í–¥):

```bash
# ëª¨ë“  OSDì— 16GB ë©”ëª¨ë¦¬ í• ë‹¹ (ê¸°ë³¸ 4GB â†’ 16GB)
sudo ceph config set osd osd_memory_target 17179869184

# OSD ì¬ì‹œì‘ (rolling restart)
for i in 0 1 2; do
    sudo systemctl restart ceph-osd@$i
    sleep 60  # ë³µêµ¬ ëŒ€ê¸°
done
```

3. **ë²¤ì¹˜ë§ˆí¬ ë¹„êµ**:

```bash
# íŠœë‹ ì „ ë²¤ì¹˜ë§ˆí¬
fio --name=before-tuning --rw=randwrite --bs=4k --iodepth=32 \
    --runtime=60 --filename=/dev/rbd0 | tee before.txt

# íŠœë‹ í›„ ë²¤ì¹˜ë§ˆí¬ (ë©”ëª¨ë¦¬ ì¦ê°€ í›„)
fio --name=after-tuning --rw=randwrite --bs=4k --iodepth=32 \
    --runtime=60 --filename=/dev/rbd0 | tee after.txt

# ê²°ê³¼ ë¹„êµ
grep "IOPS=" before.txt after.txt
```

4. **DB/WAL ë¶„ë¦¬** (NVMe ìˆëŠ” ê²½ìš°):

```bash
# ìƒˆ OSD ìƒì„± ì‹œ NVMeì— DB/WAL ë°°ì¹˜
sudo ceph orch daemon add osd ceph1: \
    --data-devices=/dev/sdb \
    --db-devices=/dev/nvme0n1 \
    --wal-devices=/dev/nvme0n1

# ê¸°ì¡´ OSDëŠ” ì¬ìƒì„± í•„ìš” (ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”)
```

### Lab 4: OSD ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ & ë³µêµ¬

**ëª©í‘œ**: OSD ì¥ì•  ì‹œ ìë™ ë³µêµ¬ ê³¼ì • ê´€ì°°

**ë‹¨ê³„**:

1. **ì´ˆê¸° ìƒíƒœ í™•ì¸**:

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ
sudo ceph -s

# PG ìƒíƒœ
sudo ceph pg stat
```

2. **OSD ì¥ì•  ì‹œë®¬ë ˆì´ì…˜**:

```bash
# OSD 2 ì¤‘ì§€
sudo systemctl stop ceph-osd@2

# ìƒíƒœ ë³€í™” ê´€ì°° (10ì´ˆ í›„ down, 300ì´ˆ í›„ out)
watch -n 5 'sudo ceph osd tree'

# PG ìƒíƒœ ë³€í™”
watch -n 5 'sudo ceph pg stat'
# ì¶œë ¥: XX active+clean, YY active+degraded
```

3. **ë³µêµ¬ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§**:

```bash
# Recovery ì§„í–‰ë¥ 
sudo ceph -w
# ì¶œë ¥ ì˜ˆì‹œ:
# recovery: 1024/3072 objects degraded (33.3%)
# 10 MiB/s, 1000 objects/s

# PGë³„ ìƒíƒœ
sudo ceph pg dump | grep recovering
```

4. **OSD ë³µêµ¬**:

```bash
# OSD ì¬ì‹œì‘
sudo systemctl start ceph-osd@2

# ë³µêµ¬ ì™„ë£Œ ëŒ€ê¸°
sudo ceph -w
# ëª¨ë“  PGê°€ active+cleanì´ ë  ë•Œê¹Œì§€ ëŒ€ê¸°

# ìµœì¢… ìƒíƒœ í™•ì¸
sudo ceph -s
```

### Lab 5: ì¼ë°˜ì ì¸ ë¬¸ì œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ëª©í‘œ**: ì‹¤ì œ ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤ í•´ê²° ì‹¤ìŠµ

**ì‹œë‚˜ë¦¬ì˜¤ 1: PG Stuck in Peering**

```bash
# PG ìƒíƒœ í™•ì¸
sudo ceph pg dump pgs | grep peering

# PG ì¿¼ë¦¬ (ìƒì„¸ ì •ë³´)
sudo ceph pg 1.a query | jq '.state'

# í•´ê²°: OSD ì¬ì‹œì‘
sudo systemctl restart ceph-osd@<primary_osd>

# ì—¬ì „íˆ í•´ê²° ì•ˆ ë˜ë©´: MON ì¬ì‹œì‘
sudo systemctl restart ceph-mon@ceph1
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: Slow Ops**

```bash
# Slow ops í™•ì¸
sudo ceph health detail
# HEALTH_WARN 5 slow ops

# ì›ì¸ ë¶„ì„
sudo ceph daemon osd.0 dump_ops_in_flight
sudo ceph daemon osd.0 dump_historic_ops | jq '.ops[] | select(.duration > 30)'

# ë””ìŠ¤í¬ ì„±ëŠ¥ í™•ì¸
sudo smartctl -a /dev/sdb | grep -i error
sudo iostat -x 1 10  # I/O ì§€ì—° í™•ì¸

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
ping -c 10 ceph2
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ìš©ëŸ‰ ë¶€ì¡±**

```bash
# ìš©ëŸ‰ í™•ì¸
sudo ceph df

# ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” Pool í™•ì¸
sudo ceph osd pool ls detail | grep size

# í•´ê²° 1: ë¶ˆí•„ìš”í•œ ë°ì´í„° ì‚­ì œ
sudo rbd ls <pool>
sudo rbd rm <pool>/<image>

# í•´ê²° 2: OSD ì¶”ê°€
sudo ceph orch daemon add osd ceph4:/dev/sdb

# í•´ê²° 3: ì„ì‹œë¡œ full ratio ì¦ê°€ (ë¹„ì¶”ì²œ)
sudo ceph osd set-full-ratio 0.97
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- **Monitoring Overview**: [https://docs.ceph.com/en/latest/monitoring/](https://docs.ceph.com/en/latest/monitoring/)
- **Monitoring Services (cephadm)**: [https://docs.ceph.com/en/latest/cephadm/services/monitoring/](https://docs.ceph.com/en/latest/cephadm/services/monitoring/)
- **Prometheus Module**: [https://docs.ceph.com/en/latest/mgr/prometheus/](https://docs.ceph.com/en/latest/mgr/prometheus/)
- **Monitoring OSDs and PGs**: [https://docs.ceph.com/en/reef/rados/operations/monitoring-osd-pg/](https://docs.ceph.com/en/reef/rados/operations/monitoring-osd-pg/)
- **Health Checks**: [https://docs.ceph.com/en/reef/rados/operations/health-checks/](https://docs.ceph.com/en/reef/rados/operations/health-checks/)
- **Troubleshooting PGs**: [https://docs.ceph.com/en/reef/rados/troubleshooting/troubleshooting-pg/](https://docs.ceph.com/en/reef/rados/troubleshooting/troubleshooting-pg/)
- **Troubleshooting OSDs**: [https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/)

### Rook ë¬¸ì„œ

- **Rook Ceph Monitoring**: [https://rook.io/docs/rook/latest/Storage-Configuration/Monitoring/ceph-monitoring/](https://rook.io/docs/rook/latest/Storage-Configuration/Monitoring/ceph-monitoring/)

### ë²¤ì¹˜ë§ˆí‚¹ & ì„±ëŠ¥ íŠœë‹

- **How to Benchmark Ceph Storage Performance**: [https://openmetal.io/resources/blog/how-to-benchmark-ceph-storage-performance/](https://openmetal.io/resources/blog/how-to-benchmark-ceph-storage-performance/)
- **Ceph Performance Benchmark and Optimization (croit)**: [https://www.croit.io/blog/ceph-performance-benchmark-and-optimization](https://www.croit.io/blog/ceph-performance-benchmark-and-optimization)
- **How to Tune Ceph for Block Storage Performance**: [https://openmetal.io/resources/blog/how-to-tune-ceph-for-block-storage-performance/](https://openmetal.io/resources/blog/how-to-tune-ceph-for-block-storage-performance/)
- **BlueStore Default vs. Tuned Performance**: [https://ceph.io/community/bluestore-default-vs-tuned-performance-comparison/](https://ceph.io/community/bluestore-default-vs-tuned-performance-comparison/)
- **Ceph All-Flash/NVMe Performance**: [https://croit.io/blog/ceph-performance-test-and-optimization](https://croit.io/blog/ceph-performance-test-and-optimization)
- **Tuning Ceph Performance (4sysops)**: [https://4sysops.com/archives/tuning-ceph-performance/](https://4sysops.com/archives/tuning-ceph-performance/)
- **Comprehensive Guide to Tuning Rook-Ceph**: [https://www.cloudraft.io/blog/rook-ceph-performance-tuning](https://www.cloudraft.io/blog/rook-ceph-performance-tuning)

### Red Hat & SUSE ë¬¸ì„œ

- **Red Hat Ceph Performance Benchmark (RHCS 5)**: [https://docs.redhat.com/en/documentation/red_hat_ceph_storage/5/html/administration_guide/ceph-performance-benchmarking](https://docs.redhat.com/en/documentation/red_hat_ceph_storage/5/html/administration_guide/ceph-performance-benchmarking)
- **Red Hat Troubleshooting Ceph OSDs**: [https://docs.redhat.com/en/documentation/red_hat_ceph_storage/6/html/troubleshooting_guide/troubleshooting-ceph-osds](https://docs.redhat.com/en/documentation/red_hat_ceph_storage/6/html/troubleshooting_guide/troubleshooting-ceph-osds)
- **SUSE Troubleshooting Ceph Health Status**: [https://documentation.suse.com/ses/7.1/html/ses-all/bp-troubleshooting-status.html](https://documentation.suse.com/ses/7.1/html/ses-all/bp-troubleshooting-status.html)

### ë¸”ë¡œê·¸ & ì•„í‹°í´

- **Monitoring Ceph with Prometheus (Sysdig)**: [https://www.sysdig.com/blog/monitoring-ceph-prometheus](https://www.sysdig.com/blog/monitoring-ceph-prometheus)
- **Ceph Stretch Clusters Part 3: Handling Failures (2025)**: [https://ceph.io/en/news/blog/2025/stretch-cluuuuuuuuusters-part3/](https://ceph.io/en/news/blog/2025/stretch-cluuuuuuuuusters-part3/)

### ë„êµ¬

- **Ceph Benchmarking Tool (CBT)**: [https://github.com/ceph/cbt](https://github.com/ceph/cbt)
- **Benchmark Ceph Cluster (Ceph Wiki)**: [https://tracker.ceph.com/projects/ceph/wiki/Benchmark_Ceph_Cluster_Performance](https://tracker.ceph.com/projects/ceph/wiki/Benchmark_Ceph_Cluster_Performance)

### í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ í•­ëª©ì— ì²´í¬í•˜ì„¸ìš”:

- [ ] cephadmê³¼ Rookì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ì ì ˆí•œ ë°°í¬ ë°©ë²•ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- [ ] Prometheus + Grafana ëª¨ë‹ˆí„°ë§ ìŠ¤íƒì„ ë°°í¬í•˜ê³  ìš´ì˜í•  ìˆ˜ ìˆë‹¤
- [ ] Prometheus ë©”íŠ¸ë¦­ì„ ì´í•´í•˜ê³  PromQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] rados benchì™€ FIOë¡œ í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆë‹¤
- [ ] BlueStore ë©”ëª¨ë¦¬ íƒ€ê²Ÿ, DB/WAL ë¶„ë¦¬ ë“± ì„±ëŠ¥ íŠœë‹ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] OSD ì¥ì•  ì‹œ ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ê³  ë³µêµ¬ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆë‹¤
- [ ] PG ìƒíƒœ(active, clean, degraded, recovering, backfilling)ë¥¼ í•´ì„í•  ìˆ˜ ìˆë‹¤
- [ ] mClock ìŠ¤ë¡œí‹€ë§ì„ í†µí•´ ë³µêµ¬ ì‘ì—…ì´ í´ë¼ì´ì–¸íŠ¸ I/Oì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤
- [ ] Slow ops, Clock skew, OSD full ë“± ì¼ë°˜ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤
- [ ] Ceph í´ëŸ¬ìŠ¤í„°ì˜ ìš©ëŸ‰ ì„ê³„ê°’(85%/90%/95%)ì„ ì´í•´í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤

---

**ë‹¤ìŒ ì±•í„°**: [Ch11. Block Storage (Cinder)](./Ch11.Block_Storage_Cinder.md)ì—ì„œ OpenStack Cinder ì•„í‚¤í…ì²˜, Volume Driver, QoS, Multi-backendë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
