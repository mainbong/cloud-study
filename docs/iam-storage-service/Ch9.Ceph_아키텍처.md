# Ch9. Ceph ì•„í‚¤í…ì²˜

## ğŸ“‹ ê°œìš” ë° í•™ìŠµ ëª©í‘œ

### ê°œìš”

**Ceph**ëŠ” í†µí•© ì†Œí”„íŠ¸ì›¨ì–´ ì •ì˜ ìŠ¤í† ë¦¬ì§€(Software-Defined Storage) í”Œë«í¼ìœ¼ë¡œ, **ê°ì²´ ìŠ¤í† ë¦¬ì§€(Object)**, **ë¸”ë¡ ìŠ¤í† ë¦¬ì§€(Block)**, **íŒŒì¼ ìŠ¤í† ë¦¬ì§€(File)**ë¥¼ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. CephëŠ” **RADOS**(Reliable Autonomic Distributed Object Store)ë¥¼ í•µì‹¬ ë ˆì´ì–´ë¡œ ë‘ê³ , ê·¸ ìœ„ì— **RBD**(RADOS Block Device), **RGW**(RADOS Gateway), **CephFS**(Ceph File System)ë¥¼ ê³„ì¸µí™”í•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, Ceph **Squid** ë¦´ë¦¬ìŠ¤ëŠ” **BlueStore V3** ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì„ í†µí•´ **25% IOPS í–¥ìƒ**, **RocksDB ë©”íƒ€ë°ì´í„° ìµœì í™”**ë¡œ **10-20% ì„±ëŠ¥ ê°œì„ **, **ARM ì•„í‚¤í…ì²˜ ì§€ì›** ë“± ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ìš”êµ¬í•˜ëŠ” ê³ ì„±ëŠ¥Â·ê³ ê°€ìš©ì„±Â·í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

### í•™ìŠµ ëª©í‘œ

ì´ ì±•í„°ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **Ceph ì•„í‚¤í…ì²˜ ì´í•´**: RADOS, CRUSH, ì»´í¬ë„ŒíŠ¸(MON, OSD, MGR, MDS, RGW) ì—­í•  ì„¤ëª…
2. **CRUSH ì•Œê³ ë¦¬ì¦˜ ë¶„ì„**: ë°ì´í„° ë°°ì¹˜, ë³µì œ, ì¥ì•  ë„ë©”ì¸ ì„¤ì • ì›ë¦¬ ì´í•´
3. **Pool & PG ì„¤ê³„**: Placement Group ê³„ì‚°, Erasure Coding vs Replication ì„ íƒ
4. **BlueStore ì—”ì§„ ì´í•´**: RocksDB ë©”íƒ€ë°ì´í„°, ì„±ëŠ¥ íŠ¹ì„±, V3 ê°œì„ ì‚¬í•­ íŒŒì•…
5. **Ceph ì„œë¹„ìŠ¤ í™œìš©**: RBD, RGW, CephFSì˜ ë™ì‘ ì›ë¦¬ ë° ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì´í•´

---

## ğŸ”‘ í•µì‹¬ ê°œë… ë° ì´ë¡ 

### 1. Ceph ì „ì²´ ì•„í‚¤í…ì²˜

#### 1.1 ê³„ì¸µ êµ¬ì¡°

CephëŠ” **RADOS**ë¥¼ í•µì‹¬ìœ¼ë¡œ 3ê³„ì¸µ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "Application Layer"
        Apps[Applications]
    end

    subgraph "Interface Layer"
        RBD[RBD<br/>RADOS Block Device<br/>VM ë””ìŠ¤í¬, Kubernetes PV]
        RGW[RGW<br/>RADOS Gateway<br/>S3/Swift API]
        CephFS[CephFS<br/>Ceph File System<br/>POSIX í˜¸í™˜]
    end

    subgraph "Core Layer - RADOS"
        LIBRADOS[librados Library]

        subgraph "RADOS Daemons"
            MON[MON<br/>Monitor<br/>í´ëŸ¬ìŠ¤í„° ë§µ ê´€ë¦¬]
            OSD[OSD<br/>Object Storage Daemon<br/>ë°ì´í„° ì €ì¥]
            MGR[MGR<br/>Manager<br/>ëª¨ë‹ˆí„°ë§/í”ŒëŸ¬ê·¸ì¸]
            MDS[MDS<br/>Metadata Server<br/>CephFS ë©”íƒ€ë°ì´í„°]
        end
    end

    subgraph "Storage Layer"
        Disk1[Disk 1]
        Disk2[Disk 2]
        Disk3[Disk 3]
        DiskN[Disk N]
    end

    Apps --> RBD
    Apps --> RGW
    Apps --> CephFS

    RBD --> LIBRADOS
    RGW --> LIBRADOS
    CephFS --> LIBRADOS

    LIBRADOS --> MON
    LIBRADOS --> OSD
    LIBRADOS --> MGR
    CephFS --> MDS

    OSD --> Disk1
    OSD --> Disk2
    OSD --> Disk3
    OSD --> DiskN
```

**í•µì‹¬ ì›ì¹™**:

- **No Single Point of Failure**: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ë‹¤ì¤‘í™”
- **í™•ì¥ì„±**: ì—‘ì‚¬ë°”ì´íŠ¸ ê·œëª¨ê¹Œì§€ ì„ í˜• í™•ì¥
- **ììœ¨ì„±**: CRUSH ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¤‘ì•™ ë©”íƒ€ë°ì´í„° ì„œë²„ ë¶ˆí•„ìš”

#### 1.2 RADOS (Reliable Autonomic Distributed Object Store)

**RADOS**ëŠ” Cephì˜ í•µì‹¬ ìŠ¤í† ë¦¬ì§€ ë ˆì´ì–´ë¡œ, ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- **ìë™ ë³µì œ(Auto-replication)**: ë°ì´í„°ë¥¼ ì—¬ëŸ¬ OSDì— ìë™ ë³µì œ
- **ìê°€ ì¹˜ìœ (Self-healing)**: OSD ì¥ì•  ì‹œ ìë™ ë³µêµ¬
- **ë°ì´í„° ë¬´ê²°ì„±**: Scrubbingì„ í†µí•œ ë°ì´í„° ê²€ì¦
- **ë¶„ì‚° ê°ì²´ ì €ì¥ì†Œ**: ê°€ë³€ í¬ê¸° ê°ì²´ë¥¼ í™•ì¥ ê°€ëŠ¥í•˜ê²Œ ì €ì¥

### 2. Ceph í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### 2.1 MON (Monitor)

**ì—­í• **: í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë§µ(Cluster Map)ì˜ ë§ˆìŠ¤í„° ë³µì‚¬ë³¸ ìœ ì§€

```mermaid
graph LR
    subgraph "MON Cluster (Quorum)"
        MON1[MON 1]
        MON2[MON 2]
        MON3[MON 3]
    end

    subgraph "Cluster Maps"
        MonMap[Monitor Map<br/>MON ì£¼ì†Œ]
        OSDMap[OSD Map<br/>OSD ìƒíƒœ]
        PGMap[PG Map<br/>PG ìƒíƒœ]
        CRUSHMap[CRUSH Map<br/>í† í´ë¡œì§€]
        MDSMap[MDS Map<br/>MDS ìƒíƒœ]
    end

    MON1 -.Paxos Consensus.-> MON2
    MON2 -.Paxos Consensus.-> MON3
    MON3 -.Paxos Consensus.-> MON1

    MON1 --> MonMap
    MON1 --> OSDMap
    MON1 --> PGMap
    MON1 --> CRUSHMap
    MON1 --> MDSMap
```

**íŠ¹ì§•**:

- **Quorum**: í™€ìˆ˜ ê°œ(ë³´í†µ 3ê°œ ë˜ëŠ” 5ê°œ)ë¡œ êµ¬ì„±í•˜ì—¬ ê³¼ë°˜ìˆ˜ í•©ì˜ë¡œ ê²°ì •
- **Paxos ì•Œê³ ë¦¬ì¦˜**: ë¶„ì‚° í•©ì˜ í”„ë¡œí† ì½œë¡œ í´ëŸ¬ìŠ¤í„° ë§µ ì—…ë°ì´íŠ¸
- **ê²½ëŸ‰ í”„ë¡œì„¸ìŠ¤**: ë””ìŠ¤í¬ I/Oê°€ ì ì–´ ë‚®ì€ ì‚¬ì–‘ ì„œë²„ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥

#### 2.2 OSD (Object Storage Daemon)

**ì—­í• **: ì‹¤ì œ ë°ì´í„° ì €ì¥, ì½ê¸°/ì“°ê¸°/ë³µì œ ì‘ì—… ìˆ˜í–‰

```mermaid
graph TB
    subgraph "OSD ë‚´ë¶€ êµ¬ì¡°"
        OSDDaemon[OSD Daemon]

        subgraph "BlueStore (Storage Engine)"
            BlueFS[BlueFS<br/>RocksDB ì „ìš© FS]
            RocksDB[RocksDB<br/>ë©”íƒ€ë°ì´í„°]
            BlueStore[BlueStore<br/>ê°ì²´ ë°ì´í„°]
        end

        BlockDevice[Block Device<br/>/dev/sdb]
    end

    OSDDaemon --> BlueFS
    OSDDaemon --> RocksDB
    OSDDaemon --> BlueStore

    BlueFS --> BlockDevice
    RocksDB --> BlueFS
    BlueStore --> BlockDevice
```

**ì£¼ìš” ê¸°ëŠ¥**:

- **ë°ì´í„° ì €ì¥**: BlueStore ì—”ì§„ìœ¼ë¡œ ê°ì²´ë¥¼ ë¸”ë¡ ë””ë°”ì´ìŠ¤ì— ì§ì ‘ ì €ì¥
- **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**: RocksDBì— ê°ì²´ ë©”íƒ€ë°ì´í„° ì €ì¥
- **ë³µì œ**: Primary OSDê°€ ë°ì´í„°ë¥¼ Replica OSDë¡œ ë³µì œ
- **Recovery**: OSD ì¥ì•  ì‹œ ë‹¤ë¥¸ OSDì—ì„œ ë°ì´í„° ë³µêµ¬
- **Scrubbing**: ì£¼ê¸°ì ìœ¼ë¡œ ê°ì²´ ë¬´ê²°ì„± ê²€ì¦ (Light Scrub ë§¤ì¼, Deep Scrub ì£¼ 1íšŒ)

**BlueStore vs FileStore (Legacy)**:

| íŠ¹ì„± | BlueStore (ê¸°ë³¸) | FileStore (Legacy) |
|------|------------------|-------------------|
| **ì•„í‚¤í…ì²˜** | ë¸”ë¡ ë””ë°”ì´ìŠ¤ ì§ì ‘ ì‚¬ìš© | XFS/ext4 íŒŒì¼ì‹œìŠ¤í…œ ìœ„ |
| **ë©”íƒ€ë°ì´í„°** | RocksDB | LevelDB + XAttr |
| **ì„±ëŠ¥** | ~18% IOPS ì¦ê°€, 15% ë ˆì´í„´ì‹œ ê°ì†Œ | ê¸°ì¤€ |
| **Double-write** | ì—†ìŒ (single-write) | ìˆìŒ (ì €ë„ + FS) |
| **ìƒíƒœ** | ê¸°ë³¸ê°’ (Luminous ì´í›„) | Deprecated |

#### 2.3 MGR (Manager)

**ì—­í• **: ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸, ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, í”ŒëŸ¬ê·¸ì¸ ëª¨ë“ˆ ì œê³µ

**ì£¼ìš” ëª¨ë“ˆ** (2025):

- **Dashboard**: ì›¹ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ UI
- **Prometheus**: ë©”íŠ¸ë¦­ exporter
- **Telemetry**: ì‚¬ìš© í†µê³„ ìˆ˜ì§‘ (ì˜µíŠ¸ì¸)
- **Balancer**: PG ìë™ ë°¸ëŸ°ì‹±
- **Orchestrator**: cephadm/Rook í†µí•© (í´ëŸ¬ìŠ¤í„° ë°°í¬/ê´€ë¦¬ ìë™í™”)

**í•„ìˆ˜ ì—¬ë¶€**: Luminous (12.x) ì´í›„ **í•„ìˆ˜ ë°ëª¬** (MGR ì—†ì´ëŠ” í´ëŸ¬ìŠ¤í„° ì •ìƒ ë™ì‘ ë¶ˆê°€)

#### 2.4 MDS (Metadata Server)

**ì—­í• **: CephFSì˜ POSIX ë©”íƒ€ë°ì´í„° ê´€ë¦¬

```mermaid
sequenceDiagram
    participant Client as CephFS Client
    participant MDS
    participant RADOS

    Client->>MDS: open("/data/file.txt")
    MDS->>MDS: ë””ë ‰í† ë¦¬/íŒŒì¼ ì´ë¦„ â†’ inode ë§¤í•‘
    MDS-->>Client: inode ì •ë³´ + RADOS object ID

    Client->>RADOS: read(object_id)
    RADOS-->>Client: íŒŒì¼ ë°ì´í„° (ì§ì ‘ I/O)

    Note over Client,RADOS: ë©”íƒ€ë°ì´í„°ëŠ” MDS, ë°ì´í„°ëŠ” RADOSë¡œ ë¶„ë¦¬
```

**íŠ¹ì§•**:

- **Active-Standby**: ì—¬ëŸ¬ MDS ì¤‘ 1ê°œë§Œ Active (ì¥ì•  ì‹œ Standbyê°€ ìŠ¹ê²©)
- **ë©”íƒ€ë°ì´í„°ë§Œ ì²˜ë¦¬**: ì‹¤ì œ íŒŒì¼ ë°ì´í„°ëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ RADOSì— ì§ì ‘ I/O
- **ìºì‹±**: ìµœê·¼ ì ‘ê·¼í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹±

#### 2.5 RGW (RADOS Gateway)

**ì—­í• **: S3/Swift í˜¸í™˜ ê°ì²´ ìŠ¤í† ë¦¬ì§€ API ì œê³µ

**ì•„í‚¤í…ì²˜**:

```
HTTP Request (S3 API)
    â†“
RGW (Apache/nginx + FastCGI)
    â†“
librados
    â†“
RADOS (ê°ì²´ ì €ì¥)
```

**ì£¼ìš” ê¸°ëŠ¥**:

- **S3 API**: AWS S3 í˜¸í™˜ (ë²„í‚·, ê°ì²´, ACL, ë²„ì „ ê´€ë¦¬)
- **Swift API**: OpenStack Swift í˜¸í™˜
- **ë©€í‹°í…Œë„Œì‹œ**: User, Tenant, Bucket ë¶„ë¦¬
- **ë©€í‹°ì‚¬ì´íŠ¸ ë³µì œ**: ì—¬ëŸ¬ Ceph í´ëŸ¬ìŠ¤í„° ê°„ ë³µì œ (Active-Active)

### 3. CRUSH ì•Œê³ ë¦¬ì¦˜

#### 3.1 CRUSH (Controlled Replication Under Scalable Hashing)

CRUSHëŠ” **ì¤‘ì•™ ì§‘ì¤‘ì‹ ë©”íƒ€ë°ì´í„° ì„œë²„ ì—†ì´** ë°ì´í„° ìœ„ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "CRUSH ê³„ì¸µ (ì˜ˆì‹œ)"
        Root[root: datacenter]

        Row1[row1: rack1]
        Row2[row2: rack2]

        Host1[host1: server1]
        Host2[host2: server2]
        Host3[host3: server3]
        Host4[host4: server4]

        OSD1[osd.0]
        OSD2[osd.1]
        OSD3[osd.2]
        OSD4[osd.3]
        OSD5[osd.4]
        OSD6[osd.5]
        OSD7[osd.6]
        OSD8[osd.7]
    end

    Root --> Row1
    Root --> Row2

    Row1 --> Host1
    Row1 --> Host2
    Row2 --> Host3
    Row2 --> Host4

    Host1 --> OSD1
    Host1 --> OSD2
    Host2 --> OSD3
    Host2 --> OSD4
    Host3 --> OSD5
    Host3 --> OSD6
    Host4 --> OSD7
    Host4 --> OSD8
```

**CRUSH Map êµ¬ì„± ìš”ì†Œ**:

1. **Bucket Hierarchy**: root â†’ row â†’ rack â†’ host â†’ osd (ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥)
2. **Bucket Types**: root, datacenter, room, row, pod, pdu, rack, chassis, host, osd
3. **Failure Domain**: ë³µì œë³¸ì„ ë°°ì¹˜í•  ì¥ì•  ë„ë©”ì¸ (ì˜ˆ: host, rack)
4. **Device Class**: HDD, SSD, NVMe ë“± ë””ë°”ì´ìŠ¤ í´ë˜ìŠ¤ (ë£°ì—ì„œ í•„í„°ë§ ê°€ëŠ¥)

#### 3.2 CRUSH Rule

Poolë§ˆë‹¤ CRUSH Ruleì„ ì§€ì •í•˜ì—¬ **ë°ì´í„° ë°°ì¹˜ ì •ì±…**ì„ ì •ì˜í•©ë‹ˆë‹¤:

```bash
# CRUSH Rule ì˜ˆì‹œ (replicated pool, failure-domain=host)
rule replicated_rule {
    id 0
    type replicated
    step take default class ssd      # SSD í´ë˜ìŠ¤ ë””ë°”ì´ìŠ¤ë§Œ ì‚¬ìš©
    step chooseleaf firstn 0 type host  # ì„œë¡œ ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì— ë°°ì¹˜
    step emit
}

# Erasure-coded pool ë£°
rule erasure_rule {
    id 1
    type erasure
    step set_chooseleaf_tries 5
    step set_choose_tries 100
    step take default
    step chooseleaf indep 0 type rack   # ì„œë¡œ ë‹¤ë¥¸ ë™ì— ë°°ì¹˜
    step emit
}
```

**Rule íŒŒë¼ë¯¸í„°**:

- **type**: `replicated` (ë³µì œ) ë˜ëŠ” `erasure` (EC)
- **step take**: ì‹œì‘ bucket ì„ íƒ (+ device class í•„í„°)
- **step chooseleaf**: Nê°œì˜ í•˜ìœ„ bucket ì„ íƒ
  - `firstn`: replicated poolìš© (ìˆœì°¨ ì„ íƒ)
  - `indep`: erasure-coded poolìš© (ë…ë¦½ ì„ íƒ, ì¬ë§¤í•‘ ì‹œ ìµœì†Œí™”)
- **failure domain**: `host`, `rack`, `row` ë“± ì¥ì•  ê²©ë¦¬ ë‹¨ìœ„

#### 3.3 ë°ì´í„° ë°°ì¹˜ ê³¼ì •

1. **Object â†’ PG ë§¤í•‘**:
   ```
   PG ID = hash(object_name) % pg_num
   ```

2. **PG â†’ OSD ë§¤í•‘** (CRUSH ì•Œê³ ë¦¬ì¦˜):
   ```
   OSD_list = CRUSH(PG_ID, CRUSH_Map, CRUSH_Rule, num_replicas)
   ```

3. **í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ê³„ì‚°**:
   - í´ë¼ì´ì–¸íŠ¸ëŠ” CRUSH Mapì„ ê°€ì§€ê³  ìˆì–´ OSD ìœ„ì¹˜ë¥¼ ì§ì ‘ ê³„ì‚°
   - ì¤‘ì•™ ë©”íƒ€ë°ì´í„° ì„œë²„ ë¶ˆí•„ìš” â†’ ë³‘ëª© ì—†ìŒ

### 4. Placement Groups (PG)

#### 4.1 PGì˜ ì—­í• 

**Placement Group**ì€ **ì—¬ëŸ¬ ê°ì²´ë¥¼ í•˜ë‚˜ì˜ ë…¼ë¦¬ ê·¸ë£¹**ìœ¼ë¡œ ë¬¶ì–´ ê´€ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì…ë‹ˆë‹¤:

```mermaid
graph LR
    subgraph "Pool (pg_num=8)"
        PG0[PG 0.0]
        PG1[PG 0.1]
        PG2[PG 0.2]
        PG7[PG 0.7]
    end

    subgraph "Objects"
        Obj1[object1]
        Obj2[object2]
        Obj3[object3]
        ObjN[objectN]
    end

    subgraph "OSDs"
        OSD1[OSD 1]
        OSD2[OSD 2]
        OSD3[OSD 3]
    end

    Obj1 -.hash % 8.-> PG0
    Obj2 -.hash % 8.-> PG1
    Obj3 -.hash % 8.-> PG0
    ObjN -.hash % 8.-> PG7

    PG0 --> OSD1
    PG0 --> OSD2
    PG1 --> OSD2
    PG1 --> OSD3
    PG7 --> OSD1
    PG7 --> OSD3
```

**PG ìˆ˜ ê²°ì •**:

- **ê¸°ë³¸ ê°€ì´ë“œ**: Poolë‹¹ **50-100 PG** ê¶Œì¥
- **ê³µì‹** (2025):
  ```
  PG ìˆ˜ = (Total_OSDs Ã— mon_target_pg_per_osd) / pool_size
  ```
- **mon_target_pg_per_osd**: ê¸°ë³¸ê°’ 100, **BlueStore í™˜ê²½ì—ì„œëŠ” 200-250 ê¶Œì¥**

**ì˜ˆì‹œ ê³„ì‚°**:

```
í´ëŸ¬ìŠ¤í„°: OSD 30ê°œ
Pool: size=3 (3-way replication)
mon_target_pg_per_osd = 200

PG ìˆ˜ = (30 Ã— 200) / 3 = 2000

â†’ pg_num = 2048 (2ì˜ ê±°ë“­ì œê³±ìœ¼ë¡œ ë°˜ì˜¬ë¦¼)
```

#### 4.2 PG States

| ìƒíƒœ | ì„¤ëª… |
|------|------|
| **active** | PGê°€ ì •ìƒ ë™ì‘ ì¤‘ |
| **clean** | ëª¨ë“  ê°ì²´ê°€ ì˜¬ë°”ë¥¸ ë³µì œë³¸ ìˆ˜ë¥¼ ê°€ì§ |
| **peering** | OSD ê°„ PG ìƒíƒœ ë™ê¸°í™” ì¤‘ |
| **degraded** | ì¼ë¶€ ê°ì²´ê°€ ë³µì œë³¸ ìˆ˜ ë¯¸ë‹¬ (OSD ì¥ì•  ë“±) |
| **recovering** | ê°ì²´ ë³µêµ¬ ì§„í–‰ ì¤‘ |
| **backfilling** | ìƒˆ OSDì— PG ë°ì´í„° ì´ë™ ì¤‘ |
| **remapped** | PGê°€ ìƒˆë¡œìš´ OSD ì„¸íŠ¸ë¡œ ì¬ë§¤í•‘ë¨ |
| **stale** | MONì´ PG ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ë°›ì§€ ëª»í•¨ (OSD ì¥ì• ) |

### 5. Pool íƒ€ì…

#### 5.1 Replicated Pool

**ë™ì‘**: ê°ì²´ë¥¼ Nê°œì˜ OSDì— **ì™„ì „ ë³µì‚¬**

```mermaid
graph LR
    Object[Object: 4MB]

    OSD1[OSD 1<br/>4MB]
    OSD2[OSD 2<br/>4MB]
    OSD3[OSD 3<br/>4MB]

    Object --> OSD1
    Object --> OSD2
    Object --> OSD3

    Note1[ì´ ì €ì¥ ìš©ëŸ‰: 12MB]
    Note2[Storage Overhead: 3x]
```

**íŠ¹ì§•**:

- **size**: ë³µì œë³¸ ìˆ˜ (ê¸°ë³¸ê°’ 3)
- **min_size**: ìµœì†Œ ë³µì œë³¸ ìˆ˜ (I/O í—ˆìš© ì„ê³„ê°’, ê¸°ë³¸ê°’ 2)
- **ì¥ì **: ì½ê¸° ì„±ëŠ¥ ìš°ìˆ˜ (ì—¬ëŸ¬ OSDì—ì„œ ë³‘ë ¬ ì½ê¸° ê°€ëŠ¥)
- **ë‹¨ì **: ìŠ¤í† ë¦¬ì§€ ì˜¤ë²„í—¤ë“œ ë†’ìŒ (3ë°°)

#### 5.2 Erasure-Coded Pool

**ë™ì‘**: ê°ì²´ë¥¼ **Kê°œ ë°ì´í„° ì²­í¬ + Mê°œ íŒ¨ë¦¬í‹° ì²­í¬**ë¡œ ë¶„í• 

```mermaid
graph LR
    Object[Object: 4MB]

    subgraph "EC 4+2"
        D1[Data 1<br/>1MB]
        D2[Data 2<br/>1MB]
        D3[Data 3<br/>1MB]
        D4[Data 4<br/>1MB]
        P1[Parity 1<br/>1MB]
        P2[Parity 2<br/>1MB]
    end

    Object -.Split + Encode.-> D1
    Object -.Split + Encode.-> D2
    Object -.Split + Encode.-> D3
    Object -.Split + Encode.-> D4
    Object -.Encode.-> P1
    Object -.Encode.-> P2

    Note1[ì´ ì €ì¥ ìš©ëŸ‰: 6MB]
    Note2[Storage Overhead: 1.5x]
    Note3[ìµœëŒ€ 2ê°œ OSD ì¥ì•  í—ˆìš©]
```

**í”„ë¡œíŒŒì¼ íŒŒë¼ë¯¸í„°**:

- **K**: ë°ì´í„° ì²­í¬ ìˆ˜ (ì˜ˆ: 4)
- **M**: íŒ¨ë¦¬í‹° ì²­í¬ ìˆ˜ (ì˜ˆ: 2)
- **crush-failure-domain**: ì²­í¬ ë°°ì¹˜ ì¥ì•  ë„ë©”ì¸ (host, rack ë“±)

**Storage Overhead ê³„ì‚°**:

```
Overhead = (K + M) / K
```

| Profile | K | M | Overhead | ì¥ì•  í—ˆìš© |
|---------|---|---|----------|----------|
| 2+1 | 2 | 1 | 1.5x | 1 OSD |
| 4+2 | 4 | 2 | 1.5x | 2 OSD |
| 8+3 | 8 | 3 | 1.375x | 3 OSD |

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**:

- **Replicated**: ë†’ì€ ì„±ëŠ¥ ìš”êµ¬ (VM ë””ìŠ¤í¬, DB)
- **Erasure-Coded**: ëŒ€ìš©ëŸ‰ ì½œë“œ ë°ì´í„° (ë°±ì—…, ì•„ì¹´ì´ë¸Œ, ë¡œê·¸)

### 6. BlueStore Storage Engine (2025)

#### 6.1 BlueStore ì•„í‚¤í…ì²˜

BlueStoreëŠ” Cephì˜ **ê¸°ë³¸ ìŠ¤í† ë¦¬ì§€ ì—”ì§„**ìœ¼ë¡œ, íŒŒì¼ì‹œìŠ¤í…œ ì—†ì´ **ë¸”ë¡ ë””ë°”ì´ìŠ¤ì— ì§ì ‘ ê°ì²´ë¥¼ ì €ì¥**í•©ë‹ˆë‹¤:

```mermaid
graph TB
    subgraph "OSD Process"
        OSDDaemon[OSD Daemon]
    end

    subgraph "BlueStore Layers"
        ObjectStore[Object Store Interface]

        Allocator[Allocator<br/>ë¸”ë¡ í• ë‹¹ ê´€ë¦¬]
        Cache[Cache<br/>ê°ì²´ ìºì‹±]

        BlueFS[BlueFS<br/>RocksDB ì „ìš© FS]
        RocksDB[(RocksDB<br/>ë©”íƒ€ë°ì´í„° KV)]

        BlueStore[BlueStore<br/>ê°ì²´ ë°ì´í„°]
    end

    subgraph "Block Device"
        BlockDev[Raw Block Device<br/>/dev/nvme0n1]

        DBPart[DB Partition<br/>RocksDB ë©”íƒ€ë°ì´í„°]
        WALPart[WAL Partition<br/>Write-Ahead Log]
        DataPart[Data Partition<br/>ê°ì²´ ë°ì´í„°]
    end

    OSDDaemon --> ObjectStore
    ObjectStore --> Allocator
    ObjectStore --> Cache

    Allocator --> BlueStore
    Cache --> BlueStore

    RocksDB --> BlueFS
    BlueFS --> DBPart
    RocksDB --> WALPart
    BlueStore --> DataPart
```

**ì£¼ìš” êµ¬ì„± ìš”ì†Œ**:

1. **RocksDB**: ê°ì²´ ë©”íƒ€ë°ì´í„° KV ì €ì¥
   - Object ì†ì„± (í¬ê¸°, íƒ€ì„ìŠ¤íƒ¬í”„, ì²´í¬ì„¬)
   - Deferred ì“°ê¸° ë¡œê·¸
   - OSD ë‚´ë¶€ ìƒíƒœ (Allocator state)

2. **BlueFS**: RocksDB ì „ìš© ìµœì†Œ íŒŒì¼ì‹œìŠ¤í…œ
   - RocksDB SST íŒŒì¼, WAL ì €ì¥
   - ë¸”ë¡ ë””ë°”ì´ìŠ¤ ì§ì ‘ ì‚¬ìš©

3. **Allocator**: ë¸”ë¡ í• ë‹¹ ê´€ë¦¬
   - BlueStore V3: í• ë‹¹ ì •ë³´ë¥¼ RocksDBì—ì„œ ì œê±° â†’ umount ì‹œ í•œ ë²ˆì— ì €ì¥
   - **25% IOPS ì¦ê°€**, ë ˆì´í„´ì‹œ ê°ì†Œ

4. **Cache**: ê°ì²´ ë°ì´í„° ìºì‹±
   - ì½ê¸°/ì“°ê¸° ìºì‹œ (LRU)

#### 6.2 BlueStore V3 ê°œì„ ì‚¬í•­ (2025)

**í•µì‹¬ ë³€ê²½**: Allocation ì •ë³´ë¥¼ RocksDBì—ì„œ ì œê±°

**ì´ì „ (V2)**:
```
ê°ì²´ ì“°ê¸° â†’ BlueStore ë¸”ë¡ í• ë‹¹ â†’ RocksDB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (í• ë‹¹ ì •ë³´ í¬í•¨)
                                   â†‘
                               ë³‘ëª© ì§€ì 
```

**V3**:
```
ê°ì²´ ì“°ê¸° â†’ BlueStore ë¸”ë¡ í• ë‹¹ â†’ ë©”ëª¨ë¦¬ì—ë§Œ ê¸°ë¡
OSD umount ì‹œ â†’ ì „ì²´ Allocator ìƒíƒœ í•œ ë²ˆì— ì €ì¥
```

**ì„±ëŠ¥ ê°œì„ **:

- **IOPS**: +25% (ì†Œí˜• ëœë¤ ì“°ê¸°)
- **ë ˆì´í„´ì‹œ**: RocksDB KV sync ë³‘ëª© ì œê±°ë¡œ 10-20% ê°ì†Œ
- **ì ìš©**: Pacific ì´í›„ ë¦´ë¦¬ìŠ¤

#### 6.3 RocksDB Column Families (Pacific+)

**ìµœì í™”**: Key-value ë°ì´í„°ë¥¼ ì—¬ëŸ¬ Column Familyë¡œ ë¶„ë¦¬

```
Column Family 1: Object Metadata (key: object_id)
Column Family 2: Deferred Writes
Column Family 3: OSD State
```

**íš¨ê³¼**:

- **ìºì‹± íš¨ìœ¨**: ìœ ì‚¬í•œ í‚¤ë¼ë¦¬ ê·¸ë£¹í™”
- **Compaction ì •ë°€ë„**: CFë³„ ë…ë¦½ì ì¸ compaction

### 7. Ceph Services (RBD, RGW, CephFS)

#### 7.1 RBD (RADOS Block Device)

**ìš©ë„**: ê°€ìƒ ë””ìŠ¤í¬ ì´ë¯¸ì§€ë¥¼ RADOS ê°ì²´ë¡œ ì €ì¥

```mermaid
sequenceDiagram
    participant VM
    participant Librbd
    participant RADOS

    VM->>Librbd: write(offset=0, size=4MB)
    Librbd->>Librbd: 4MB â†’ 4ê°œ ê°ì²´ (1MBì”© ìŠ¤íŠ¸ë¼ì´í•‘)

    loop 4ê°œ ê°ì²´
        Librbd->>RADOS: write(object_id, data)
        RADOS-->>Librbd: ACK
    end

    Librbd-->>VM: write ì™„ë£Œ
```

**íŠ¹ì§•**:

- **Thin Provisioning**: ì‹¤ì œ ì‚¬ìš©í•œ ë§Œí¼ë§Œ ì €ì¥ (10GB ë””ìŠ¤í¬ ìƒì„±í•´ë„ 0MB ì‚¬ìš©)
- **Snapshot**: ì½ê¸° ì „ìš© ìŠ¤ëƒ…ìƒ· (CoW)
- **Clone**: ìŠ¤ëƒ…ìƒ·ì—ì„œ ì“°ê¸° ê°€ëŠ¥í•œ í´ë¡  ìƒì„± (VM í…œí”Œë¦¿ì—ì„œ ë¹ ë¥¸ VM ìƒì„±)
- **Striping**: ëŒ€í˜• ë¸”ë¡ì„ ì—¬ëŸ¬ ê°ì²´ë¡œ ë¶„í• í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

#### 7.2 RGW (RADOS Gateway)

**ì•„í‚¤í…ì²˜**:

```
S3/Swift Client
    â†“
HTTPS (443)
    â†“
RGW (Civetweb / Apache)
    â†“
librados
    â†“
RADOS (metadata pool + data pool)
```

**Pool êµ¬ì„±**:

- **`.rgw.root`**: Gateway ì„¤ì •
- **`.rgw.buckets.index`**: Bucket ì¸ë±ìŠ¤ (ê°ì²´ ëª©ë¡)
- **`.rgw.buckets.data`**: ì‹¤ì œ ê°ì²´ ë°ì´í„°

**ë©€í‹°ì‚¬ì´íŠ¸ ë³µì œ**:

```
Region 1 (Master)          Region 2 (Replica)
    RGW                           RGW
     â†“                             â†“
  RADOS 1  â†--- ë©”íƒ€ë°ì´í„° ë™ê¸°í™” --â†’  RADOS 2
```

#### 7.3 CephFS (Ceph File System)

**ì•„í‚¤í…ì²˜**:

```
Linux Client (kernel cephfs / ceph-fuse)
    â†“
MDS (ë©”íƒ€ë°ì´í„°: /path/to/file â†’ inode)
    â†“
librados
    â†“
RADOS (metadata pool + data pool)
```

**ì£¼ìš” ê¸°ëŠ¥**:

- **POSIX í˜¸í™˜**: í‘œì¤€ íŒŒì¼ì‹œìŠ¤í…œ ì¸í„°í˜ì´ìŠ¤ (mkdir, open, read, write)
- **Snapshot**: ë””ë ‰í† ë¦¬ë³„ ìŠ¤ëƒ…ìƒ·
- **Quota**: ë””ë ‰í† ë¦¬ë³„ ìš©ëŸ‰ ì œí•œ
- **Multi-active MDS** (Pacific+): ì—¬ëŸ¬ MDSê°€ ë™ì‹œì— Active (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ íŒŒí‹°ì…”ë‹)

---

## ğŸ’» ì‹¤ìŠµ ê°€ì´ë“œ (Hands-on)

### Lab 1: Ceph í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜ (cephadm)

**ëª©í‘œ**: cephadmì„ ì‚¬ìš©í•˜ì—¬ 3-ë…¸ë“œ Ceph í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

**í™˜ê²½**:

- ë…¸ë“œ: ceph1, ceph2, ceph3 (ê° 2ê°œ ë””ìŠ¤í¬: OSìš© /dev/sda, Ceph OSDìš© /dev/sdb)
- OS: Ubuntu 22.04 ë˜ëŠ” Rocky Linux 9

**ë‹¨ê³„**:

1. **Bootstrap í´ëŸ¬ìŠ¤í„° (ceph1)**:

```bash
# cephadm ì„¤ì¹˜
sudo apt install -y cephadm  # Ubuntu
# sudo dnf install -y cephadm  # Rocky Linux

# Ceph í´ëŸ¬ìŠ¤í„° bootstrap
sudo cephadm bootstrap --mon-ip 192.168.1.10 --ssh-user root

# ì¶œë ¥ ì˜ˆì‹œ:
# Ceph Dashboard is now available at:
#     URL: https://192.168.1.10:8443/
#     User: admin
#     Password: abc123xyz
```

2. **ì¶”ê°€ ë…¸ë“œ ë“±ë¡ (ceph1ì—ì„œ ì‹¤í–‰)**:

```bash
# SSH key ë³µì‚¬
ssh-copy-id root@ceph2
ssh-copy-id root@ceph3

# ë…¸ë“œ ì¶”ê°€
sudo ceph orch host add ceph2 192.168.1.11
sudo ceph orch host add ceph3 192.168.1.12

# í˜¸ìŠ¤íŠ¸ ëª©ë¡ í™•ì¸
sudo ceph orch host ls
```

3. **OSD ì¶”ê°€**:

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ í™•ì¸
sudo ceph orch device ls

# ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ë¥¼ OSDë¡œ ì¶”ê°€
sudo ceph orch apply osd --all-available-devices

# ë˜ëŠ” íŠ¹ì • ë””ë°”ì´ìŠ¤ ì§€ì •
sudo ceph orch daemon add osd ceph1:/dev/sdb
sudo ceph orch daemon add osd ceph2:/dev/sdb
sudo ceph orch daemon add osd ceph3:/dev/sdb

# OSD ìƒíƒœ í™•ì¸
sudo ceph osd tree
```

4. **MON/MGR ë°°í¬**:

```bash
# MONì„ 3ê°œ ë…¸ë“œì— ë°°í¬
sudo ceph orch apply mon 3

# MGRì„ 2ê°œ ë…¸ë“œì— ë°°í¬
sudo ceph orch apply mgr 2

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
sudo ceph -s
```

**ì¶œë ¥ ì˜ˆì‹œ**:

```
  cluster:
    id:     a1b2c3d4-e5f6-1234-5678-abcdef123456
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)
    mgr: ceph1(active, since 4m), standbys: ceph2
    osd: 3 osds: 3 up (since 3m), 3 in (since 3m)

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 297 GiB / 300 GiB avail
    pgs:     1 active+clean
```

### Lab 2: Pool ìƒì„± (Replicated vs Erasure-Coded)

**ëª©í‘œ**: Replicated Poolê³¼ Erasure-Coded Pool ìƒì„± ë° ë¹„êµ

**ì½”ë“œ**:

```bash
# === Replicated Pool ===
# Pool ìƒì„± (size=3, pg_num=128)
sudo ceph osd pool create my-replicated-pool 128 replicated

# Pool ì†ì„± ì„¤ì •
sudo ceph osd pool set my-replicated-pool size 3
sudo ceph osd pool set my-replicated-pool min_size 2

# Pool ì •ë³´ í™•ì¸
sudo ceph osd pool get my-replicated-pool all

# === Erasure-Coded Pool ===
# EC í”„ë¡œíŒŒì¼ ìƒì„± (k=4, m=2)
sudo ceph osd erasure-code-profile set my-ec-profile \
    k=4 \
    m=2 \
    crush-failure-domain=host

# EC Pool ìƒì„±
sudo ceph osd pool create my-ec-pool 128 erasure my-ec-profile

# EC Pool ì •ë³´ í™•ì¸
sudo ceph osd pool get my-ec-pool erasure_code_profile

# === Pool ëª©ë¡ ì¡°íšŒ ===
sudo ceph osd pool ls detail
```

**í…ŒìŠ¤íŠ¸ ê°ì²´ ì“°ê¸°**:

```bash
# rados CLIë¡œ ê°ì²´ ì—…ë¡œë“œ
echo "Test data for replicated pool" > test.txt
sudo rados -p my-replicated-pool put test-object test.txt

# ê°ì²´ í™•ì¸
sudo rados -p my-replicated-pool ls
sudo rados -p my-replicated-pool stat test-object

# Storage ì‚¬ìš©ëŸ‰ í™•ì¸
sudo ceph df
```

**ì¶œë ¥ ì˜ˆì‹œ**:

```
GLOBAL:
    SIZE        AVAIL       RAW USED     %RAW USED
    300 GiB     297 GiB     3.0 GiB          1.00
POOLS:
    POOL                   ID     STORED      OBJECTS     USED        %USED     MAX AVAIL
    my-replicated-pool     1      35 B        1           105 B       0         99 GiB
    my-ec-pool             2      0 B         0           0 B         0         132 GiB
```

### Lab 3: CRUSH Map ì¡°ì‘

**ëª©í‘œ**: CRUSH Mapì„ ìˆ˜ì •í•˜ì—¬ SSD ì „ìš© Pool ìƒì„±

**ë‹¨ê³„**:

1. **Device Class í™•ì¸**:

```bash
# ëª¨ë“  OSDì˜ device class ì¡°íšŒ
sudo ceph osd tree

# ì¶œë ¥ ì˜ˆì‹œ:
# ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
# -1         0.29279  root default
# -3         0.09760      host ceph1
#  0    ssd  0.09760          osd.0         up   1.00000  1.00000
# -5         0.09760      host ceph2
#  1    hdd  0.09760          osd.1         up   1.00000  1.00000
# -7         0.09760      host ceph3
#  2    hdd  0.09760          osd.2         up   1.00000  1.00000
```

2. **SSD ì „ìš© CRUSH Rule ìƒì„±**:

```bash
# SSD ì „ìš© ë£° ìƒì„±
sudo ceph osd crush rule create-replicated ssd-rule default host ssd

# Rule í™•ì¸
sudo ceph osd crush rule ls
sudo ceph osd crush rule dump ssd-rule
```

3. **SSD Pool ìƒì„±**:

```bash
# SSD ì „ìš© Pool ìƒì„±
sudo ceph osd pool create ssd-pool 64 replicated

# SSD Rule ì ìš©
sudo ceph osd pool set ssd-pool crush_rule ssd-rule

# Pool ì •ë³´ í™•ì¸
sudo ceph osd pool get ssd-pool crush_rule
```

4. **ê²€ì¦**:

```bash
# Poolì— ê°ì²´ ì“°ê¸°
echo "SSD data" > ssd-test.txt
sudo rados -p ssd-pool put ssd-test-obj ssd-test.txt

# PG â†’ OSD ë§¤í•‘ í™•ì¸ (SSD OSDë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ ê²€ì¦)
sudo ceph pg dump pgs | grep ssd-pool
```

### Lab 4: RBD Block Device ì‚¬ìš©

**ëª©í‘œ**: RBD ì´ë¯¸ì§€ ìƒì„±, ë§ˆìš´íŠ¸, ìŠ¤ëƒ…ìƒ·

**ì½”ë“œ**:

```bash
# === RBD Pool ìƒì„± ===
sudo ceph osd pool create rbd 128 replicated
sudo ceph osd pool application enable rbd rbd

# === RBD ì´ë¯¸ì§€ ìƒì„± ===
sudo rbd create --size 10G rbd/my-disk

# ì´ë¯¸ì§€ ëª©ë¡ í™•ì¸
sudo rbd ls rbd
sudo rbd info rbd/my-disk

# === RBD ì´ë¯¸ì§€ ë§µí•‘ (Block Deviceë¡œ ë§ˆìš´íŠ¸) ===
sudo rbd map rbd/my-disk

# ë§µí•‘ëœ ë””ë°”ì´ìŠ¤ í™•ì¸
sudo rbd showmapped
# id  pool  namespace  image     snap  device
# 0   rbd              my-disk   -     /dev/rbd0

# === íŒŒì¼ì‹œìŠ¤í…œ ìƒì„± & ë§ˆìš´íŠ¸ ===
sudo mkfs.ext4 /dev/rbd0
sudo mkdir /mnt/rbd-disk
sudo mount /dev/rbd0 /mnt/rbd-disk

# íŒŒì¼ ì“°ê¸°
echo "Hello from RBD" | sudo tee /mnt/rbd-disk/test.txt

# === Snapshot ìƒì„± ===
# (ë§ˆìš´íŠ¸ í•´ì œ í•„ìš”)
sudo umount /mnt/rbd-disk
sudo rbd unmap /dev/rbd0

# ìŠ¤ëƒ…ìƒ· ìƒì„±
sudo rbd snap create rbd/my-disk@snap1

# ìŠ¤ëƒ…ìƒ· ëª©ë¡
sudo rbd snap ls rbd/my-disk

# === Clone ìƒì„± (ìŠ¤ëƒ…ìƒ·ì—ì„œ ìƒˆ ì´ë¯¸ì§€) ===
sudo rbd snap protect rbd/my-disk@snap1
sudo rbd clone rbd/my-disk@snap1 rbd/my-disk-clone

# Clone ì •ë³´
sudo rbd info rbd/my-disk-clone
```

### Lab 5: CephFS File System Mount

**ëª©í‘œ**: CephFS íŒŒì¼ì‹œìŠ¤í…œ ìƒì„± ë° í´ë¼ì´ì–¸íŠ¸ ë§ˆìš´íŠ¸

**ì½”ë“œ**:

1. **CephFS ìƒì„±**:

```bash
# Metadata Pool ìƒì„±
sudo ceph osd pool create cephfs_metadata 64 replicated

# Data Pool ìƒì„±
sudo ceph osd pool create cephfs_data 128 replicated

# CephFS ìƒì„±
sudo ceph fs new my-cephfs cephfs_metadata cephfs_data

# CephFS ìƒíƒœ í™•ì¸
sudo ceph fs ls
sudo ceph fs status my-cephfs
```

2. **MDS ë°°í¬**:

```bash
# MDS ë°ëª¬ ë°°í¬ (Active 1, Standby 1)
sudo ceph orch apply mds my-cephfs --placement="2 ceph1 ceph2"

# MDS ìƒíƒœ í™•ì¸
sudo ceph mds stat
```

3. **í´ë¼ì´ì–¸íŠ¸ ë§ˆìš´íŠ¸ (Kernel Driver)**:

```bash
# ì¸ì¦ í‚¤ í™•ì¸
sudo ceph auth get client.admin

# /etc/ceph/ceph.client.admin.keyring íŒŒì¼ì— í‚¤ ì €ì¥

# ë§ˆìš´íŠ¸
sudo mount -t ceph ceph1:6789:/ /mnt/cephfs -o name=admin,secret=AQD...==

# ë˜ëŠ” keyring íŒŒì¼ ì‚¬ìš©
sudo mount -t ceph ceph1:6789:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret

# ë§ˆìš´íŠ¸ í™•ì¸
df -h /mnt/cephfs
```

4. **CephFS ì‚¬ìš©**:

```bash
# íŒŒì¼ ìƒì„±
echo "Hello CephFS" | sudo tee /mnt/cephfs/test.txt

# ë””ë ‰í† ë¦¬ ìŠ¤ëƒ…ìƒ·
sudo mkdir /mnt/cephfs/.snap/snap1

# ìŠ¤ëƒ…ìƒ· í™•ì¸
ls /mnt/cephfs/.snap/

# Quota ì„¤ì • (ë””ë ‰í† ë¦¬ë³„)
sudo setfattr -n ceph.quota.max_bytes -v 1000000000 /mnt/cephfs/data  # 1GB
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- **Ceph ê³µì‹ ì‚¬ì´íŠ¸**: [https://ceph.io/](https://ceph.io/)
- **Ceph Technology**: [https://ceph.io/en/discover/technology/](https://ceph.io/en/discover/technology/)
- **Ceph Architecture**: [https://docs.ceph.com/en/reef/architecture/](https://docs.ceph.com/en/reef/architecture/)
- **CRUSH Maps**: [https://docs.ceph.com/en/latest/rados/operations/crush-map/](https://docs.ceph.com/en/latest/rados/operations/crush-map/)
- **Placement Groups**: [https://docs.ceph.com/en/reef/rados/operations/placement-groups/](https://docs.ceph.com/en/reef/rados/operations/placement-groups/)
- **BlueStore Configuration**: [https://docs.ceph.com/en/reef/rados/configuration/bluestore-config-ref/](https://docs.ceph.com/en/reef/rados/configuration/bluestore-config-ref/)
- **Erasure Code**: [https://docs.ceph.com/en/reef/rados/operations/erasure-code/](https://docs.ceph.com/en/reef/rados/operations/erasure-code/)
- **Ceph Glossary**: [https://docs.ceph.com/en/latest/glossary/](https://docs.ceph.com/en/latest/glossary/)

### ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸

- **Squid Release**: [https://docs.ceph.com/en/latest/releases/squid/](https://docs.ceph.com/en/latest/releases/squid/)

### í•˜ë“œì›¨ì–´ ê°€ì´ë“œ

- **Hardware Recommendations**: [https://docs.ceph.com/en/latest/start/hardware-recommendations/](https://docs.ceph.com/en/latest/start/hardware-recommendations/)
- **IBM Storage Ceph Concepts**: [https://www.redbooks.ibm.com/redpieces/pdfs/redp5721.pdf](https://www.redbooks.ibm.com/redpieces/pdfs/redp5721.pdf)

### Red Hat Ceph Storage ë¬¸ì„œ

- **Red Hat Storage Strategies Guide**: [https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html-single/storage_strategies_guide/index](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html-single/storage_strategies_guide/index)
- **Red Hat Placement Groups**: [https://docs.redhat.com/en/documentation/red_hat_ceph_storage/5/html/storage_strategies_guide/placement_groups_pgs](https://docs.redhat.com/en/documentation/red_hat_ceph_storage/5/html/storage_strategies_guide/placement_groups_pgs)

### ë¸”ë¡œê·¸ & ì•„í‹°í´

- **From RADOS to Ceph Services (RBD, RGW, CephFS)**: [https://dev.to/seno21/from-rados-to-ceph-services-rbd-rgw-and-cephfs-6ho](https://dev.to/seno21/from-rados-to-ceph-services-rbd-rgw-and-cephfs-6ho)
- **Ceph RocksDB Tuning Deep-Dive**: [https://ceph.io/en/news/blog/2022/rocksdb-tuning-deep-dive/](https://ceph.io/en/news/blog/2022/rocksdb-tuning-deep-dive/)
- **Comparing Ceph BlueStore and FileStore**: [https://www.micron.com/about/blog/storage/ssd/comparing-ceph-bluestore-filestore-block](https://www.micron.com/about/blog/storage/ssd/comparing-ceph-bluestore-filestore-block)

### êµìœ¡ ìë£Œ

- **Ceph Storage Fundamentals - Level 1**: [https://ecintelligence.ma/en/sessions/ceph-storage-fundamentals-level-1-20250804/](https://ecintelligence.ma/en/sessions/ceph-storage-fundamentals-level-1-20250804/)

### Wikipedia

- **Ceph (software)**: [https://en.wikipedia.org/wiki/Ceph_(software)](https://en.wikipedia.org/wiki/Ceph_(software))

### í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ í•­ëª©ì— ì²´í¬í•˜ì„¸ìš”:

- [ ] RADOS, CRUSH ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] Ceph ì»´í¬ë„ŒíŠ¸(MON, OSD, MGR, MDS, RGW)ì˜ ì—­í• ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] CRUSH Mapê³¼ CRUSH Ruleì„ í¸ì§‘í•˜ì—¬ ë°ì´í„° ë°°ì¹˜ ì •ì±…ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤
- [ ] PG ìˆ˜ ê³„ì‚° ê³µì‹ì„ ì´í•´í•˜ê³  ì ì ˆí•œ PG ìˆ˜ë¥¼ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤
- [ ] Replicated Poolê³¼ Erasure-Coded Poolì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ìš©ë„ì— ë§ê²Œ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- [ ] BlueStore V3ì˜ ì„±ëŠ¥ ê°œì„  ë©”ì»¤ë‹ˆì¦˜(Allocator state ê´€ë¦¬)ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] RBD ì´ë¯¸ì§€ ìƒì„±, ìŠ¤ëƒ…ìƒ·, í´ë¡  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] CephFSë¥¼ ë§ˆìš´íŠ¸í•˜ê³  POSIX íŒŒì¼ì‹œìŠ¤í…œìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] RGWë¥¼ í†µí•´ S3 í˜¸í™˜ ê°ì²´ ìŠ¤í† ë¦¬ì§€ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤
- [ ] Ceph í´ëŸ¬ìŠ¤í„°ì˜ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¬¸ì œë¥¼ ì§„ë‹¨í•  ìˆ˜ ìˆë‹¤

---

**ë‹¤ìŒ ì±•í„°**: [Ch10. Ceph ìš´ì˜ & ì„±ëŠ¥](./Ch10.Ceph_ìš´ì˜_ì„±ëŠ¥.md)ì—ì„œ í´ëŸ¬ìŠ¤í„° ë°°í¬, ëª¨ë‹ˆí„°ë§, ì„±ëŠ¥ ìµœì í™”, íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì„ í•™ìŠµí•©ë‹ˆë‹¤.
