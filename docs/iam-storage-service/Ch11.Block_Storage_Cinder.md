# Ch11. Block Storage (Cinder)

## ğŸ“‹ ê°œìš” ë° í•™ìŠµ ëª©í‘œ

### ê°œìš”

**OpenStack Cinder**ëŠ” ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ë¡œ, ê°€ìƒ ë¨¸ì‹ (VM)ì— ì—°ê²°í•  ìˆ˜ ìˆëŠ” **ì˜êµ¬ ë¸”ë¡ ë””ë°”ì´ìŠ¤(Persistent Block Device)**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. CinderëŠ” ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ(Ceph RBD, LVM, NetApp, EMC ë“±)ë¥¼ ì¶”ìƒí™”í•˜ì—¬ **í†µí•©ëœ API**ë¡œ ë³¼ë¥¨ ê´€ë¦¬ë¥¼ ì œê³µí•˜ë©°, **QoS**, **ìŠ¤ëƒ…ìƒ·**, **ë°±ì—…**, **ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜**, **ë³¼ë¥¨ ì•”í˜¸í™”** ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, CinderëŠ” **Kubernetes Operator ê¸°ë°˜ ë°°í¬**, **Active-Active HA** (Ceph RBD ë°±ì—”ë“œ), **IOPS/ëŒ€ì—­í­ QoS ì œì–´** (Rocky 13.0.0+), **ì¦ë¶„ ë°±ì—…**, **ë©€í‹° ë°±ì—”ë“œ ê´€ë¦¬**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Ceph RBDì™€ì˜ í†µí•©ì´ ê°€ì¥ ì¼ë°˜ì ì´ë©°, **Copy-on-Write(CoW) í´ë¡ **, **ì”¬ í”„ë¡œë¹„ì €ë‹**, **ìŠ¤ëƒ…ìƒ· ì§€ì›**ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ìŠ¤í† ë¦¬ì§€ í™œìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### í•™ìŠµ ëª©í‘œ

ì´ ì±•í„°ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **Cinder ì•„í‚¤í…ì²˜ ì´í•´**: API, Scheduler, Volume, Backup ì»´í¬ë„ŒíŠ¸ ì—­í•  ì„¤ëª…
2. **Volume Driver ì„¤ì •**: Ceph RBD, LVM ë°±ì—”ë“œ êµ¬ì„±
3. **QoS ì„¤ì •**: Volume Typeê³¼ QoS Specìœ¼ë¡œ IOPS/ëŒ€ì—­í­ ì œì–´
4. **Multi-backend êµ¬ì„±**: ì—¬ëŸ¬ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ ë™ì‹œ ìš´ì˜
5. **ë³¼ë¥¨ ìš´ì˜**: ë§ˆì´ê·¸ë ˆì´ì…˜, ìŠ¤ëƒ…ìƒ·, ë°±ì—…/ë³µì› ìˆ˜í–‰

---

## ğŸ”‘ í•µì‹¬ ê°œë… ë° ì´ë¡ 

### 1. Cinder ì•„í‚¤í…ì²˜

#### 1.1 ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°

```mermaid
graph TB
    subgraph "OpenStack Services"
        Nova[Nova Compute]
        CLI[OpenStack CLI]
        Horizon[Horizon Dashboard]
    end

    subgraph "Cinder Components"
        API[cinder-api<br/>RESTful API]
        Scheduler[cinder-scheduler<br/>ë°±ì—”ë“œ ì„ íƒ]
        Volume[cinder-volume<br/>ë³¼ë¥¨ ê´€ë¦¬]
        Backup[cinder-backup<br/>ë°±ì—… ê´€ë¦¬]
    end

    subgraph "Message Queue"
        RabbitMQ[RabbitMQ / Kafka]
    end

    subgraph "Database"
        DB[(MySQL / PostgreSQL)]
    end

    subgraph "Storage Backends"
        Ceph[Ceph RBD]
        LVM[LVM + iSCSI]
        NetApp[NetApp]
    end

    CLI --> API
    Horizon --> API
    Nova --> API

    API --> RabbitMQ
    Scheduler --> RabbitMQ
    Volume --> RabbitMQ
    Backup --> RabbitMQ

    API --> DB
    Scheduler --> DB
    Volume --> DB
    Backup --> DB

    Volume --> Ceph
    Volume --> LVM
    Volume --> NetApp
    Backup --> Ceph
```

**ì»´í¬ë„ŒíŠ¸ ì—­í• **:

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ë°°í¬ ìˆ˜ |
|----------|------|---------|
| **cinder-api** | RESTful API ì œê³µ, ìš”ì²­ ê²€ì¦ | ì—¬ëŸ¬ ê°œ (HA) |
| **cinder-scheduler** | ë³¼ë¥¨ ìƒì„± ìš”ì²­ ì‹œ ì ì ˆí•œ ë°±ì—”ë“œ ì„ íƒ | ì—¬ëŸ¬ ê°œ (HA) |
| **cinder-volume** | ì‹¤ì œ ë³¼ë¥¨ CRUD, ìŠ¤ëƒ…ìƒ·, í´ë¡  ìˆ˜í–‰ | ë°±ì—”ë“œë‹¹ 1ê°œ ì´ìƒ (Active-Active ê°€ëŠ¥) |
| **cinder-backup** | ë³¼ë¥¨ ë°±ì—…/ë³µì› (Swift, Ceph, NFSë¡œ) | ì—¬ëŸ¬ ê°œ (HA) |

#### 1.2 ë³¼ë¥¨ ìƒì„± íë¦„

```mermaid
sequenceDiagram
    participant User
    participant API
    participant Scheduler
    participant Volume
    participant Ceph

    User->>API: POST /v3/volumes (size=10GB, type=ceph-ssd)
    API->>DB: ë³¼ë¥¨ ë ˆì½”ë“œ ìƒì„± (status=creating)
    API->>RabbitMQ: create_volume ë©”ì‹œì§€

    RabbitMQ->>Scheduler: create_volume ë©”ì‹œì§€ ì „ë‹¬
    Scheduler->>Scheduler: í•„í„°/ê°€ì¤‘ì¹˜ ì ìš©
    Scheduler->>Scheduler: ë°±ì—”ë“œ ì„ íƒ (ceph-rbd-1)
    Scheduler->>RabbitMQ: create_volume_on_backend(ceph-rbd-1)

    RabbitMQ->>Volume: create_volume ë©”ì‹œì§€ ì „ë‹¬
    Volume->>Ceph: rbd create pool/volume-id --size 10G
    Ceph-->>Volume: ì„±ê³µ

    Volume->>DB: status=available ì—…ë°ì´íŠ¸
    Volume-->>API: ì™„ë£Œ
    API-->>User: ë³¼ë¥¨ ì •ë³´ ë°˜í™˜
```

### 2. Storage Backend ì„¤ì •

#### 2.1 Backend Isolation Model (2025)

Kubernetes í™˜ê²½ì—ì„œ Cinder OperatorëŠ” **Backend Isolation Model**ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

```yaml
apiVersion: cinder.openstack.org/v1beta1
kind: CinderVolume
metadata:
  name: cinder-volume-ceph
  namespace: openstack
spec:
  replicas: 3  # Active-Active (Ceph RBDë§Œ ì§€ì›)
  storageBackend:
    name: ceph-rbd
    type: rbd
    config:
      rbd_pool: volumes
      rbd_user: cinder
      rbd_secret_uuid: "abc-123-def"
      rbd_ceph_conf: /etc/ceph/ceph.conf
      rbd_flatten_volume_from_snapshot: false
      rbd_max_clone_depth: 5
      rbd_store_chunk_size: 4  # 4MB
```

**íŠ¹ì§•**:

- ê° ë°±ì—”ë“œê°€ ë…ë¦½ì ì¸ **StatefulSet**ìœ¼ë¡œ ë°°í¬
- í•œ ë°±ì—”ë“œì˜ ì¥ì• ê°€ ë‹¤ë¥¸ ë°±ì—”ë“œì— ì˜í–¥ ì—†ìŒ
- ë°±ì—”ë“œë³„ ë…ë¦½ì ì¸ ìŠ¤ì¼€ì¼ë§/ì—…ê·¸ë ˆì´ë“œ

#### 2.2 Ceph RBD Backend

**cinder.conf ì„¤ì •**:

```ini
[DEFAULT]
enabled_backends = ceph-rbd-ssd, ceph-rbd-hdd

[ceph-rbd-ssd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-rbd-ssd
rbd_pool = volumes-ssd
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
rbd_secret_uuid = abc-123-def-456
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4

[ceph-rbd-hdd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-rbd-hdd
rbd_pool = volumes-hdd
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
rbd_secret_uuid = abc-123-def-456
```

**ì£¼ìš” íŒŒë¼ë¯¸í„°**:

- **rbd_pool**: Ceph Pool ì´ë¦„ (volumes, volumes-ssd ë“±)
- **rbd_user**: Ceph ì¸ì¦ ì‚¬ìš©ì (keyring í•„ìš”)
- **rbd_flatten_volume_from_snapshot**: falseë¡œ ì„¤ì • ì‹œ CoW í´ë¡  ì‚¬ìš© (ë¹ ë¥¸ ìƒì„±)
- **rbd_max_clone_depth**: CoW í´ë¡  ì²´ì¸ ìµœëŒ€ ê¹Šì´ (5 ê¶Œì¥)
- **rbd_store_chunk_size**: ê°ì²´ í¬ê¸° (MB, ê¸°ë³¸ê°’ 4MB)

**Ceph ì¸ì¦ ì„¤ì •**:

```bash
# Cephì—ì„œ Cinderìš© ì‚¬ìš©ì ìƒì„±
sudo ceph auth get-or-create client.cinder \
    mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images' \
    -o /etc/ceph/ceph.client.cinder.keyring

# Nova Compute ë…¸ë“œì—ì„œë„ í•„ìš” (VM attachë¥¼ ìœ„í•´)
sudo ceph auth get-or-create client.cinder-backup \
    mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups' \
    -o /etc/ceph/ceph.client.cinder-backup.keyring
```

#### 2.3 LVM Backend

**cinder.conf ì„¤ì •**:

```ini
[lvm-backend]
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_backend_name = lvm-backend
volume_group = cinder-volumes
target_protocol = iscsi
target_helper = lioadm
iscsi_ip_address = 192.168.1.10
```

**LVM ë³¼ë¥¨ ê·¸ë£¹ ìƒì„±**:

```bash
# ë¬¼ë¦¬ ë³¼ë¥¨ ìƒì„±
sudo pvcreate /dev/sdb

# ë³¼ë¥¨ ê·¸ë£¹ ìƒì„±
sudo vgcreate cinder-volumes /dev/sdb

# í™•ì¸
sudo vgdisplay cinder-volumes
```

**íŠ¹ì§•**:

- **Local ìŠ¤í† ë¦¬ì§€**: ê° ë…¸ë“œì˜ ë¡œì»¬ ë””ìŠ¤í¬ ì‚¬ìš©
- **iSCSI ë…¸ì¶œ**: LVM ë³¼ë¥¨ì„ iSCSI íƒ€ê²Ÿìœ¼ë¡œ ë…¸ì¶œ
- **ì„±ëŠ¥**: SSD ë¡œì»¬ ë””ìŠ¤í¬ ì‚¬ìš© ì‹œ ë†’ì€ IOPS
- **ë‹¨ì **: ë…¸ë“œ ê°„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì–´ë ¤ì›€ (ë°ì´í„° ë³µì‚¬ í•„ìš”)

### 3. Volume Types & QoS

#### 3.1 Volume Type

**Volume Type**ì€ ë³¼ë¥¨ì˜ íŠ¹ì„±ì„ ì •ì˜í•˜ëŠ” í…œí”Œë¦¿ì…ë‹ˆë‹¤:

```bash
# Volume Type ìƒì„±
openstack volume type create \
    --description "Ceph SSD Storage" \
    --property volume_backend_name=ceph-rbd-ssd \
    ceph-ssd

openstack volume type create \
    --description "Ceph HDD Storage" \
    --property volume_backend_name=ceph-rbd-hdd \
    ceph-hdd

# Volume Type ëª©ë¡
openstack volume type list
```

**Extra-specs** (ë°±ì—”ë“œë³„ ì„¸ë¶€ ì„¤ì •):

```bash
# Ceph RBD ìŠ¤íŠ¸ë¼ì´í•‘ ì„¤ì •
openstack volume type set ceph-ssd \
    --property rbd_stripe_unit=4194304 \
    --property rbd_stripe_count=2

# LVM thin provisioning í™œì„±í™”
openstack volume type set lvm-thin \
    --property lvm_type=thin
```

#### 3.2 QoS Specification

**QoS Spec**ì„ í†µí•´ **IOPS/ëŒ€ì—­í­ ì œí•œ**ì„ ì„¤ì •í•©ë‹ˆë‹¤ (Rocky 13.0.0+):

```bash
# QoS Spec ìƒì„± (IOPS ì œí•œ)
openstack volume qos create \
    --consumer front-end \
    --property read_iops_sec=1000 \
    --property write_iops_sec=800 \
    --property total_iops_sec=1500 \
    standard-qos

# QoS Spec ìƒì„± (ëŒ€ì—­í­ ì œí•œ)
openstack volume qos create \
    --consumer front-end \
    --property read_bytes_sec=104857600 \   # 100 MB/s
    --property write_bytes_sec=52428800 \   # 50 MB/s
    --property total_bytes_sec=157286400 \  # 150 MB/s
    bandwidth-qos

# Burst ì§€ì› (ì¼ì‹œì ìœ¼ë¡œ ë†’ì€ IOPS í—ˆìš©)
openstack volume qos create \
    --consumer front-end \
    --property total_iops_sec=1000 \
    --property total_iops_sec_max=5000 \    # Burst IOPS
    --property total_iops_sec_max_length=60 \  # Burst ì§€ì† ì‹œê°„ (ì´ˆ)
    burst-qos

# Volume Typeì— QoS ì—°ê²°
openstack volume qos associate standard-qos ceph-ssd
```

**QoS íŒŒë¼ë¯¸í„°**:

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ë‹¨ìœ„ |
|----------|------|------|
| **read_iops_sec** | ì½ê¸° IOPS ì œí•œ | ops/sec |
| **write_iops_sec** | ì“°ê¸° IOPS ì œí•œ | ops/sec |
| **total_iops_sec** | ì „ì²´ IOPS ì œí•œ | ops/sec |
| **read_bytes_sec** | ì½ê¸° ëŒ€ì—­í­ ì œí•œ | bytes/sec |
| **write_bytes_sec** | ì“°ê¸° ëŒ€ì—­í­ ì œí•œ | bytes/sec |
| **total_bytes_sec** | ì „ì²´ ëŒ€ì—­í­ ì œí•œ | bytes/sec |
| **total_iops_sec_max** | Burst IOPS | ops/sec |
| **total_iops_sec_max_length** | Burst ì§€ì† ì‹œê°„ | seconds |

**consumer íƒ€ì…**:

- **front-end**: Hypervisor(Compute)ì—ì„œ ì ìš© (ê¶Œì¥)
- **back-end**: ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œì—ì„œ ì ìš© (ë°±ì—”ë“œ ì§€ì› í•„ìš”)

### 4. Active-Active HA

#### 4.1 Active-Active êµ¬ì„± (Ceph RBDë§Œ ì§€ì›)

**cinder.conf**:

```ini
[DEFAULT]
enabled_backends = ceph-rbd-ssd
cluster_name = cinder-cluster-1  # í´ëŸ¬ìŠ¤í„° ì´ë¦„ (etcd ë„¤ì„ìŠ¤í˜ì´ìŠ¤)

[coordination]
backend_url = etcd3+http://etcd1:2379,etcd2:2379,etcd3:2379

[ceph-rbd-ssd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-rbd-ssd
# ... (ê¸°íƒ€ RBD ì„¤ì •)
```

**íŠ¹ì§•**:

- **ë¶„ì‚° ë½**: etcdë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ cinder-volume ì¸ìŠ¤í„´ìŠ¤ê°€ ë™ì‹œì— ì‹¤í–‰
- **Ceph RBD ì „ìš©**: LVM, iSCSI ë°±ì—”ë“œëŠ” Active-Passiveë§Œ ì§€ì›
- **ì„±ëŠ¥**: ì—¬ëŸ¬ ë³¼ë¥¨ ì‘ì—…ì„ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥

**ì œì•½ ì‚¬í•­**:

- ëª¨ë“  ë°±ì—”ë“œê°€ Active-Activeë¥¼ ì§€ì›í•´ì•¼ í•¨ (í˜¼í•© ë¶ˆê°€)
- etcd í´ëŸ¬ìŠ¤í„° í•„ìš” (3-5 ë…¸ë“œ ê¶Œì¥)

### 5. ë³¼ë¥¨ ìš´ì˜

#### 5.1 ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜

**ë§ˆì´ê·¸ë ˆì´ì…˜ ìœ í˜•**:

1. **Storage-assisted Migration** (ë°±ì—”ë“œ ì§€ì›): ë°±ì—”ë“œê°€ ì§ì ‘ ë§ˆì´ê·¸ë ˆì´ì…˜ (ë¹ ë¦„, CephëŠ” ë¯¸ì§€ì›)
2. **Host-assisted Migration**: Cinderê°€ ë°ì´í„° ë³µì‚¬ (ëŠë¦¼, ëª¨ë“  ë°±ì—”ë“œ ì§€ì›)

**ë§ˆì´ê·¸ë ˆì´ì…˜ ëª…ë ¹**:

```bash
# ë³¼ë¥¨ì„ ë‹¤ë¥¸ ë°±ì—”ë“œë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
openstack volume migrate \
    --host ceph-rbd-hdd@ceph-rbd-hdd#ceph-rbd-hdd \
    <volume-id>

# ì§„í–‰ ìƒí™© í™•ì¸
openstack volume show <volume-id> | grep migration_status
```

**ì—°ê²°ëœ ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜**:

```bash
# VMì— ì—°ê²°ëœ ë³¼ë¥¨ë„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ëŠ¥ (libvirtë§Œ ì§€ì›)
openstack server migrate --live <host> <server-id>
```

#### 5.2 ìŠ¤ëƒ…ìƒ·

**Ceph RBD ìŠ¤ëƒ…ìƒ·** (CoW, ì¦‰ì‹œ ìƒì„±):

```bash
# ìŠ¤ëƒ…ìƒ· ìƒì„±
openstack volume snapshot create \
    --volume <volume-id> \
    --name my-snapshot

# ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³¼ë¥¨ ìƒì„± (CoW í´ë¡ )
openstack volume create \
    --snapshot my-snapshot \
    --size 10 \
    my-cloned-volume

# Cephì—ì„œ í™•ì¸
sudo rbd ls volumes
sudo rbd snap ls volumes/volume-abc123
```

**CoW í´ë¡  ì²´ì¸**:

```
ë³¼ë¥¨ A
  â”œâ”€ ìŠ¤ëƒ…ìƒ· A-1
  â”‚   â””â”€ ë³¼ë¥¨ B (í´ë¡ )
  â”‚       â”œâ”€ ìŠ¤ëƒ…ìƒ· B-1
  â”‚       â”‚   â””â”€ ë³¼ë¥¨ C (í´ë¡ )
  â”‚       â””â”€ ìŠ¤ëƒ…ìƒ· B-2
  â””â”€ ìŠ¤ëƒ…ìƒ· A-2
```

**rbd_max_clone_depth** ì œí•œìœ¼ë¡œ ì²´ì¸ ê¹Šì´ ì œì–´ â†’ ê¹Šì´ ì´ˆê³¼ ì‹œ ìë™ flatten

#### 5.3 ë°±ì—… & ë³µì›

**ë°±ì—… ë°±ì—”ë“œ**: Swift, Ceph, NFS ì§€ì›

**cinder-backup.conf**:

```ini
[DEFAULT]
backup_driver = cinder.backup.drivers.ceph

[ceph]
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_pool = backups
backup_ceph_chunk_size = 134217728  # 128MB
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
```

**ë°±ì—…/ë³µì›**:

```bash
# ì „ì²´ ë°±ì—…
openstack volume backup create --name backup-1 <volume-id>

# ì¦ë¶„ ë°±ì—… (ì´ì „ ë°±ì—… ì´í›„ ë³€ê²½ì‚¬í•­ë§Œ)
openstack volume backup create \
    --name backup-2 \
    --incremental \
    <volume-id>

# ë³µì›
openstack volume backup restore <backup-id> <volume-id>
```

**ë°±ì—… vs ìŠ¤ëƒ…ìƒ·**:

| íŠ¹ì„± | ìŠ¤ëƒ…ìƒ· | ë°±ì—… |
|------|--------|------|
| **ìœ„ì¹˜** | ë™ì¼ Pool (ë³¼ë¥¨ê³¼ í•¨ê»˜) | ë³„ë„ Repository (Swift, Ceph backups pool) |
| **ëª©ì ** | ë¹ ë¥¸ ë¡¤ë°±, í´ë¡  ìƒì„± | ì¥ê¸° ë³´ê´€, ì¬í•´ ë³µêµ¬ |
| **ì¦ë¶„ ì§€ì›** | âŒ | âœ… (NFS, Ceph, Swift) |
| **ì†ë„** | ë¹ ë¦„ (CoW) | ëŠë¦¼ (ë°ì´í„° ë³µì‚¬) |
| **ìš©ëŸ‰** | ì°¨ì´ë§Œ ì €ì¥ (CoW) | ì „ì²´ ë˜ëŠ” ì¦ë¶„ ì €ì¥ |

---

## ğŸ’» ì‹¤ìŠµ ê°€ì´ë“œ (Hands-on)

### Lab 1: Cinder ì„¤ì¹˜ (Ceph Backend)

**ëª©í‘œ**: Ceph RBD ë°±ì—”ë“œë¡œ Cinder êµ¬ì„±

**í™˜ê²½**:

- OpenStack: DevStack ë˜ëŠ” Kolla-Ansible
- Ceph í´ëŸ¬ìŠ¤í„° (Ch9, Ch10 ì°¸ì¡°)

**ë‹¨ê³„**:

1. **Ceph Pool ìƒì„±**:

```bash
# Volumes Pool ìƒì„±
sudo ceph osd pool create volumes 128 replicated
sudo ceph osd pool application enable volumes rbd

# Backups Pool ìƒì„±
sudo ceph osd pool create backups 64 replicated
sudo ceph osd pool application enable backups rbd

# Images Pool (Glanceìš©, ì„ íƒ)
sudo ceph osd pool create images 64 replicated
sudo ceph osd pool application enable images rbd
```

2. **Ceph ì¸ì¦ ì„¤ì •**:

```bash
# Cinder ì‚¬ìš©ì ìƒì„±
sudo ceph auth get-or-create client.cinder \
    mon 'profile rbd' \
    osd 'profile rbd pool=volumes, profile rbd pool=vms' \
    mgr 'profile rbd pool=volumes, profile rbd pool=vms' \
    -o /etc/ceph/ceph.client.cinder.keyring

# Cinder-Backup ì‚¬ìš©ì ìƒì„±
sudo ceph auth get-or-create client.cinder-backup \
    mon 'profile rbd' \
    osd 'profile rbd pool=backups' \
    mgr 'profile rbd pool=backups' \
    -o /etc/ceph/ceph.client.cinder-backup.keyring

# Nova ì‚¬ìš©ì ìƒì„± (VM attachìš©)
sudo ceph auth get-or-create client.nova \
    mon 'profile rbd' \
    osd 'profile rbd pool=volumes, profile rbd pool=vms' \
    mgr 'profile rbd pool=volumes, profile rbd pool=vms' \
    -o /etc/ceph/ceph.client.nova.keyring
```

3. **Cinder ì„¤ì •** (`/etc/cinder/cinder.conf`):

```ini
[DEFAULT]
enabled_backends = ceph-rbd
glance_api_version = 2

[ceph-rbd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-rbd
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
rbd_secret_uuid = <libvirt-secret-uuid>  # ì•„ë˜ì—ì„œ ìƒì„±
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
```

4. **Libvirt Secret ìƒì„±** (Nova Compute ë…¸ë“œ):

```bash
# Secret UUID ìƒì„±
uuidgen
# ì˜ˆì‹œ: 457eb676-33da-42ec-9a8c-9293d545c337

# Secret XML íŒŒì¼ ìƒì„±
cat > secret.xml <<EOF
<secret ephemeral='no' private='no'>
  <uuid>457eb676-33da-42ec-9a8c-9293d545c337</uuid>
  <usage type='ceph'>
    <name>client.cinder secret</name>
  </usage>
</secret>
EOF

# Libvirtì— Secret ë“±ë¡
sudo virsh secret-define --file secret.xml

# Secret ê°’ ì„¤ì • (Ceph key)
sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 \
    --base64 $(sudo ceph auth get-key client.cinder)
```

5. **Cinder ì„œë¹„ìŠ¤ ì¬ì‹œì‘**:

```bash
sudo systemctl restart cinder-volume cinder-scheduler cinder-api
```

6. **í™•ì¸**:

```bash
# ë³¼ë¥¨ ìƒì„±
openstack volume create --size 1 test-volume

# Cephì—ì„œ í™•ì¸
sudo rbd ls volumes
sudo rbd info volumes/volume-<id>
```

### Lab 2: Volume Types & QoS ì„¤ì •

**ëª©í‘œ**: ì„±ëŠ¥ ì œí•œì´ ìˆëŠ” Volume Type ìƒì„±

**ë‹¨ê³„**:

1. **Volume Type ìƒì„±**:

```bash
# Standard Tier (ì œí•œ ì—†ìŒ)
openstack volume type create \
    --property volume_backend_name=ceph-rbd \
    standard

# Performance Tier (QoS ì ìš©)
openstack volume type create \
    --property volume_backend_name=ceph-rbd \
    performance

# Economy Tier (ë‚®ì€ QoS)
openstack volume type create \
    --property volume_backend_name=ceph-rbd \
    economy
```

2. **QoS Spec ìƒì„±**:

```bash
# High Performance QoS (10000 IOPS)
openstack volume qos create \
    --consumer front-end \
    --property total_iops_sec=10000 \
    --property total_bytes_sec=1073741824 \  # 1 GB/s
    high-performance

# Standard QoS (2000 IOPS)
openstack volume qos create \
    --consumer front-end \
    --property total_iops_sec=2000 \
    --property total_bytes_sec=209715200 \  # 200 MB/s
    standard-qos

# Economy QoS (500 IOPS)
openstack volume qos create \
    --consumer front-end \
    --property total_iops_sec=500 \
    --property total_bytes_sec=52428800 \  # 50 MB/s
    economy-qos
```

3. **QoS ì—°ê²°**:

```bash
openstack volume qos associate high-performance performance
openstack volume qos associate standard-qos standard
openstack volume qos associate economy-qos economy
```

4. **í…ŒìŠ¤íŠ¸**:

```bash
# Economy ë³¼ë¥¨ ìƒì„±
openstack volume create --size 10 --type economy test-economy

# VMì— ì—°ê²°
openstack server add volume <server-id> test-economy

# VM ë‚´ë¶€ì—ì„œ FIO í…ŒìŠ¤íŠ¸
sudo fio --name=test --rw=randwrite --bs=4k --iodepth=32 \
    --numjobs=4 --size=1G --runtime=60 --filename=/dev/vdb

# ì¶œë ¥ì—ì„œ IOPS í™•ì¸ (500 IOPSì— ì œí•œë˜ì–´ì•¼ í•¨)
```

### Lab 3: Multi-backend êµ¬ì„±

**ëª©í‘œ**: SSD Poolê³¼ HDD Poolì„ ë³„ë„ ë°±ì—”ë“œë¡œ êµ¬ì„±

**ë‹¨ê³„**:

1. **Ceph Pool ìƒì„±**:

```bash
# SSD Pool (CRUSH Ruleë¡œ SSD OSDë§Œ ì‚¬ìš©)
sudo ceph osd pool create volumes-ssd 128 replicated
sudo ceph osd pool set volumes-ssd crush_rule ssd-rule  # Ch9 ì°¸ì¡°
sudo ceph osd pool application enable volumes-ssd rbd

# HDD Pool
sudo ceph osd pool create volumes-hdd 128 replicated
sudo ceph osd pool application enable volumes-hdd rbd
```

2. **cinder.conf ì—…ë°ì´íŠ¸**:

```ini
[DEFAULT]
enabled_backends = ceph-ssd, ceph-hdd

[ceph-ssd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-ssd
rbd_pool = volumes-ssd
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder

[ceph-hdd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph-hdd
rbd_pool = volumes-hdd
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
```

3. **Volume Type ìƒì„±**:

```bash
openstack volume type create \
    --property volume_backend_name=ceph-ssd \
    ssd

openstack volume type create \
    --property volume_backend_name=ceph-hdd \
    hdd
```

4. **ì‚¬ìš©**:

```bash
# SSD ë³¼ë¥¨
openstack volume create --size 10 --type ssd my-ssd-volume

# HDD ë³¼ë¥¨
openstack volume create --size 100 --type hdd my-hdd-volume

# ë°±ì—”ë“œ í™•ì¸
openstack volume show my-ssd-volume | grep os-vol-host-attr
# os-vol-host-attr:host | ceph-controller@ceph-ssd#ceph-ssd
```

### Lab 4: ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜

**ëª©í‘œ**: HDD ë°±ì—”ë“œì—ì„œ SSD ë°±ì—”ë“œë¡œ ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜

**ë‹¨ê³„**:

1. **ì´ˆê¸° ë³¼ë¥¨ ìƒì„±**:

```bash
# HDD ë³¼ë¥¨ ìƒì„±
openstack volume create --size 10 --type hdd migrate-test

# VMì— ì—°ê²° (ì„ íƒ)
openstack server add volume test-vm migrate-test
```

2. **ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰**:

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ (Host-assisted, ì—°ê²° í•´ì œ í•„ìš”)
openstack server remove volume test-vm migrate-test

openstack volume migrate \
    --host ceph-controller@ceph-ssd#ceph-ssd \
    migrate-test

# ì§„í–‰ ìƒí™© í™•ì¸
watch -n 5 'openstack volume show migrate-test | grep migration_status'
```

3. **Cephì—ì„œ í™•ì¸**:

```bash
# HDD Pool (ë³µì‚¬ ì§„í–‰ ì¤‘)
sudo rbd ls volumes-hdd

# SSD Pool (ë³µì‚¬ ì™„ë£Œ í›„)
sudo rbd ls volumes-ssd
```

4. **ì™„ë£Œ í›„**:

```bash
# Volume Typeì´ ìë™ìœ¼ë¡œ ë³€ê²½ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆ˜ë™ ë³€ê²½
openstack volume set --type ssd migrate-test

# VMì— ë‹¤ì‹œ ì—°ê²°
openstack server add volume test-vm migrate-test
```

### Lab 5: ë°±ì—… & ë³µì›

**ëª©í‘œ**: Ceph ë°±ì—”ë“œë¡œ ì¦ë¶„ ë°±ì—… ìˆ˜í–‰

**ë‹¨ê³„**:

1. **í…ŒìŠ¤íŠ¸ ë³¼ë¥¨ ìƒì„± ë° ë°ì´í„° ì“°ê¸°**:

```bash
# ë³¼ë¥¨ ìƒì„±
openstack volume create --size 5 backup-test

# VMì— ì—°ê²°
openstack server add volume test-vm backup-test

# VM ë‚´ë¶€ì—ì„œ ë°ì´í„° ì“°ê¸°
ssh user@test-vm
sudo mkfs.ext4 /dev/vdb
sudo mount /dev/vdb /mnt
echo "Initial data" | sudo tee /mnt/file1.txt
sudo umount /mnt
exit
```

2. **ì „ì²´ ë°±ì—…**:

```bash
# ë°±ì—… (VMì—ì„œ ë¶„ë¦¬ í•„ìš”)
openstack server remove volume test-vm backup-test

openstack volume backup create \
    --name backup-full-1 \
    backup-test

# ì§„í–‰ ìƒí™©
watch -n 5 'openstack volume backup show backup-full-1 | grep status'

# Cephì—ì„œ í™•ì¸
sudo rbd ls backups
```

3. **ë°ì´í„° ë³€ê²½ ë° ì¦ë¶„ ë°±ì—…**:

```bash
# ë‹¤ì‹œ ì—°ê²°í•˜ì—¬ ë°ì´í„° ì¶”ê°€
openstack server add volume test-vm backup-test

ssh user@test-vm
sudo mount /dev/vdb /mnt
echo "Updated data" | sudo tee /mnt/file2.txt
sudo umount /mnt
exit

# ì¦ë¶„ ë°±ì—…
openstack server remove volume test-vm backup-test

openstack volume backup create \
    --name backup-incremental-1 \
    --incremental \
    backup-test

# ë°±ì—… ëª©ë¡ (ì¦ë¶„ ë°±ì—…ì€ parentë¥¼ ê°€ë¦¬í‚´)
openstack volume backup list
```

4. **ë³µì›**:

```bash
# ìƒˆ ë³¼ë¥¨ìœ¼ë¡œ ë³µì›
openstack volume create --size 5 restored-volume

openstack volume backup restore \
    backup-incremental-1 \
    restored-volume

# VMì— ì—°ê²°í•˜ì—¬ í™•ì¸
openstack server add volume test-vm restored-volume

ssh user@test-vm
sudo mount /dev/vdc /mnt
ls /mnt
cat /mnt/file1.txt  # Initial data
cat /mnt/file2.txt  # Updated data
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- **Cinder Documentation (2025.1)**: [https://docs.openstack.org/cinder/2025.1/](https://docs.openstack.org/cinder/2025.1/)
- **Ceph RBD Driver**: [https://docs.openstack.org/cinder/2025.1/configuration/block-storage/drivers/ceph-rbd-volume-driver.html](https://docs.openstack.org/cinder/2025.1/configuration/block-storage/drivers/ceph-rbd-volume-driver.html)
- **Basic Volume QoS**: [https://docs.openstack.org/cinder/latest/admin/basic-volume-qos.html](https://docs.openstack.org/cinder/latest/admin/basic-volume-qos.html)
- **High Availability**: [https://docs.openstack.org/cinder/latest/contributor/high_availability.html](https://docs.openstack.org/cinder/latest/contributor/high_availability.html)
- **Migrate Volumes**: [https://docs.openstack.org/cinder/latest/admin/volume-migration.html](https://docs.openstack.org/cinder/latest/admin/volume-migration.html)
- **Backup and Restore**: [https://docs.openstack.org/cinder/latest/admin/volume-backups.html](https://docs.openstack.org/cinder/latest/admin/volume-backups.html)

### Operator & Deployment

- **Cinder Operator Storage Backend Configuration**: [https://deepwiki.com/openstack-k8s-operators/cinder-operator/4.2.4-storage-backend-configuration](https://deepwiki.com/openstack-k8s-operators/cinder-operator/4.2.4-storage-backend-configuration)
- **Kolla-Ansible Cinder Guide**: [https://docs.openstack.org/kolla-ansible/2025.1/reference/storage/cinder-guide.html](https://docs.openstack.org/kolla-ansible/2025.1/reference/storage/cinder-guide.html)

### Red Hat ë¬¸ì„œ

- **Red Hat RHOSP 17.0 Storage Guide**: [https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.0/html/storage_guide/assembly_configuring-the-block-storage-service_osp-storage-guide](https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.0/html/storage_guide/assembly_configuring-the-block-storage-service_osp-storage-guide)
- **Red Hat RHOSP 16.0 Block Storage and Volumes**: [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/storage_guide/ch-cinder](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/storage_guide/ch-cinder)
- **Red Hat Block Storage Backup Guide**: [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/block_storage_backup_guide/using-cinder-backup](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/block_storage_backup_guide/using-cinder-backup)

### ì•„í‹°í´ & ë¸”ë¡œê·¸

- **NetApp Key Concepts**: [https://netapp.github.io/openstack-deploy-ops-guide/kilo/content/section_cinder-key-concepts.html](https://netapp.github.io/openstack-deploy-ops-guide/kilo/content/section_cinder-key-concepts.html)
- **NetApp Cinder Backup and Restore**: [https://netapp.github.io/blog/2015/03/12/cinder-backup-restore-overview/](https://netapp.github.io/blog/2015/03/12/cinder-backup-restore-overview/)

### í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ í•­ëª©ì— ì²´í¬í•˜ì„¸ìš”:

- [ ] Cinder ì»´í¬ë„ŒíŠ¸(API, Scheduler, Volume, Backup)ì˜ ì—­í• ì„ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] Ceph RBD ë°±ì—”ë“œë¥¼ ì„¤ì •í•˜ê³  ë³¼ë¥¨ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤
- [ ] Volume Typeê³¼ QoS Specì„ ì‚¬ìš©í•˜ì—¬ IOPS/ëŒ€ì—­í­ì„ ì œì–´í•  ìˆ˜ ìˆë‹¤
- [ ] Multi-backend êµ¬ì„±ìœ¼ë¡œ ì—¬ëŸ¬ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œë¥¼ ë™ì‹œì— ìš´ì˜í•  ìˆ˜ ìˆë‹¤
- [ ] Active-Active HAì˜ ê°œë…ê³¼ ì œì•½ì‚¬í•­(Ceph RBD ì „ìš©)ì„ ì´í•´í–ˆë‹¤
- [ ] CoW ìŠ¤ëƒ…ìƒ·ê³¼ í´ë¡ ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‹¤
- [ ] ë³¼ë¥¨ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ìˆ˜í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆë‹¤
- [ ] ì „ì²´ ë°±ì—…ê³¼ ì¦ë¶„ ë°±ì—…ì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] ìŠ¤ëƒ…ìƒ·ê³¼ ë°±ì—…ì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] Libvirt Secretì„ ì„¤ì •í•˜ì—¬ Novaê°€ Ceph RBD ë³¼ë¥¨ì„ attachí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤

---

**ë‹¤ìŒ ì±•í„°**: [Ch12. Storage Lifecycle](./Ch12.Storage_Lifecycle.md)ì—ì„œ Storage Tiering, Lifecycle ì •ì±…, ì••ì¶•, ë¹„ìš© ìµœì í™”ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
