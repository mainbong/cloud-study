# Ch5. í´ë¼ìš°ë“œ ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§

## ğŸ“‹ ê°œìš”

í´ë¼ìš°ë“œ ì¸í”„ë¼ì˜ ì•ˆì •ì ì¸ ìš´ì˜ì„ ìœ„í•´ì„œëŠ” ì¢…í•©ì ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë³¸ ì¥ì—ì„œëŠ” Prometheusì™€ Grafanaë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘, Lokië¥¼ í™œìš©í•œ ë¡œê·¸ ê´€ë¦¬, OpenTelemetryë¥¼ í†µí•œ ë¶„ì‚° íŠ¸ë ˆì´ì‹±ê¹Œì§€ í†µí•© Observability í”Œë«í¼ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, ObservabilityëŠ” ë” ì´ìƒ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ì…ë‹ˆë‹¤. 70% ì´ìƒì˜ ê¸°ì—…ì´ Prometheusì™€ OpenTelemetryë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©°, í†µí•© í…”ë ˆë©”íŠ¸ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

1. **Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì´í•´ ë° êµ¬ì¶•**
   - Prometheus ì•„í‚¤í…ì²˜ ë° ë°ì´í„° ëª¨ë¸ ì´í•´
   - kube-prometheus-stackì„ í™œìš©í•œ Kubernetes ëª¨ë‹ˆí„°ë§
   - PromQLì„ ì‚¬ìš©í•œ ê³ ê¸‰ ì¿¼ë¦¬ ì‘ì„±

2. **Grafana ì‹œê°í™” ë° ì•ŒëŸ¬íŒ… êµ¬ì„±**
   - íš¨ê³¼ì ì¸ ëŒ€ì‹œë³´ë“œ ì„¤ê³„ (3-3-3 Rule)
   - AlertManagerë¥¼ í†µí•œ ì•ŒëŸ¬íŠ¸ ê´€ë¦¬
   - Runbook ìë™í™” ë° incident response í†µí•©

3. **ë¡œê·¸ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•**
   - Grafana Lokië¥¼ í™œìš©í•œ ê²½ëŸ‰ ë¡œê·¸ ìˆ˜ì§‘
   - LogQLì„ í†µí•œ ë¡œê·¸ ë¶„ì„
   - Promtailì„ ì‚¬ìš©í•œ ë¡œê·¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

4. **ë¶„ì‚° íŠ¸ë ˆì´ì‹± êµ¬í˜„**
   - OpenTelemetryë¥¼ í™œìš©í•œ í‘œì¤€í™”ëœ íŠ¸ë ˆì´ì‹±
   - Context propagation ë° span ê´€ë¦¬
   - Jaeger/Tempoë¥¼ í™œìš©í•œ trace ë¶„ì„

5. **í†µí•© Observability í”Œë«í¼ ìš´ì˜**
   - Metrics, Logs, Traces í†µí•© ë¶„ì„
   - Storage ë° retention ì •ì±… ìˆ˜ë¦½
   - ë³´ì•ˆ ë° ë°±ì—… ì „ëµ

---

## Part 1: Prometheus - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì˜ í‘œì¤€

### 1.1 Prometheus ì•„í‚¤í…ì²˜

PrometheusëŠ” CNCF ì¡¸ì—… í”„ë¡œì íŠ¸ë¡œ, ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ì˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸:**

- **Prometheus Server**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥ (TSDB)
- **Exporters**: ë‹¤ì–‘í•œ ì‹œìŠ¤í…œì˜ ë©”íŠ¸ë¦­ì„ Prometheus í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- **Pushgateway**: ë‹¨ê¸° ì‘ì—…ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **AlertManager**: ì•ŒëŸ¬íŠ¸ ë¼ìš°íŒ… ë° ì•Œë¦¼ ë°œì†¡

**ë°ì´í„° ëª¨ë¸:**
```
<metric_name>{<label_name>=<label_value>, ...} <value> <timestamp>

ì˜ˆì‹œ:
http_requests_total{method="GET", endpoint="/api/users", status="200"} 1027 1732435200
```

### 1.2 Kubernetes ëª¨ë‹ˆí„°ë§ - kube-prometheus-stack

2025ë…„ ê¸°ì¤€, kube-prometheus-stack Helm ì°¨íŠ¸ê°€ Kubernetes ëª¨ë‹ˆí„°ë§ì˜ de facto standardì…ë‹ˆë‹¤.

**ì„¤ì¹˜:**
```bash
# Helm ë ˆí¬ì§€í† ë¦¬ ì¶”ê°€
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# kube-prometheus-stack ì„¤ì¹˜
helm install kube-prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set grafana.adminPassword=admin123
```

**í¬í•¨ ì»´í¬ë„ŒíŠ¸:**

- Prometheus Operator
- Prometheus Server
- Grafana
- AlertManager
- Node Exporter
- kube-state-metrics
- ë‹¤ì–‘í•œ ServiceMonitor ë° PrometheusRule

### 1.3 ë ˆì´ë¸” ì„¤ê³„ Best Practices (2025)

**ì¢‹ì€ ë ˆì´ë¸” ì„¤ê³„:**
```yaml
# âœ… GOOD: ê°„ê²°í•˜ê³  ì˜ë¯¸ ìˆëŠ” ë ˆì´ë¸”
http_requests_total{
  env="prod",              # environment_production (X)
  svc="api",               # service_name (X)
  method="GET",
  status="200"
}

# âŒ BAD: ì¹´ë””ë„ë¦¬í‹°ê°€ ë†’ì€ ë ˆì´ë¸” (user_id, timestamp ë“±)
http_requests_total{
  user_id="12345",         # ìˆ˜ë°±ë§Œ ê°œì˜ ê³ ìœ  ê°’ ìƒì„±
  timestamp="1732435200"   # ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ ì‹œê³„ì—´ ìƒì„±
}
```

**ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨í™”:**

- ë ˆì´ë¸”ì€ ê°„ê²°í•˜ê²Œ (3-5ì)
- ì¹´ë””ë„ë¦¬í‹°ê°€ ë†’ì€ ê°’ì€ ë ˆì´ë¸”ë¡œ ì‚¬ìš© ê¸ˆì§€
- í™˜ê²½ë³„ retention ì •ì±… ë¶„ë¦¬

---

## Part 2: PromQL - ê°•ë ¥í•œ ì¿¼ë¦¬ ì–¸ì–´

### 2.1 PromQL ê¸°ë³¸

**ê¸°ë³¸ ì¿¼ë¦¬:**
```promql
# íŠ¹ì • ë©”íŠ¸ë¦­ ì¡°íšŒ
http_requests_total

# ë ˆì´ë¸” í•„í„°ë§
http_requests_total{method="GET", status="200"}

# ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
http_requests_total{endpoint=~"/api/.*"}

# ë ˆì´ë¸” ê°’ ì œì™¸
http_requests_total{status!="500"}
```

**Range Vector ë° í•¨ìˆ˜:**
```promql
# ìµœê·¼ 5ë¶„ê°„ ì´ˆë‹¹ ìš”ì²­ë¥  (Rate)
rate(http_requests_total[5m])

# 5ë¶„ê°„ í‰ê·  ì¦ê°€ëŸ‰
increase(http_requests_total[5m])

# ìˆœê°„ ë²¡í„°ì˜ í•©ê³„
sum(rate(http_requests_total[5m]))

# ë ˆì´ë¸”ë³„ ê·¸ë£¹í™”
sum by (status) (rate(http_requests_total[5m]))
```

### 2.2 ì‹¤ì „ PromQL ì¿¼ë¦¬

**CPU ì‚¬ìš©ë¥  ê³„ì‚°:**
```promql
# ì»¨í…Œì´ë„ˆ CPU ì‚¬ìš©ë¥  (%)
100 * (
  sum by (pod, namespace) (rate(container_cpu_usage_seconds_total[5m]))
  /
  sum by (pod, namespace) (container_spec_cpu_quota / container_spec_cpu_period)
)

# Node CPU ì‚¬ìš©ë¥ 
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ :**
```promql
# ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
100 * (
  container_memory_working_set_bytes
  /
  container_spec_memory_limit_bytes
)

# Node ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))
```

**ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡:**
```promql
# í˜„ì¬ ì¶”ì„¸ë¡œ ë””ìŠ¤í¬ê°€ ê°€ë“ ì°° ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
predict_linear(node_filesystem_avail_bytes{mountpoint="/"}[1h], 4*3600) < 0
```

**HTTP ì—ëŸ¬ìœ¨:**
```promql
# 5ë¶„ê°„ 5xx ì—ëŸ¬ìœ¨ (%)
100 * (
  sum(rate(http_requests_total{status=~"5.."}[5m]))
  /
  sum(rate(http_requests_total[5m]))
)
```

### 2.3 Four Golden Signals ëª¨ë‹ˆí„°ë§

**1. Latency (ì§€ì—°ì‹œê°„):**
```promql
# P95 ë ˆì´í„´ì‹œ
histogram_quantile(0.95,
  sum by (le) (rate(http_request_duration_seconds_bucket[5m]))
)
```

**2. Traffic (íŠ¸ë˜í”½):**
```promql
# ì´ˆë‹¹ ìš”ì²­ ìˆ˜
sum(rate(http_requests_total[5m]))
```

**3. Errors (ì—ëŸ¬):**
```promql
# ì—ëŸ¬ìœ¨
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))
```

**4. Saturation (í¬í™”ë„):**
```promql
# CPU í¬í™”ë„
avg(node_load5) / count(node_cpu_seconds_total{mode="idle"})
```

---

## Part 3: Grafana - ì‹œê°í™” ë° ì•ŒëŸ¬íŒ…

### 3.1 Grafana ëŒ€ì‹œë³´ë“œ ì„¤ê³„ - 3-3-3 Rule (2025)

2025ë…„ best practiceë¡œ ìë¦¬ì¡ì€ **3-3-3 Rule**:
- **3 rows** of panels
- **3 panels** per row
- **3 key metrics** per panel

**ëŒ€ì‹œë³´ë“œ êµ¬ì„± ì˜ˆì‹œ:**
```
Row 1: Overview
â”œâ”€â”€ Panel 1: Total Requests (current, trend)
â”œâ”€â”€ Panel 2: Error Rate (current, threshold)
â””â”€â”€ Panel 3: Avg Response Time (P50, P95, P99)

Row 2: Resource Usage
â”œâ”€â”€ Panel 1: CPU Usage (by pod)
â”œâ”€â”€ Panel 2: Memory Usage (by pod)
â””â”€â”€ Panel 3: Network I/O (ingress/egress)

Row 3: Application Metrics
â”œâ”€â”€ Panel 1: Active Connections
â”œâ”€â”€ Panel 2: Database Query Time
â””â”€â”€ Panel 3: Cache Hit Rate
```

### 3.2 ëŒ€ì‹œë³´ë“œ as Code

**Grafana Dashboard JSON:**
```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "tags": ["kubernetes", "infrastructure"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      }
    ]
  }
}
```

**Terraformìœ¼ë¡œ ëŒ€ì‹œë³´ë“œ ë°°í¬:**
```hcl
resource "grafana_dashboard" "kubernetes_overview" {
  config_json = file("${path.module}/dashboards/k8s-overview.json")

  folder = grafana_folder.infrastructure.id
}

resource "grafana_folder" "infrastructure" {
  title = "Infrastructure"
}
```

### 3.3 AlertManager ì„¤ì •

**PrometheusRule CRD:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-alerts
  namespace: monitoring
spec:
  groups:
    - name: kubernetes.rules
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Pod has restarted {{ $value }} times in the last 15 minutes"
            runbook_url: "https://runbooks.example.com/pod-crash-loop"

        - alert: HighMemoryUsage
          expr: |
            100 * (container_memory_working_set_bytes / container_spec_memory_limit_bytes) > 90
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Container memory usage is above 90%"
            description: "Container {{ $labels.pod }}/{{ $labels.container }} memory usage is {{ $value }}%"
            runbook_url: "https://runbooks.example.com/high-memory"

        - alert: DiskSpaceLow
          expr: |
            100 * (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 10
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Disk space is below 10%"
            description: "Node {{ $labels.instance }} has only {{ $value }}% disk space remaining"
```

**AlertManager ë¼ìš°íŒ…:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/xxx'

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
        - match:
            severity: critical
          receiver: 'pagerduty'
          continue: true

        - match:
            severity: warning
          receiver: 'slack'

    receivers:
      - name: 'default'
        slack_configs:
          - channel: '#alerts'
            title: '{{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

      - name: 'pagerduty'
        pagerduty_configs:
          - service_key: 'YOUR_PAGERDUTY_KEY'

      - name: 'slack'
        slack_configs:
          - channel: '#warnings'
```

---

## Part 4: Grafana Loki - ë¡œê·¸ ê´€ë¦¬

### 4.1 Loki ì•„í‚¤í…ì²˜

Grafana LokiëŠ” "Prometheus for logs"ë¡œ ì„¤ê³„ëœ ê²½ëŸ‰ ë¡œê·¸ ì§‘ê³„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**Loki vs ELK Stack (2025):**
| íŠ¹ì§• | Loki | ELK Stack |
|------|------|-----------|
| ì¸ë±ì‹± | ë ˆì´ë¸”ë§Œ ì¸ë±ì‹± | ì „ì²´ ë¡œê·¸ ì¸ë±ì‹± |
| ìŠ¤í† ë¦¬ì§€ | ë‚®ìŒ (50% ì ˆì•½) | ë†’ìŒ |
| ì¿¼ë¦¬ ì†ë„ | ë¹ ë¦„ (ë ˆì´ë¸” ê¸°ë°˜) | ë§¤ìš° ë¹ ë¦„ (ì „ë¬¸ ê²€ìƒ‰) |
| ì„¤ì • ë³µì¡ë„ | ë‚®ìŒ | ë†’ìŒ |
| Grafana í†µí•© | ë„¤ì´í‹°ë¸Œ | í”ŒëŸ¬ê·¸ì¸ í•„ìš” |

**Loki ì„¤ì¹˜:**
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack \
  --namespace monitoring \
  --set grafana.enabled=false \
  --set promtail.enabled=true \
  --set loki.persistence.enabled=true \
  --set loki.persistence.size=100Gi
```

### 4.2 Promtail ì„¤ì •

**Promtail ConfigMap:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: monitoring
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
      - url: http://loki:3100/loki/api/v1/push

    scrape_configs:
      # Kubernetes pod logs
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container

        pipeline_stages:
          # JSON ë¡œê·¸ íŒŒì‹±
          - json:
              expressions:
                level: level
                msg: message
                timestamp: time

          # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
          - timestamp:
              source: timestamp
              format: RFC3339

          # ë ˆì´ë¸” ì¶”ì¶œ
          - labels:
              level:
```

### 4.3 LogQL ì¿¼ë¦¬

**ê¸°ë³¸ ì¿¼ë¦¬:**
```logql
# íŠ¹ì • namespaceì˜ ë¡œê·¸
{namespace="production"}

# ì—¬ëŸ¬ ì¡°ê±´
{namespace="production", container="api"}

# ì •ê·œí‘œí˜„ì‹
{pod=~"api-.*"}

# ë¡œê·¸ ë‚´ìš© í•„í„°ë§
{namespace="production"} |= "error"

# ì •ê·œí‘œí˜„ì‹ í•„í„°
{namespace="production"} |~ "error|ERROR|Error"
```

**ê³ ê¸‰ ì¿¼ë¦¬:**
```logql
# JSON í•„ë“œ ì¶”ì¶œ ë° í•„í„°ë§
{namespace="production"}
  | json
  | level="error"
  | line_format "{{.timestamp}} {{.message}}"

# ë©”íŠ¸ë¦­ ìƒì„± - ì´ˆë‹¹ ì—ëŸ¬ ë¡œê·¸ ìˆ˜
sum(rate({namespace="production"} |= "error" [5m])) by (pod)

# Quantile ê³„ì‚°
quantile_over_time(0.99,
  {namespace="production"}
    | json
    | unwrap duration [5m]
) by (endpoint)
```

**í†µí•© ì¿¼ë¦¬ (Metrics + Logs):**
```logql
# Grafanaì—ì„œ ë©”íŠ¸ë¦­ê³¼ ë¡œê·¸ í•¨ê»˜ ë³´ê¸°
# Panel 1: Error rate (Prometheus)
sum(rate(http_requests_total{status="500"}[5m]))

# Panel 2: Error logs (Loki)
{namespace="production", status="500"} | json
```

---

## Part 5: OpenTelemetry - ë¶„ì‚° íŠ¸ë ˆì´ì‹±

### 5.1 OpenTelemetry ì•„í‚¤í…ì²˜

OpenTelemetryëŠ” 2025ë…„ í˜„ì¬ Observabilityì˜ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤. CNCFì˜ ë‘ ë²ˆì§¸ë¡œ í™œë°œí•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸:**

- **SDK**: ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸¡ (12+ ì–¸ì–´ ì§€ì›)
- **Collector**: í…”ë ˆë©”íŠ¸ë¦¬ ë°ì´í„° ìˆ˜ì§‘/ì²˜ë¦¬/ì „ì†¡
- **Exporter**: ë‹¤ì–‘í•œ ë°±ì—”ë“œë¡œ ë°ì´í„° ì „ì†¡

**ë°ì´í„° íƒ€ì…:**

- **Traces**: ë¶„ì‚° íŠ¸ëœì­ì…˜ ì¶”ì 
- **Metrics**: ì‹œê³„ì—´ ë°ì´í„° (Prometheus í˜¸í™˜)
- **Logs**: êµ¬ì¡°í™”ëœ ë¡œê·¸

### 5.2 OpenTelemetry Collector ë°°í¬

**Kubernetes DaemonSet:**
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.91.0
          ports:
            - containerPort: 4317  # OTLP gRPC
            - containerPort: 4318  # OTLP HTTP
            - containerPort: 8888  # Prometheus metrics
          volumeMounts:
            - name: config
              mountPath: /etc/otel-collector-config.yaml
              subPath: otel-collector-config.yaml
      volumes:
        - name: config
          configMap:
            name: otel-collector-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: monitoring
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 10s
              static_configs:
                - targets: ['0.0.0.0:8888']

    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024

      memory_limiter:
        check_interval: 1s
        limit_mib: 512

      # Sampling (10% of traces)
      probabilistic_sampler:
        sampling_percentage: 10

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"

      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true

      loki:
        endpoint: http://loki:3100/loki/api/v1/push

      logging:
        loglevel: debug

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, probabilistic_sampler]
          exporters: [jaeger, logging]

        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus]

        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [loki]
```

### 5.3 ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸¡

**Python (FastAPI) ì˜ˆì œ:**
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.sdk.resources import Resource
from fastapi import FastAPI

# Resource ì •ì˜
resource = Resource(attributes={
    "service.name": "api-service",
    "service.version": "1.0.0",
    "deployment.environment": "production"
})

# Tracer Provider ì„¤ì •
trace.set_tracer_provider(TracerProvider(resource=resource))
tracer = trace.get_tracer(__name__)

# OTLP Exporter ì„¤ì •
otlp_exporter = OTLPSpanExporter(
    endpoint="http://otel-collector:4317",
    insecure=True
)

# Span Processor ì¶”ê°€
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

app = FastAPI()

# FastAPI ìë™ ê³„ì¸¡
FastAPIInstrumentor.instrument_app(app)

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    # Custom span ìƒì„±
    with tracer.start_as_current_span("fetch_user_from_db") as span:
        span.set_attribute("user.id", user_id)
        span.set_attribute("db.system", "postgresql")

        # DB ì¡°íšŒ ë¡œì§
        user = await fetch_user_from_db(user_id)

        span.set_attribute("user.found", user is not None)

        if not user:
            span.set_status(Status(StatusCode.ERROR, "User not found"))

        return user

async def fetch_user_from_db(user_id: int):
    # Nested span
    with tracer.start_as_current_span("db_query") as span:
        span.set_attribute("db.statement", f"SELECT * FROM users WHERE id = {user_id}")
        # ... DB ì¿¼ë¦¬ ì‹¤í–‰
        return {"id": user_id, "name": "John Doe"}
```

**Go ì˜ˆì œ:**
```go
package main

import (
    "context"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
    "go.opentelemetry.io/otel/trace"
    "google.golang.org/grpc"
)

func initTracer() (*sdktrace.TracerProvider, error) {
    ctx := context.Background()

    // OTLP exporter
    exporter, err := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint("otel-collector:4317"),
        otlptracegrpc.WithInsecure(),
    )
    if err != nil {
        return nil, err
    }

    // Resource
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName("go-service"),
            semconv.ServiceVersion("1.0.0"),
        ),
    )
    if err != nil {
        return nil, err
    }

    // TracerProvider
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithResource(res),
        sdktrace.WithSampler(sdktrace.TraceIDRatioBased(0.1)), // 10% sampling
    )

    otel.SetTracerProvider(tp)
    return tp, nil
}

func processOrder(ctx context.Context, orderID string) error {
    tracer := otel.Tracer("order-processor")

    ctx, span := tracer.Start(ctx, "processOrder")
    defer span.End()

    span.SetAttributes(
        attribute.String("order.id", orderID),
    )

    // Nested span
    if err := validateOrder(ctx, orderID); err != nil {
        span.RecordError(err)
        return err
    }

    if err := chargePayment(ctx, orderID); err != nil {
        span.RecordError(err)
        return err
    }

    return nil
}

func validateOrder(ctx context.Context, orderID string) error {
    _, span := otel.Tracer("order-processor").Start(ctx, "validateOrder")
    defer span.End()

    // Validation logic
    return nil
}
```

### 5.4 Context Propagation

**HTTP Headers (W3C Trace Context):**
```
traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
tracestate: congo=t61rcWkgMzE,rojo=00f067aa0ba902b7
```

**ë¶„ì‚° íŠ¸ë ˆì´ì‹± í”Œë¡œìš°:**
```
Client Request
    â””â”€> API Gateway (Span 1)
        â”œâ”€> Auth Service (Span 2)
        â”‚   â””â”€> Database (Span 3)
        â””â”€> Order Service (Span 4)
            â”œâ”€> Payment Service (Span 5)
            â”‚   â””â”€> External API (Span 6)
            â””â”€> Inventory Service (Span 7)
                â””â”€> Cache (Span 8)
```

---

## Part 6: í†µí•© Observability ì „ëµ

### 6.1 Layered Monitoring Architecture (2025 Best Practice)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Global Prometheus                      â”‚
â”‚  (Aggregated critical metrics across clusters)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster Prometheusâ”‚      â”‚ Cluster Prometheus â”‚
â”‚   (Cluster A)     â”‚      â”‚   (Cluster B)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                            â–²
        â”‚                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Prometheus â”‚          â”‚ App Prometheus â”‚
â”‚ (Business Logic)â”‚         â”‚ (Business Logic)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Storage & Retention ì „ëµ

**Prometheus Retention:**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# TSDB ì„¤ì •
storage:
  tsdb:
    retention.time: 15d        # 15ì¼ ë³´ê´€
    retention.size: 50GB       # ìµœëŒ€ 50GB

    # Compaction ì„¤ì •
    min-block-duration: 2h
    max-block-duration: 36h

# Remote Write (ì¥ê¸° ì €ì¥)
remote_write:
  - url: "http://thanos-receive:19291/api/v1/receive"
    queue_config:
      capacity: 10000
      max_shards: 50
```

**Thanosë¥¼ í™œìš©í•œ ì¥ê¸° ì €ì¥:**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-store
spec:
  serviceName: thanos-store
  replicas: 1
  template:
    spec:
      containers:
        - name: thanos
          image: quay.io/thanos/thanos:v0.33.0
          args:
            - store
            - --data-dir=/var/thanos/store
            - --objstore.config-file=/etc/thanos/objstore.yml
          volumeMounts:
            - name: object-storage-config
              mountPath: /etc/thanos
      volumes:
        - name: object-storage-config
          secret:
            secretName: thanos-objstore-config
---
apiVersion: v1
kind: Secret
metadata:
  name: thanos-objstore-config
stringData:
  objstore.yml: |
    type: S3
    config:
      bucket: "thanos-metrics"
      endpoint: "s3.amazonaws.com"
      access_key: "XXX"
      secret_key: "YYY"
```

### 6.3 ë³´ì•ˆ ì„¤ì •

**Prometheus TLS ë° ì¸ì¦:**
```yaml
# prometheus-additional-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-additional-config
  namespace: monitoring
stringData:
  prometheus-additional.yaml: |
    # Basic Auth
    basic_auth_users:
      admin: $2y$10$...  # bcrypt hash

    # TLS ì„¤ì •
    tls_server_config:
      cert_file: /etc/prometheus/tls/tls.crt
      key_file: /etc/prometheus/tls/tls.key
      client_ca_file: /etc/prometheus/tls/ca.crt
      client_auth_type: RequireAndVerifyClientCert
```

**Grafana OAuth í†µí•© (Keycloak):**
```ini
# grafana.ini
[auth.generic_oauth]
enabled = true
name = Keycloak
allow_sign_up = true
client_id = grafana
client_secret = YOUR_SECRET
scopes = openid profile email
auth_url = https://keycloak.example.com/auth/realms/master/protocol/openid-connect/auth
token_url = https://keycloak.example.com/auth/realms/master/protocol/openid-connect/token
api_url = https://keycloak.example.com/auth/realms/master/protocol/openid-connect/userinfo
role_attribute_path = contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'
```

---

## ğŸ› ï¸ ì‹¤ìŠµ ê°€ì´ë“œ

### ì‹¤ìŠµ 1: kube-prometheus-stack ë°°í¬ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

**ëª©í‘œ:** Kubernetes í´ëŸ¬ìŠ¤í„°ì— ì™„ì „í•œ ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬

**ë‹¨ê³„:**
```bash
# 1. Helm ì°¨íŠ¸ values íŒŒì¼ ì¤€ë¹„
cat <<EOF > values.yaml
prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    # ì™¸ë¶€ ë ˆì´ë¸” ì¶”ê°€
    externalLabels:
      cluster: prod-cluster-01
      region: ap-northeast-2

    # ServiceMonitor ìë™ ë°œê²¬
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false

grafana:
  adminPassword: "ChangeMe123!"
  persistence:
    enabled: true
    size: 10Gi

  # í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜
  plugins:
    - grafana-piechart-panel
    - grafana-clock-panel

  # ë°ì´í„°ì†ŒìŠ¤ ìë™ êµ¬ì„±
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://kube-prometheus-prometheus:9090
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki:3100

alertmanager:
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname']
      receiver: 'slack'
    receivers:
      - name: 'slack'
        slack_configs:
          - api_url: 'YOUR_SLACK_WEBHOOK'
            channel: '#alerts'
EOF

# 2. ì„¤ì¹˜
helm install kube-prometheus prometheus-community/kube-prometheus-stack \
  -f values.yaml \
  --namespace monitoring \
  --create-namespace

# 3. Port-forwardë¡œ ì ‘ì† í™•ì¸
kubectl port-forward -n monitoring svc/kube-prometheus-grafana 3000:80
# http://localhost:3000 (admin/ChangeMe123!)
```

### ì‹¤ìŠµ 2: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ServiceMonitor)

**ì‹œë‚˜ë¦¬ì˜¤:** FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# app.py
from fastapi import FastAPI
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import Response
import time

app = FastAPI()

# ë©”íŠ¸ë¦­ ì •ì˜
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time

    http_requests_total.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    http_request_duration_seconds.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)

    return response

@app.get("/metrics")
def metrics():
    return Response(generate_latest(), media_type="text/plain")

@app.get("/api/users")
def get_users():
    return {"users": []}
```

**ServiceMonitor ìƒì„±:**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: fastapi-app
  namespace: default
  labels:
    app: fastapi
spec:
  selector:
    matchLabels:
      app: fastapi
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
```

### ì‹¤ìŠµ 3: Grafana Loki + Promtail ë¡œê·¸ ìˆ˜ì§‘

```bash
# Loki Stack ì„¤ì¹˜
helm install loki grafana/loki-stack \
  --namespace monitoring \
  --set grafana.enabled=false \
  --set promtail.enabled=true

# Grafanaì— Loki ë°ì´í„°ì†ŒìŠ¤ ì¶”ê°€ (ì´ë¯¸ valuesì— ìˆìœ¼ë©´ skip)
# Grafana UI -> Configuration -> Data Sources -> Add Loki
# URL: http://loki:3100

# ë¡œê·¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
# Grafana Exploreì—ì„œ:
{namespace="default"} |= "error" | json | line_format "{{.level}} {{.msg}}"
```

### ì‹¤ìŠµ 4: OpenTelemetryë¡œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íŠ¸ë ˆì´ì‹±

**1. Jaeger ì„¤ì¹˜:**
```bash
kubectl create namespace observability
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/crds/jaegertracing.io_jaegers_crd.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/service_account.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role_binding.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/operator.yaml

# Jaeger ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
cat <<EOF | kubectl apply -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: observability
spec:
  strategy: allInOne
  allInOne:
    image: jaegertracing/all-in-one:latest
    options:
      log-level: info
  storage:
    type: memory
  ingress:
    enabled: false
  ui:
    options:
      dependencies:
        menuEnabled: true
EOF
```

**2. OpenTelemetry Collector ë°°í¬:** (ìœ„ì˜ Part 5.2 YAML ì‚¬ìš©)

**3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traced-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: traced-app
  template:
    metadata:
      labels:
        app: traced-app
    spec:
      containers:
        - name: app
          image: your-traced-app:latest
          env:
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://otel-collector:4317"
            - name: OTEL_SERVICE_NAME
              value: "traced-app"
            - name: OTEL_TRACES_SAMPLER
              value: "parentbased_traceidratio"
            - name: OTEL_TRACES_SAMPLER_ARG
              value: "0.1"
```

---

## ğŸ’» ì˜ˆì œ ì½”ë“œ

### ì˜ˆì œ 1: ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (Terraform)

```hcl
# grafana.tf
terraform {
  required_providers {
    grafana = {
      source  = "grafana/grafana"
      version = "~> 2.0"
    }
  }
}

provider "grafana" {
  url  = "http://grafana.example.com"
  auth = var.grafana_api_key
}

# í´ë” ìƒì„±
resource "grafana_folder" "kubernetes" {
  title = "Kubernetes"
}

# Prometheus ë°ì´í„°ì†ŒìŠ¤
resource "grafana_data_source" "prometheus" {
  type = "prometheus"
  name = "Prometheus"
  url  = "http://prometheus:9090"

  json_data_encoded = jsonencode({
    httpMethod    = "POST"
    timeInterval  = "30s"
  })
}

# ëŒ€ì‹œë³´ë“œ
resource "grafana_dashboard" "cluster_overview" {
  folder      = grafana_folder.kubernetes.id
  config_json = file("${path.module}/dashboards/cluster-overview.json")
}

# Alert ë£°
resource "grafana_rule_group" "kubernetes_alerts" {
  name             = "kubernetes-alerts"
  folder_uid       = grafana_folder.kubernetes.uid
  interval_seconds = 60

  rule {
    name      = "PodCrashLooping"
    condition = "C"

    data {
      ref_id = "A"

      relative_time_range {
        from = 600
        to   = 0
      }

      datasource_uid = grafana_data_source.prometheus.uid

      model = jsonencode({
        expr = "rate(kube_pod_container_status_restarts_total[15m]) > 0"
      })
    }

    data {
      ref_id = "C"

      relative_time_range {
        from = 600
        to   = 0
      }

      datasource_uid = "-100"

      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [0]
              type   = "gt"
            }
            operator = {
              type = "and"
            }
            query = {
              params = ["A"]
            }
            type = "query"
          }
        ]
      })
    }

    no_data_state  = "NoData"
    exec_err_state = "Alerting"
    for            = "5m"

    annotations = {
      description = "Pod has restarted multiple times"
      runbook_url = "https://runbooks.example.com/pod-crash"
    }

    labels = {
      severity = "critical"
    }
  }
}
```

### ì˜ˆì œ 2: ë©€í‹°í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Thanos)

```yaml
# thanos-sidecar.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus
  namespace: monitoring
spec:
  serviceName: prometheus
  replicas: 2
  template:
    spec:
      containers:
        # Prometheus
        - name: prometheus
          image: prom/prometheus:v2.48.0
          args:
            - --config.file=/etc/prometheus/prometheus.yml
            - --storage.tsdb.path=/prometheus
            - --storage.tsdb.min-block-duration=2h
            - --storage.tsdb.max-block-duration=2h
            - --web.enable-lifecycle
          volumeMounts:
            - name: storage
              mountPath: /prometheus

        # Thanos Sidecar
        - name: thanos-sidecar
          image: quay.io/thanos/thanos:v0.33.0
          args:
            - sidecar
            - --prometheus.url=http://localhost:9090
            - --tsdb.path=/prometheus
            - --objstore.config-file=/etc/thanos/objstore.yml
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
          volumeMounts:
            - name: storage
              mountPath: /prometheus
            - name: thanos-objstore
              mountPath: /etc/thanos
      volumes:
        - name: thanos-objstore
          secret:
            secretName: thanos-objstore-config
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 100Gi
---
# thanos-querier.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-querier
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: thanos-querier
  template:
    metadata:
      labels:
        app: thanos-querier
    spec:
      containers:
        - name: thanos
          image: quay.io/thanos/thanos:v0.33.0
          args:
            - query
            - --http-address=0.0.0.0:10902
            - --grpc-address=0.0.0.0:10901
            - --store=dnssrv+_grpc._tcp.prometheus-0.prometheus.monitoring.svc.cluster.local
            - --store=dnssrv+_grpc._tcp.prometheus-1.prometheus.monitoring.svc.cluster.local
            - --store=dnssrv+_grpc._tcp.thanos-store.monitoring.svc.cluster.local
```

### ì˜ˆì œ 3: í†µí•© Observability with OpenTelemetry

```yaml
# otel-demo-app.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:

    processors:
      batch:

      # ë©”íŠ¸ë¦­ì—ì„œ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
      spanmetrics:
        metrics_exporter: prometheus
        latency_histogram_buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
        dimensions:
          - name: http.method
          - name: http.status_code

      # ë¦¬ì†ŒìŠ¤ ì†ì„± ì¶”ê°€
      resource:
        attributes:
          - key: environment
            value: production
            action: insert

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"

      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true

      loki:
        endpoint: http://loki:3100/loki/api/v1/push
        labels:
          attributes:
            service.name: "service_name"
            level: "severity"

    connectors:
      spanmetrics:

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch, resource]
          exporters: [jaeger, spanmetrics]

        metrics:
          receivers: [otlp, spanmetrics]
          processors: [batch]
          exporters: [prometheus]

        logs:
          receivers: [otlp]
          processors: [batch, resource]
          exporters: [loki]
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
1. **Prometheus**
   - [Prometheus ê³µì‹ ë¬¸ì„œ](https://prometheus.io/docs/)
   - [PromQL ê°€ì´ë“œ](https://prometheus.io/docs/prometheus/latest/querying/basics/)
   - [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)

2. **Grafana**
   - [Grafana ê³µì‹ ë¬¸ì„œ](https://grafana.com/docs/)
   - [Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)
   - [Grafana Loki](https://grafana.com/docs/loki/latest/)

3. **OpenTelemetry**
   - [OpenTelemetry ê³µì‹ ì‚¬ì´íŠ¸](https://opentelemetry.io/)
   - [Collector ë¬¸ì„œ](https://opentelemetry.io/docs/collector/)
   - [Instrumentation ê°€ì´ë“œ](https://opentelemetry.io/docs/instrumentation/)

4. **Thanos**
   - [Thanos ê³µì‹ ë¬¸ì„œ](https://thanos.io/tip/thanos/quick-tutorial.md/)

### 2025 Best Practices ì•„í‹°í´
1. [Prometheus Monitoring: Complete Setup & Best Practices](https://www.glukhov.org/post/2025/11/monitoring-with-prometheus/)
2. [How SREs Use Prometheus and Grafana to Crush MTTR in 2025](https://rootly.com/sre/how-sres-use-prometheus-and-grafana-to-crush-mttr-in-2025)
3. [Backend Observability in 2025: Distributed Tracing with OpenTelemetry](https://medium.com/@shbhggrwl/backend-observability-in-2025-distributed-tracing-with-opentelemetry-af338a987abb)
4. [Kubernetes Monitoring in 2025: A Complete Guide](https://linuxcloudservers.com/kubernetes-monitoring/)

### íŠœí† ë¦¬ì–¼ ë° ì˜ˆì œ
1. [Get started with Grafana and Prometheus](https://grafana.com/docs/grafana/latest/getting-started/get-started-grafana-prometheus/)
2. [OpenTelemetry Distributed Tracing Tutorial](https://www.withcoherence.com/articles/opentelemetry-distributed-tracing-tutorial-and-best-practices)
3. [Grafana Loki Log Aggregation](https://grafana.com/docs/loki/latest/get-started/)

### ì»¤ë®¤ë‹ˆí‹° ë° ë„êµ¬
1. **Awesome Prometheus**: https://github.com/roaldnefs/awesome-prometheus
2. **Grafana Dashboards**: https://grafana.com/grafana/dashboards/
3. **PromLens** (PromQL Query Builder): https://promlens.com/

---

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ ê°œë…
- [ ] Prometheusì˜ pull ê¸°ë°˜ ì•„í‚¤í…ì²˜ ì´í•´
- [ ] TSDBì™€ ì‹œê³„ì—´ ë°ì´í„° ëª¨ë¸ ì´í•´
- [ ] ë ˆì´ë¸”ê³¼ ì¹´ë””ë„ë¦¬í‹°ì˜ ê´€ê³„ ì´í•´
- [ ] Four Golden Signals ê°œë… ìˆ™ì§€

### Prometheus & PromQL
- [ ] kube-prometheus-stack ë°°í¬ ë° ì„¤ì •
- [ ] ServiceMonitor/PodMonitorë¡œ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] PromQL ê¸°ë³¸ ì¿¼ë¦¬ ì‘ì„± (rate, sum, avg, quantile)
- [ ] Recording Rulesì™€ Alerting Rules ì‘ì„±
- [ ] Prometheus federation ë˜ëŠ” Thanos êµ¬ì„±

### Grafana
- [ ] ë°ì´í„°ì†ŒìŠ¤ ì—°ê²° (Prometheus, Loki)
- [ ] 3-3-3 Ruleì„ ì ìš©í•œ ëŒ€ì‹œë³´ë“œ ì„¤ê³„
- [ ] ë³€ìˆ˜(Variables)ë¥¼ í™œìš©í•œ ë™ì  ëŒ€ì‹œë³´ë“œ ì‘ì„±
- [ ] AlertManager í†µí•© ë° ì•Œë¦¼ ì±„ë„ ì„¤ì •
- [ ] Dashboard as Code (Terraform/jsonnet)

### Logging (Loki)
- [ ] Loki + Promtail ì„¤ì¹˜ ë° ì„¤ì •
- [ ] LogQL ì¿¼ë¦¬ ì‘ì„± (í•„í„°, íŒŒì‹±, ë©”íŠ¸ë¦­ ì¶”ì¶œ)
- [ ] Lokiì™€ Prometheus ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„ ë¶„ì„
- [ ] ë¡œê·¸ retention ì •ì±… ì„¤ì •

### Distributed Tracing (OpenTelemetry)
- [ ] OpenTelemetry Collector ë°°í¬
- [ ] Python/Go ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸¡ (auto & manual)
- [ ] Trace context propagation ì´í•´
- [ ] Jaeger/Tempoì—ì„œ trace ë¶„ì„
- [ ] Span metrics í™œìš©

### í†µí•© ë° ìš´ì˜
- [ ] Layered monitoring architecture ì„¤ê³„
- [ ] Storage ë° retention ì „ëµ ìˆ˜ë¦½
- [ ] TLS ë° ì¸ì¦ ì„¤ì •
- [ ] Backup ë° disaster recovery ê³„íš
- [ ] SLO/SLI ì •ì˜ ë° ëª¨ë‹ˆí„°ë§

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **[Ch6.Linux_OS_ì´ë¯¸ì§€.md](./Ch6.Linux_OS_ì´ë¯¸ì§€.md)**
   - OS ì´ë¯¸ì§€ êµ¬ì¡° ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•
   - Packerë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ë¹Œë“œ íŒŒì´í”„ë¼ì¸
   - ì´ë¯¸ì§€ ë””ë²„ê¹… ë° ìµœì í™”

2. **ì‹¬í™” ì£¼ì œ**
   - **SLO/SLI/SLA Engineering**: Error Budget ê¸°ë°˜ ìš´ì˜
   - **Chaos Engineering**: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸
   - **AIOps**: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ì˜ˆì¸¡

3. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - í”„ë¡œë•ì…˜ í™˜ê²½ Observability í”Œë«í¼ êµ¬ì¶•
   - Multi-cluster/Multi-cloud ëª¨ë‹ˆí„°ë§
   - FinOpsë¥¼ ìœ„í•œ ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-24
**ë‹¤ìŒ ì±•í„°:** [Ch6.Linux_OS_ì´ë¯¸ì§€.md](./Ch6.Linux_OS_ì´ë¯¸ì§€.md)
