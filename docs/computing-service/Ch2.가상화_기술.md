# Ch2. ê°€ìƒí™” ê¸°ìˆ  - KVM/QEMU ì‹¬í™”

## ğŸ“‹ ê°œìš”

ê°€ìƒí™”ëŠ” í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ, ë¬¼ë¦¬ í•˜ë“œì›¨ì–´ë¥¼ ì¶”ìƒí™”í•˜ì—¬ ì—¬ëŸ¬ ê°€ìƒ ë¨¸ì‹ ì´ í•˜ë‚˜ì˜ ë¬¼ë¦¬ ì„œë²„ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë³¸ ì¥ì—ì„œëŠ” Linux ì»¤ë„ ê¸°ë°˜ì˜ ê°€ìƒí™” ì†”ë£¨ì…˜ì¸ KVM(Kernel-based Virtual Machine)ê³¼ QEMUì˜ ì•„í‚¤í…ì²˜ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ê³ , CPU, ë©”ëª¨ë¦¬, I/O ê°€ìƒí™” ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ë©°, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, **QEMU 10.2**ì™€ **KVM**ì€ Intel TDX, AMD SEVë¥¼ í†µí•œ ê¸°ë°€ ì»´í“¨íŒ…(Confidential Computing), ARM ì¤‘ì²© ê°€ìƒí™” ê°œì„ , RISC-V ì§€ì› ë“± ìµœì‹  ê¸°ëŠ¥ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

1. **KVM/QEMU ì•„í‚¤í…ì²˜ ì´í•´**
   - KVMê³¼ QEMUì˜ ì—­í•  ë° ìƒí˜¸ì‘ìš©
   - í•˜ë“œì›¨ì–´ ê°€ìƒí™” í™•ì¥ (Intel VT-x, AMD-V)
   - Libvirtë¥¼ í†µí•œ ê´€ë¦¬ ê³„ì¸µ

2. **CPU ê°€ìƒí™” ë§ˆìŠ¤í„°í•˜ê¸°**
   - vCPU ìŠ¤ì¼€ì¤„ë§ ë° ì˜¤ë²„ì»¤ë°‹
   - CPU í”¼ë‹ ë° NUMA ì¸ì‹
   - CPU ëª¨ë¸ ë° ê¸°ëŠ¥ í”Œë˜ê·¸

3. **ë©”ëª¨ë¦¬ ê°€ìƒí™” ì‹¬í™”**
   - EPT/NPTë¥¼ í†µí•œ 2ë‹¨ê³„ í˜ì´ì§€ í…Œì´ë¸”
   - Huge Pages (2MB, 1GB)
   - Memory Ballooning ë° KSM

4. **I/O ê°€ìƒí™” ë° ì„±ëŠ¥ ìµœì í™”**
   - VirtIO ë“œë¼ì´ë²„ (virtio-blk, virtio-scsi, virtio-net)
   - VFIO ë° SR-IOV for PCI íŒ¨ìŠ¤ìŠ¤ë£¨
   - ë””ìŠ¤í¬ I/O ìŠ¤ì¼€ì¤„ëŸ¬ íŠœë‹

5. **NUMA ì•„í‚¤í…ì²˜ ë° ìµœì í™”**
   - NUMA ë…¸ë“œ ë° ë©”ëª¨ë¦¬ ì§€ì—­ì„±
   - vCPUì™€ ë©”ëª¨ë¦¬ì˜ NUMA ì •ë ¬
   - ì„±ëŠ¥ ì˜í–¥ ë¶„ì„

6. **í”„ë¡œë•ì…˜ ì„±ëŠ¥ íŠœë‹**
   - ë²¤ì¹˜ë§ˆí‚¹ ë° ëª¨ë‹ˆí„°ë§
   - ì‹¤ì „ íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸
   - íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

---

## Part 1: KVM/QEMU ì•„í‚¤í…ì²˜

### 1.1 KVMê³¼ QEMUì˜ ì—­í• 

**KVM (Kernel-based Virtual Machine):**

- Linux ì»¤ë„ ëª¨ë“ˆ (`kvm.ko`, `kvm-intel.ko`, `kvm-amd.ko`)
- í•˜ë“œì›¨ì–´ ê°€ìƒí™” í™•ì¥ í™œìš© (Intel VT-x, AMD-V)
- CPUì™€ ë©”ëª¨ë¦¬ ê°€ìƒí™” ì œê³µ
- Type-1 Hypervisorë¡œ ë™ì‘

**QEMU (Quick Emulator):**

- ì‚¬ìš©ì ê³µê°„ í”„ë¡œì„¸ìŠ¤
- I/O ì¥ì¹˜ ì—ë®¬ë ˆì´ì…˜ (ë””ìŠ¤í¬, ë„¤íŠ¸ì›Œí¬, GPU ë“±)
- ë™ì  ë°”ì´ë„ˆë¦¬ ë³€í™˜ (TCG - Tiny Code Generator)
- KVMê³¼ í•¨ê»˜ ì‚¬ìš© ì‹œ í•˜ë“œì›¨ì–´ ê°€ì† í™œìš©

**í†µí•© ì•„í‚¤í…ì²˜:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Guest VM (Virtual Machine)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Guest OS (Linux/Windows)                       â”‚   â”‚
â”‚  â”‚   Applications                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                â”‚
â”‚         â”œâ”€ vCPU â”€â”€â”                                      â”‚
â”‚         â”œâ”€ vMem â”€â”€â”¼â”€ Virtualized Hardware              â”‚
â”‚         â””â”€ vI/O â”€â”€â”˜                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KVM (Kernel Space)  â”‚       â”‚   QEMU (User Space)     â”‚
â”‚  - CPU virtualizationâ”‚       â”‚  - Device emulation     â”‚
â”‚  - Memory management â”‚â—„â”€â”€â”€â”€â”€â”€â”¤  - I/O handling         â”‚
â”‚  - /dev/kvm interfaceâ”‚ ioctl â”‚  - VM process           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Linux Kernel (Host OS)                      â”‚
â”‚  - Process Scheduler                                     â”‚
â”‚  - Memory Manager                                        â”‚
â”‚  - Hardware Drivers                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Physical Hardware                             â”‚
â”‚  - CPU (Intel VT-x / AMD-V)                              â”‚
â”‚  - Memory (RAM)                                          â”‚
â”‚  - I/O Devices (NIC, Disk, GPU)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í•˜ë“œì›¨ì–´ ê°€ìƒí™” í™•ì¥

**Intel VT-x (Virtualization Technology):**

- **VMX (Virtual Machine Extensions)**: Root ëª¨ë“œì™€ Non-root ëª¨ë“œ
- **EPT (Extended Page Tables)**: 2ë‹¨ê³„ ì£¼ì†Œ ë³€í™˜
- **VPID (Virtual Processor ID)**: TLB íƒœê¹…ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ ìµœì í™”
- **VT-d (I/O Virtualization)**: IOMMUë¥¼ í†µí•œ DMA ë¦¬ë§¤í•‘

**AMD-V (AMD Virtualization):**

- **SVM (Secure Virtual Machine)**: Host ëª¨ë“œì™€ Guest ëª¨ë“œ
- **NPT (Nested Page Tables)**: EPTì™€ ë™ì¼í•œ 2ë‹¨ê³„ í˜ì´ì§€ í…Œì´ë¸”
- **ASID (Address Space Identifier)**: VPIDì™€ ë™ì¼
- **AMD-Vi (IOMMU)**: VT-dì™€ ë™ì¼

**í•˜ë“œì›¨ì–´ ì§€ì› í™•ì¸:**
```bash
# Intel VT-x ì§€ì› í™•ì¸
grep -E 'vmx' /proc/cpuinfo

# AMD-V ì§€ì› í™•ì¸
grep -E 'svm' /proc/cpuinfo

# KVM ëª¨ë“ˆ ë¡œë“œ í™•ì¸
lsmod | grep kvm
# kvm_intel (Intel)
# kvm_amd (AMD)

# /dev/kvm ì¥ì¹˜ íŒŒì¼ í™•ì¸
ls -l /dev/kvm
# crw-rw-rw- 1 root kvm 10, 232 Nov 24 10:00 /dev/kvm
```

### 1.3 Libvirt ê´€ë¦¬ ê³„ì¸µ

**Libvirt ì•„í‚¤í…ì²˜:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Management Tools                       â”‚
â”‚  - virsh (CLI)                            â”‚
â”‚  - virt-manager (GUI)                     â”‚
â”‚  - OpenStack Nova                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          libvirtd (Daemon)                â”‚
â”‚  - VM lifecycle management                â”‚
â”‚  - XML-based configuration                â”‚
â”‚  - Storage/Network pools                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Driver Interface
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚ QEMU â”‚  â”‚  LXC  â”‚  â”‚  Xen  â”‚  â”‚ VMwareâ”‚
â”‚Driverâ”‚  â”‚Driver â”‚  â”‚Driver â”‚  â”‚Driver â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Libvirt XML ë„ë©”ì¸ ì •ì˜:**
```xml
<domain type='kvm'>
  <name>test-vm</name>
  <memory unit='GiB'>4</memory>
  <vcpu placement='static'>2</vcpu>

  <!-- CPU ì„¤ì • -->
  <cpu mode='host-passthrough' check='none'>
    <topology sockets='1' dies='1' cores='2' threads='1'/>
    <numa>
      <cell id='0' cpus='0-1' memory='4' unit='GiB'/>
    </numa>
  </cpu>

  <!-- OS ë¶€íŒ… -->
  <os>
    <type arch='x86_64' machine='pc-q35-7.2'>hvm</type>
    <boot dev='hd'/>
  </os>

  <!-- ë””ìŠ¤í¬ -->
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none' io='native'/>
      <source file='/var/lib/libvirt/images/test-vm.qcow2'/>
      <target dev='vda' bus='virtio'/>
    </disk>

    <!-- ë„¤íŠ¸ì›Œí¬ -->
    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
    </interface>

    <!-- VNC ì½˜ì†” -->
    <graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
```

---

## Part 2: CPU ê°€ìƒí™”

### 2.1 vCPUì™€ ë¬¼ë¦¬ CPU ë§¤í•‘

**vCPU ìŠ¤ì¼€ì¤„ë§:**
KVMì—ì„œ ê° vCPUëŠ” Linux ì»¤ë„ì˜ ì¼ë°˜ í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œë¡œ êµ¬í˜„ë©ë‹ˆë‹¤. CFS (Completely Fair Scheduler)ê°€ vCPU ìŠ¤ë ˆë“œë¥¼ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤.

```bash
# QEMU í”„ë¡œì„¸ìŠ¤ ë° vCPU ìŠ¤ë ˆë“œ í™•ì¸
ps -eLf | grep qemu-system-x86_64

# ì˜ˆì‹œ ì¶œë ¥:
# qemu      1234  1233  1234  ... qemu-system-x86_64 (main thread)
# qemu      1234  1233  1235  ... CPU 0/KVM          (vCPU 0)
# qemu      1234  1233  1236  ... CPU 1/KVM          (vCPU 1)
```

### 2.2 CPU ì˜¤ë²„ì»¤ë°‹

**ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨:**
```
vCPU ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ = (ì´ vCPU ìˆ˜) / (ë¬¼ë¦¬ CPU ì½”ì–´ ìˆ˜)

ì˜ˆì‹œ:

- ë¬¼ë¦¬ CPU: 32 cores
- VM 1: 8 vCPUs
- VM 2: 8 vCPUs
- VM 3: 8 vCPUs
- VM 4: 8 vCPUs
- ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ = 32 / 32 = 1:1 (ì´ìƒì )

ì˜¤ë²„ì»¤ë°‹ (aggressive):

- ë¬¼ë¦¬ CPU: 32 cores
- VM 10ê°œ x 8 vCPUs = 80 vCPUs
- ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ = 80 / 32 = 2.5:1
```

**ê¶Œì¥ì‚¬í•­:**

- **í”„ë¡œë•ì…˜**: 1:1 ~ 2:1
- **ê°œë°œ/í…ŒìŠ¤íŠ¸**: 4:1 ~ 8:1
- **VDI (Virtual Desktop)**: 8:1 ~ 16:1 (CPU idleì´ ë†’ìŒ)

### 2.3 CPU í”¼ë‹ (Pinning)

**CPU í”¼ë‹ì˜ ì¥ì :**

- L1/L2/L3 ìºì‹œ ì§€ì—­ì„± í–¥ìƒ
- NUMA ë…¸ë“œ ê²½ê³„ êµì°¨ ë°©ì§€
- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥

**Libvirtë¥¼ í†µí•œ CPU í”¼ë‹:**
```xml
<domain type='kvm'>
  <vcpu placement='static'>4</vcpu>
  <cputune>
    <!-- vCPU 0 â†’ Physical CPU 4 -->
    <vcpupin vcpu='0' cpuset='4'/>
    <!-- vCPU 1 â†’ Physical CPU 5 -->
    <vcpupin vcpu='1' cpuset='5'/>
    <!-- vCPU 2 â†’ Physical CPU 6 -->
    <vcpupin vcpu='2' cpuset='6'/>
    <!-- vCPU 3 â†’ Physical CPU 7 -->
    <vcpupin vcpu='3' cpuset='7'/>

    <!-- Emulator ìŠ¤ë ˆë“œ í”¼ë‹ -->
    <emulatorpin cpuset='0-1'/>

    <!-- I/O ìŠ¤ë ˆë“œ í”¼ë‹ -->
    <iothreadpin iothread='1' cpuset='2-3'/>
  </cputune>
</domain>
```

**virshë¥¼ í†µí•œ ë™ì  í”¼ë‹:**
```bash
# vCPU 0ì„ ë¬¼ë¦¬ CPU 4ì— í”¼ë‹
virsh vcpupin test-vm 0 4

# í˜„ì¬ í”¼ë‹ ìƒíƒœ í™•ì¸
virsh vcpuinfo test-vm
```

### 2.4 CPU ëª¨ë¸ ë° ê¸°ëŠ¥ í”Œë˜ê·¸

**CPU ëª¨ë“œ:**

- **host-passthrough**: í˜¸ìŠ¤íŠ¸ CPU ê·¸ëŒ€ë¡œ ë…¸ì¶œ (ë§ˆì´ê·¸ë ˆì´ì…˜ ì œì•½)
- **host-model**: í˜¸ìŠ¤íŠ¸ CPUì™€ ìœ ì‚¬í•œ ëª¨ë¸ (ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ëŠ¥)
- **custom**: íŠ¹ì • CPU ëª¨ë¸ ì§€ì •

```xml
<!-- host-passthrough: ìµœê³  ì„±ëŠ¥ -->
<cpu mode='host-passthrough' check='none'/>

<!-- host-model: ê· í˜• -->
<cpu mode='host-model' check='partial'>
  <feature policy='require' name='vmx'/>  <!-- Nested virtualization -->
</cpu>

<!-- custom: í˜¸í™˜ì„± -->
<cpu mode='custom' match='exact' check='partial'>
  <model fallback='allow'>Skylake-Server</model>
  <feature policy='require' name='avx2'/>
  <feature policy='disable' name='mpx'/>
</cpu>
```

**ì‚¬ìš© ê°€ëŠ¥í•œ CPU ëª¨ë¸ í™•ì¸:**
```bash
# QEMUê°€ ì§€ì›í•˜ëŠ” ëª¨ë“  CPU ëª¨ë¸
qemu-system-x86_64 -cpu help

# Libvirtê°€ ì¸ì‹í•˜ëŠ” CPU ëª¨ë¸
virsh cpu-models x86_64
```

---

## Part 3: ë©”ëª¨ë¦¬ ê°€ìƒí™”

### 3.1 2ë‹¨ê³„ í˜ì´ì§€ í…Œì´ë¸” (EPT/NPT)

**ì „í†µì ì¸ Shadow Page Table vs EPT/NPT:**

**Shadow Page Table (í•˜ë“œì›¨ì–´ ê°€ìƒí™” ì´ì „):**
```
Guest Virtual Address (GVA)
    â”‚
    â–¼ (Guest Page Table - ì†Œí”„íŠ¸ì›¨ì–´ ê´€ë¦¬)
Guest Physical Address (GPA)
    â”‚
    â–¼ (Shadow Page Table - Hypervisor ê´€ë¦¬)
Host Physical Address (HPA)

ë¬¸ì œì : ë§¤ Guest í˜ì´ì§€ í…Œì´ë¸” ë³€ê²½ ì‹œ VM Exit ë°œìƒ â†’ ì„±ëŠ¥ ì €í•˜
```

**EPT/NPT (í•˜ë“œì›¨ì–´ 2ë‹¨ê³„ ë³€í™˜):**
```
Guest Virtual Address (GVA)
    â”‚
    â–¼ (Guest Page Table - Guest OS ê´€ë¦¬)
Guest Physical Address (GPA)
    â”‚
    â–¼ (EPT/NPT - í•˜ë“œì›¨ì–´ MMU)
Host Physical Address (HPA)

ì¥ì : VM Exit ê°ì†Œ, í•˜ë“œì›¨ì–´ ê°€ì† ë³€í™˜
```

### 3.2 Huge Pages

**Huge Pagesì˜ ì´ì :**

- **TLB Miss ê°ì†Œ**: ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ë” ì ì€ TLB ì—”íŠ¸ë¦¬ë¡œ ì»¤ë²„
- **í˜ì´ì§€ í…Œì´ë¸” ì˜¤ë²„í—¤ë“œ ê°ì†Œ**: í˜ì´ì§€ í…Œì´ë¸” í¬ê¸° ì¶•ì†Œ
- **ì„±ëŠ¥ í–¥ìƒ**: ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ ì›Œí¬ë¡œë“œì—ì„œ 5-20% ì„±ëŠ¥ í–¥ìƒ

**Huge Pages íƒ€ì…:**

- **2MB Huge Pages**: ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©
- **1GB Huge Pages**: ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ (128GB+) VMì— ê¶Œì¥

**í˜¸ìŠ¤íŠ¸ ì„¤ì •:**
```bash
# 1. Huge Pages í• ë‹¹ (2MB)
# ì´ ë©”ëª¨ë¦¬ì˜ 80-90% í• ë‹¹ ê¶Œì¥
echo 20480 > /proc/sys/vm/nr_hugepages  # 20480 * 2MB = 40GB

# ì˜êµ¬ ì„¤ì • (ì¬ë¶€íŒ… í›„ì—ë„ ìœ ì§€)
cat >> /etc/sysctl.conf <<EOF
vm.nr_hugepages = 20480
vm.hugetlb_shm_group = 36  # kvm ê·¸ë£¹ GID
EOF

sysctl -p

# 2. 1GB Huge Pages í• ë‹¹ (ë¶€íŒ… ì‹œ)
# /etc/default/grub ìˆ˜ì •
GRUB_CMDLINE_LINUX="default_hugepagesz=1G hugepagesz=1G hugepages=32"

# GRUB ì—…ë°ì´íŠ¸
update-grub
reboot

# 3. Huge Pages í™•ì¸
cat /proc/meminfo | grep -i huge
# HugePages_Total:   20480
# HugePages_Free:    18432
# HugePages_Rsvd:        0
# HugePages_Surp:        0
# Hugepagesize:       2048 kB

# 4. Huge Pages ë§ˆìš´íŠ¸
mkdir -p /dev/hugepages
mount -t hugetlbfs hugetlbfs /dev/hugepages

# /etc/fstabì— ì¶”ê°€
echo "hugetlbfs /dev/hugepages hugetlbfs defaults 0 0" >> /etc/fstab
```

**Libvirtì—ì„œ Huge Pages ì‚¬ìš©:**
```xml
<domain type='kvm'>
  <memory unit='GiB'>16</memory>
  <memoryBacking>
    <!-- 2MB Huge Pages -->
    <hugepages>
      <page size='2' unit='M'/>
    </hugepages>

    <!-- 1GB Huge Pages -->
    <!-- <page size='1' unit='G'/> -->

    <!-- Memory ì ê¸ˆ (swap ë°©ì§€) -->
    <locked/>
  </memoryBacking>
</domain>
```

### 3.3 Memory Ballooning

**Ballooning ê°œë…:**
í˜¸ìŠ¤íŠ¸ê°€ ë©”ëª¨ë¦¬ ì••ë°•ì„ ë°›ì„ ë•Œ, Guestì—ê²Œ ë©”ëª¨ë¦¬ ë°˜í™˜ì„ ìš”ì²­í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

```bash
# virtio-balloon ë“œë¼ì´ë²„ í™•ì¸ (Guest)
lsmod | grep virtio_balloon

# ë™ì  ë©”ëª¨ë¦¬ ì¡°ì • (Host)
virsh setmem test-vm 2G --live

# í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
virsh dominfo test-vm | grep -i memory
```

**Libvirt ì„¤ì •:**
```xml
<devices>
  <memballoon model='virtio'>
    <stats period='10'/>  <!-- 10ì´ˆë§ˆë‹¤ í†µê³„ ìˆ˜ì§‘ -->
  </memballoon>
</devices>
```

### 3.4 KSM (Kernel Same-page Merging)

**KSM ë™ì‘:**
ë™ì¼í•œ ë‚´ìš©ì˜ ë©”ëª¨ë¦¬ í˜ì´ì§€ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ ë©”ëª¨ë¦¬ ì ˆì•½ (Copy-on-Write).

**KSM í™œì„±í™”:**
```bash
# KSM í™œì„±í™”
echo 1 > /sys/kernel/mm/ksm/run

# ìŠ¤ìº” íŒŒë¼ë¯¸í„° ì¡°ì •
echo 100 > /sys/kernel/mm/ksm/sleep_millisecs  # ìŠ¤ìº” ê°„ê²©
echo 1000 > /sys/kernel/mm/ksm/pages_to_scan   # ìŠ¤ìº”í•  í˜ì´ì§€ ìˆ˜

# KSM í†µê³„
cat /sys/kernel/mm/ksm/pages_sharing
cat /sys/kernel/mm/ksm/pages_shared
```

**ì£¼ì˜ì‚¬í•­:**

- CPU ì˜¤ë²„í—¤ë“œ ë°œìƒ (ìŠ¤ìº” ë¹„ìš©)
- ë³´ì•ˆ ê³ ë ¤ì‚¬í•­ (side-channel ê³µê²© ê°€ëŠ¥ì„±)
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©

---

## Part 4: I/O ê°€ìƒí™”

### 4.1 VirtIO ì•„í‚¤í…ì²˜

**VirtIOë€?**
KVM í™˜ê²½ì—ì„œ ìµœì í™”ëœ ë°˜ê°€ìƒí™”(Paravirtualization) I/O ë“œë¼ì´ë²„ì…ë‹ˆë‹¤.

**VirtIO êµ¬ì¡°:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Guest OS                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  VirtIO Frontend Driver        â”‚  â”‚
â”‚  â”‚  (virtio-blk, virtio-net, ...) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚ virtqueue             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ (Shared Memory)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QEMU (Host)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  VirtIO Backend                â”‚  â”‚
â”‚  â”‚  (handles I/O requests)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Physical Device (Disk/NIC)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Disk I/O - virtio-blk vs virtio-scsi

**ë¹„êµí‘œ (2025 Best Practice):**

| íŠ¹ì§• | virtio-blk | virtio-scsi |
|------|-----------|-------------|
| ì„±ëŠ¥ | ë†’ìŒ (ë‚®ì€ ë ˆì´í„´ì‹œ) | ë§¤ìš° ë†’ìŒ (ê³ ë¶€í•˜) |
| Queue Depth | ì œí•œì  | ê¹ŠìŒ (ê³ ë¶€í•˜ ìœ ë¦¬) |
| TRIM/UNMAP | ì œí•œì  | ì™„ì „ ì§€ì› (SSD í•„ìˆ˜) |
| ë””ìŠ¤í¬ ìˆ˜ | ì œí•œ (26ê°œ) | ë¬´ì œí•œ |
| ì‚¬ìš© ì‚¬ë¡€ | ì›¹ì„œë²„, íŒŒì¼ì„œë²„ | DB, ë©”ì¼ì„œë²„, I/O ì§‘ì•½ì  |

**virtio-blk ì„¤ì •:**
```xml
<disk type='file' device='disk'>
  <driver name='qemu' type='qcow2' cache='none' io='native' discard='unmap'/>
  <source file='/var/lib/libvirt/images/vm.qcow2'/>
  <target dev='vda' bus='virtio'/>
</disk>
```

**virtio-scsi ì„¤ì • (ê¶Œì¥):**
```xml
<!-- SCSI ì»¨íŠ¸ë¡¤ëŸ¬ ì •ì˜ -->
<controller type='scsi' index='0' model='virtio-scsi'>
  <driver queues='4' iothread='1'/>
</controller>

<!-- ë””ìŠ¤í¬ -->
<disk type='file' device='disk'>
  <driver name='qemu' type='qcow2' cache='none' io='native' discard='unmap'/>
  <source file='/var/lib/libvirt/images/vm.qcow2'/>
  <target dev='sda' bus='scsi'/>
</disk>
```

**ìºì‹œ ëª¨ë“œ:**

- **none**: Write-through, O_DIRECT (ì•ˆì „, ì„±ëŠ¥ ì–‘í˜¸) â† ê¶Œì¥
- **writethrough**: Write-through, ìºì‹œ ì‚¬ìš© (ì•ˆì „, ëŠë¦¼)
- **writeback**: Write-back (ë¹ ë¦„, ë°ì´í„° ì†ì‹¤ ìœ„í—˜)
- **directsync**: O_DIRECT + O_SYNC (ê°€ì¥ ì•ˆì „, ëŠë¦¼)

### 4.3 Network I/O - virtio-net

**virtio-net ì„±ëŠ¥ ìµœì í™”:**
```xml
<interface type='network'>
  <source network='default'/>
  <model type='virtio'/>

  <!-- Multi-queue í™œì„±í™” (vCPU ìˆ˜ë§Œí¼) -->
  <driver name='vhost' queues='4' txmode='iothread' ioeventfd='on'/>

  <!-- vhost-net ì‚¬ìš© (ì»¤ë„ ê³µê°„ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬) -->
  <backend tap='/dev/net/tun' vhost='/dev/vhost-net'/>
</interface>
```

**Guest ë‚´ë¶€ ìµœì í™”:**
```bash
# Multi-queue í™•ì¸ (Guest)
ethtool -l eth0

# IRQ ë¶„ì‚° (ê° queueë¥¼ ë‹¤ë¥¸ vCPUì—)
echo 1 > /proc/irq/30/smp_affinity
echo 2 > /proc/irq/31/smp_affinity
echo 4 > /proc/irq/32/smp_affinity
echo 8 > /proc/irq/33/smp_affinity

# TSO/GSO í™œì„±í™” (Offload)
ethtool -K eth0 tso on gso on gro on
```

**Jumbo Frames (MTU 9000):**
```bash
# Host ë¸Œë¦¬ì§€
ip link set br0 mtu 9000

# Guest
ip link set eth0 mtu 9000

# ì„±ëŠ¥ í–¥ìƒ: 3.6 Gbps â†’ 9.4 Gbps (ì˜ëª»ëœ ì„¤ì • vs ì˜¬ë°”ë¥¸ ì„¤ì •)
```

### 4.4 VFIO ë° PCI íŒ¨ìŠ¤ìŠ¤ë£¨

**VFIO (Virtual Function I/O):**
í•˜ë“œì›¨ì–´ ì¥ì¹˜ë¥¼ ì§ì ‘ Guestì— í• ë‹¹í•˜ì—¬ ë„¤ì´í‹°ë¸Œì— ê°€ê¹Œìš´ ì„±ëŠ¥ì„ ì–»ìŠµë‹ˆë‹¤.

**ì‚¬ìš© ì‚¬ë¡€:**

- GPU íŒ¨ìŠ¤ìŠ¤ë£¨ (NVIDIA, AMD)
- ê³ ì„±ëŠ¥ NIC (10GbE, 100GbE)
- NVME SSD

**VFIO ì„¤ì •:**
```bash
# 1. IOMMU í™œì„±í™” (/etc/default/grub)
# Intel
GRUB_CMDLINE_LINUX="intel_iommu=on iommu=pt"

# AMD
GRUB_CMDLINE_LINUX="amd_iommu=on iommu=pt"

update-grub
reboot

# 2. IOMMU ê·¸ë£¹ í™•ì¸
for d in /sys/kernel/iommu_groups/*/devices/*; do
  n=${d#*/iommu_groups/*}
  n=${n%%/*}
  printf 'IOMMU Group %s ' "$n"
  lspci -nns "${d##*/}"
done

# 3. ì¥ì¹˜ ë°”ì¸ë”©
# PCI ì£¼ì†Œ í™•ì¸
lspci | grep -i nvidia
# 01:00.0 VGA compatible controller: NVIDIA Corporation

# vfio-pci ë“œë¼ì´ë²„ì— ë°”ì¸ë”©
echo "10de 1b80" > /sys/bus/pci/drivers/vfio-pci/new_id
echo "0000:01:00.0" > /sys/bus/pci/devices/0000:01:00.0/driver/unbind
echo "0000:01:00.0" > /sys/bus/pci/drivers/vfio-pci/bind
```

**Libvirt ì„¤ì •:**
```xml
<hostdev mode='subsystem' type='pci' managed='yes'>
  <source>
    <address domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
  </source>
  <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
</hostdev>
```

**SR-IOV (Single Root I/O Virtualization):**
```bash
# NIC SR-IOV VF ìƒì„±
echo 4 > /sys/class/net/ens1f0/device/sriov_numvfs

# VF í™•ì¸
lspci | grep -i virtual
# 01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function
# 01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function
```

---

## Part 5: NUMA ì•„í‚¤í…ì²˜ ë° ìµœì í™”

### 5.1 NUMA ê°œë…

**NUMA (Non-Uniform Memory Access):**
ë©€í‹° ì†Œì¼“ ì‹œìŠ¤í…œì—ì„œ ê° CPUê°€ ë¡œì»¬ ë©”ëª¨ë¦¬ì— ë¹ ë¥´ê²Œ ì ‘ê·¼í•˜ê³ , ì›ê²© ë©”ëª¨ë¦¬ì—ëŠ” ëŠë¦¬ê²Œ ì ‘ê·¼í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NUMA Node 0                    NUMA Node 1             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  CPU 0-15  â”‚                 â”‚ CPU 16-31  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚ (Fast - Local)               â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Memory    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Memory    â”‚          â”‚
â”‚  â”‚  64GB      â”‚  (Slow - Remote)â”‚  64GB      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  NIC 0     â”‚                 â”‚   NIC 1    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì„±ëŠ¥ ì°¨ì´:

- ë¡œì»¬ ë©”ëª¨ë¦¬ ì ‘ê·¼: 100 ns
- ì›ê²© ë©”ëª¨ë¦¬ ì ‘ê·¼: 200-300 ns (2-3ë°° ëŠë¦¼)
```

### 5.2 NUMA í† í´ë¡œì§€ í™•ì¸

```bash
# NUMA ë…¸ë“œ í™•ì¸
numactl --hardware

# ì¶œë ¥ ì˜ˆì‹œ:
# available: 2 nodes (0-1)
# node 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23
# node 0 size: 65536 MB
# node 0 free: 45123 MB
# node 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31
# node 1 size: 65536 MB
# node 1 free: 52341 MB
# node distances:
# node   0   1
#   0:  10  21
#   1:  21  10

# lstopo (hwloc)
lstopo --no-io --no-legend
```

### 5.3 VMì˜ NUMA ì •ë ¬ (Best Practice)

**í•µì‹¬ ì›ì¹™:**
1. **vCPUì™€ ë©”ëª¨ë¦¬ë¥¼ ë™ì¼í•œ NUMA ë…¸ë“œì— ë°°ì¹˜**
2. **ë¬¼ë¦¬ NICë„ ë™ì¼í•œ NUMA ë…¸ë“œ ì‚¬ìš©**
3. **NUMA ê²½ê³„ë¥¼ ì ˆëŒ€ ë„˜ì§€ ì•Šê¸°**

**Libvirt NUMA ì„¤ì •:**
```xml
<domain type='kvm'>
  <vcpu placement='static'>8</vcpu>
  <cpu mode='host-passthrough'>
    <numa>
      <!-- VMì˜ NUMA Node 0 â†’ Host NUMA Node 0 -->
      <cell id='0' cpus='0-7' memory='32' unit='GiB' memAccess='shared'/>
    </numa>
  </cpu>

  <numatune>
    <!-- ë©”ëª¨ë¦¬ë¥¼ Host NUMA Node 0ì—ë§Œ í• ë‹¹ -->
    <memory mode='strict' nodeset='0'/>
    <memnode cellid='0' mode='strict' nodeset='0'/>
  </numatune>

  <cputune>
    <!-- vCPU 0-7 â†’ Physical CPU 0-7 (NUMA Node 0) -->
    <vcpupin vcpu='0' cpuset='0'/>
    <vcpupin vcpu='1' cpuset='1'/>
    <vcpupin vcpu='2' cpuset='2'/>
    <vcpupin vcpu='3' cpuset='3'/>
    <vcpupin vcpu='4' cpuset='4'/>
    <vcpupin vcpu='5' cpuset='5'/>
    <vcpupin vcpu='6' cpuset='6'/>
    <vcpupin vcpu='7' cpuset='7'/>
  </cputune>
</domain>
```

**ì˜ëª»ëœ ì˜ˆ (NUMA ê²½ê³„ êµì°¨):**
```xml
<!-- âŒ BAD: vCPUê°€ ë‘ NUMA ë…¸ë“œì— ê±¸ì³ ìˆìŒ -->
<numa>
  <cell id='0' cpus='0-3' memory='16' unit='GiB'/>
  <cell id='1' cpus='4-7' memory='16' unit='GiB'/>
</numa>

<!-- vCPU 0-3ì€ NUMA 0, vCPU 4-7ì€ NUMA 1 â†’ ì›ê²© ë©”ëª¨ë¦¬ ì ‘ê·¼ ë°œìƒ -->
```

### 5.4 NUMA ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```bash
# VMì˜ NUMA í†µê³„
virsh numatune test-vm

# NUMA ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
numastat

# Per-VM NUMA í†µê³„
numastat -c qemu-system-x86

# ì›ê²© ë©”ëª¨ë¦¬ ì ‘ê·¼ í™•ì¸ (numa_miss, numa_foreign)
numastat -p $(pgrep -f test-vm)
```

---

## Part 6: í”„ë¡œë•ì…˜ ì„±ëŠ¥ íŠœë‹

### 6.1 ì¢…í•© íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

**âœ… CPU:**

- [ ] CPU í”¼ë‹ ì„¤ì • (vCPU â†’ Physical CPU)
- [ ] Emulator ë° I/O ìŠ¤ë ˆë“œ ì „ìš© CPU í• ë‹¹
- [ ] NUMA ê²½ê³„ êµì°¨ ë°©ì§€
- [ ] CPU ëª¨ë¸: host-passthrough (ì„±ëŠ¥) ë˜ëŠ” host-model (ë§ˆì´ê·¸ë ˆì´ì…˜)
- [ ] ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨: í”„ë¡œë•ì…˜ 1:1 ~ 2:1

**âœ… Memory:**

- [ ] Huge Pages í™œì„±í™” (2MB ë˜ëŠ” 1GB)
- [ ] ë©”ëª¨ë¦¬ ì ê¸ˆ (locked) - swap ë°©ì§€
- [ ] NUMA ë©”ëª¨ë¦¬ ì •ë ¬ (strict mode)
- [ ] Memory Ballooning ë¹„í™œì„±í™” (ì„±ëŠ¥ ì¤‘ì‹œ)
- [ ] KSM ë¹„í™œì„±í™” (CPU ì˜¤ë²„í—¤ë“œ íšŒí”¼)

**âœ… Disk I/O:**

- [ ] virtio-scsi ì‚¬ìš© (DB, I/O ì§‘ì•½ì )
- [ ] Cache mode: none + io=native
- [ ] discard=unmap (SSD TRIM ì§€ì›)
- [ ] I/O ìŠ¤ë ˆë“œ í™œì„±í™” ë° í”¼ë‹
- [ ] ë””ìŠ¤í¬ I/O ìŠ¤ì¼€ì¤„ëŸ¬: none (NVMe) ë˜ëŠ” mq-deadline (SSD/HDD)

**âœ… Network:**

- [ ] virtio-net + vhost-net
- [ ] Multi-queue í™œì„±í™” (vCPU ìˆ˜ë§Œí¼)
- [ ] Jumbo Frames (MTU 9000)
- [ ] TSO/GSO/GRO í™œì„±í™”
- [ ] NICì™€ vCPUë¥¼ ë™ì¼ NUMA ë…¸ë“œì— ë°°ì¹˜

**âœ… ê¸°íƒ€:**

- [ ] CPU ì ˆì „ ê¸°ëŠ¥ ë¹„í™œì„±í™” (C-states, P-states)
- [ ] Transparent Huge Pages (THP) ë¹„í™œì„±í™”
- [ ] SELinux/AppArmor ì„±ëŠ¥ ì˜í–¥ í™•ì¸

### 6.2 í˜¸ìŠ¤íŠ¸ ì»¤ë„ íŠœë‹

**CPU ì ˆì „ ë¹„í™œì„±í™”:**
```bash
# /etc/default/grub
GRUB_CMDLINE_LINUX="intel_pstate=disable processor.max_cstate=1 intel_idle.max_cstate=0"

update-grub
reboot
```

**Transparent Huge Pages ë¹„í™œì„±í™”:**
```bash
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag

# ì˜êµ¬ ì„¤ì •
cat >> /etc/rc.local <<EOF
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
EOF
```

**I/O ìŠ¤ì¼€ì¤„ëŸ¬:**
```bash
# NVMe: none
echo none > /sys/block/nvme0n1/queue/scheduler

# SSD: mq-deadline
echo mq-deadline > /sys/block/sda/queue/scheduler

# HDD: mq-deadline
echo mq-deadline > /sys/block/sdb/queue/scheduler
```

### 6.3 ë²¤ì¹˜ë§ˆí‚¹

**CPU ë²¤ì¹˜ë§ˆí¬ (sysbench):**
```bash
# Host
sysbench cpu --cpu-max-prime=20000 --threads=8 run

# Guest (ë¹„êµ)
sysbench cpu --cpu-max-prime=20000 --threads=8 run

# ëª©í‘œ: Guestê°€ Hostì˜ 95% ì´ìƒ ì„±ëŠ¥
```

**ë©”ëª¨ë¦¬ ë²¤ì¹˜ë§ˆí¬:**
```bash
# ë©”ëª¨ë¦¬ ëŒ€ì—­í­ (stream)
gcc -O3 -fopenmp -DSTREAM_ARRAY_SIZE=100000000 stream.c -o stream
export OMP_NUM_THREADS=8
./stream

# ë©”ëª¨ë¦¬ ë ˆì´í„´ì‹œ
lat_mem_rd 1024 128
```

**ë””ìŠ¤í¬ I/O ë²¤ì¹˜ë§ˆí¬ (fio):**
```bash
# ëœë¤ ì½ê¸° (IOPS)
fio --name=random-read --ioengine=libaio --iodepth=32 --rw=randread --bs=4k --direct=1 --size=4G --numjobs=4 --runtime=60 --group_reporting

# ìˆœì°¨ ì“°ê¸° (Throughput)
fio --name=sequential-write --ioengine=libaio --iodepth=32 --rw=write --bs=1m --direct=1 --size=4G --numjobs=1 --runtime=60

# ëª©í‘œ (virtio-scsi + SSD):
# - Random Read IOPS: 50K+ IOPS
# - Sequential Write: 500+ MB/s
```

**ë„¤íŠ¸ì›Œí¬ ë²¤ì¹˜ë§ˆí¬ (iperf3):**
```bash
# Server (VM1)
iperf3 -s

# Client (VM2)
iperf3 -c <VM1_IP> -t 60 -P 4

# ëª©í‘œ (10GbE):
# - TCP Throughput: 9+ Gbps
```

---

## ğŸ› ï¸ ì‹¤ìŠµ ê°€ì´ë“œ

### ì‹¤ìŠµ 1: ê³ ì„±ëŠ¥ VM ìƒì„± (ëª¨ë“  ìµœì í™” ì ìš©)

```bash
# 1. Huge Pages ì„¤ì • (2MB, 16GB í• ë‹¹)
echo 8192 > /proc/sys/vm/nr_hugepages
cat /proc/meminfo | grep -i huge

# 2. VM ìƒì„±ìš© XML ì‘ì„±
cat > high-perf-vm.xml <<'EOF'
<domain type='kvm'>
  <name>high-perf-vm</name>
  <memory unit='GiB'>16</memory>
  <currentMemory unit='GiB'>16</currentMemory>
  <vcpu placement='static'>8</vcpu>

  <!-- Huge Pages -->
  <memoryBacking>
    <hugepages>
      <page size='2' unit='M'/>
    </hugepages>
    <locked/>
  </memoryBacking>

  <!-- NUMA -->
  <cpu mode='host-passthrough' check='none'>
    <topology sockets='1' dies='1' cores='8' threads='1'/>
    <numa>
      <cell id='0' cpus='0-7' memory='16' unit='GiB'/>
    </numa>
  </cpu>

  <numatune>
    <memory mode='strict' nodeset='0'/>
  </numatune>

  <!-- CPU Pinning -->
  <cputune>
    <vcpupin vcpu='0' cpuset='2'/>
    <vcpupin vcpu='1' cpuset='3'/>
    <vcpupin vcpu='2' cpuset='4'/>
    <vcpupin vcpu='3' cpuset='5'/>
    <vcpupin vcpu='4' cpuset='6'/>
    <vcpupin vcpu='5' cpuset='7'/>
    <vcpupin vcpu='6' cpuset='8'/>
    <vcpupin vcpu='7' cpuset='9'/>
    <emulatorpin cpuset='0-1'/>
  </cputune>

  <os>
    <type arch='x86_64' machine='pc-q35-7.2'>hvm</type>
    <boot dev='hd'/>
  </os>

  <devices>
    <!-- virtio-scsi Controller -->
    <controller type='scsi' index='0' model='virtio-scsi'>
      <driver queues='8' iothread='1'/>
    </controller>

    <!-- Disk with optimal settings -->
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none' io='native' discard='unmap' iothread='1'/>
      <source file='/var/lib/libvirt/images/high-perf-vm.qcow2'/>
      <target dev='sda' bus='scsi'/>
    </disk>

    <!-- Network with multi-queue -->
    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
      <driver name='vhost' queues='8' txmode='iothread'/>
    </interface>

    <console type='pty'/>
    <graphics type='vnc' port='-1'/>
  </devices>
</domain>
EOF

# 3. VM ìƒì„±
virsh define high-perf-vm.xml

# 4. ë””ìŠ¤í¬ ì´ë¯¸ì§€ ìƒì„±
qemu-img create -f qcow2 /var/lib/libvirt/images/high-perf-vm.qcow2 100G

# 5. VM ì‹œì‘
virsh start high-perf-vm

# 6. ì„¤ì • í™•ì¸
virsh vcpuinfo high-perf-vm
virsh numatune high-perf-vm
```

### ì‹¤ìŠµ 2: NUMA ì˜í–¥ í…ŒìŠ¤íŠ¸

```bash
# ì‹œë‚˜ë¦¬ì˜¤ A: NUMA ìµœì í™” (ëª¨ë“  ë¦¬ì†ŒìŠ¤ê°€ NUMA 0)
cat > numa-optimized.xml <<EOF
<numa>
  <cell id='0' cpus='0-7' memory='16' unit='GiB'/>
</numa>
<numatune>
  <memory mode='strict' nodeset='0'/>
</numatune>
EOF

# ì‹œë‚˜ë¦¬ì˜¤ B: NUMA ë¯¸ìµœì í™” (NUMA ê²½ê³„ êµì°¨)
cat > numa-bad.xml <<EOF
<numa>
  <cell id='0' cpus='0-3' memory='8' unit='GiB'/>
  <cell id='1' cpus='4-7' memory='8' unit='GiB'/>
</numa>
<!-- numatune ì—†ìŒ - ìë™ í• ë‹¹ -->
EOF

# ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
# A: sysbench ê²°ê³¼
# B: sysbench ê²°ê³¼
# ê¸°ëŒ€: Aê°€ Bë³´ë‹¤ 10-20% ë¹ ë¦„
```

### ì‹¤ìŠµ 3: virtio-blk vs virtio-scsi ì„±ëŠ¥ ë¹„êµ

```bash
# VM 1: virtio-blk
<disk type='file' device='disk'>
  <driver name='qemu' type='raw' cache='none' io='native'/>
  <source file='/data/vm1.raw'/>
  <target dev='vda' bus='virtio'/>
</disk>

# VM 2: virtio-scsi
<controller type='scsi' model='virtio-scsi'>
  <driver queues='4'/>
</controller>
<disk type='file' device='disk'>
  <driver name='qemu' type='raw' cache='none' io='native'/>
  <source file='/data/vm2.raw'/>
  <target dev='sda' bus='scsi'/>
</disk>

# ë²¤ì¹˜ë§ˆí¬ (fio)
fio --name=test --ioengine=libaio --iodepth=32 --rw=randread --bs=4k --direct=1 --size=10G --runtime=60

# ê²°ê³¼ ë¹„êµ:
# - Low I/O: virtio-blk ê·¼ì†Œí•˜ê²Œ ìœ ë¦¬
# - High I/O: virtio-scsi 10-15% ìš°ìˆ˜
```

---

## ğŸ’» ì˜ˆì œ ì½”ë“œ

### ì˜ˆì œ 1: VM ìƒì„± ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (ìµœì í™” ì ìš©)

```bash
#!/bin/bash
# create-optimized-vm.sh

set -e

VM_NAME=$1
VCPUS=$2
MEMORY_GB=$3
DISK_SIZE_GB=$4
NUMA_NODE=$5

if [ $# -ne 5 ]; then
  echo "Usage: $0 <vm_name> <vcpus> <memory_gb> <disk_size_gb> <numa_node>"
  exit 1
fi

# Huge Pages í•„ìš”ëŸ‰ ê³„ì‚° (2MB pages)
HUGEPAGES_NEEDED=$((MEMORY_GB * 1024 / 2))
CURRENT_HUGEPAGES=$(cat /proc/sys/vm/nr_hugepages)

if [ $CURRENT_HUGEPAGES -lt $HUGEPAGES_NEEDED ]; then
  echo "Allocating Huge Pages: $HUGEPAGES_NEEDED"
  echo $HUGEPAGES_NEEDED > /proc/sys/vm/nr_hugepages
fi

# CPU í”¼ë‹ ê³„ì‚° (NUMA ë…¸ë“œ ê¸°ë°˜)
if [ $NUMA_NODE -eq 0 ]; then
  CPU_START=2
else
  CPU_START=18
fi

# XML ìƒì„±
cat > /tmp/${VM_NAME}.xml <<EOF
<domain type='kvm'>
  <name>${VM_NAME}</name>
  <memory unit='GiB'>${MEMORY_GB}</memory>
  <currentMemory unit='GiB'>${MEMORY_GB}</currentMemory>
  <vcpu placement='static'>${VCPUS}</vcpu>

  <memoryBacking>
    <hugepages><page size='2' unit='M'/></hugepages>
    <locked/>
  </memoryBacking>

  <cpu mode='host-passthrough' check='none'>
    <topology sockets='1' dies='1' cores='${VCPUS}' threads='1'/>
    <numa>
      <cell id='0' cpus='0-$((VCPUS-1))' memory='${MEMORY_GB}' unit='GiB'/>
    </numa>
  </cpu>

  <numatune>
    <memory mode='strict' nodeset='${NUMA_NODE}'/>
  </numatune>

  <cputune>
EOF

# vCPU í”¼ë‹
for i in $(seq 0 $((VCPUS-1))); do
  echo "    <vcpupin vcpu='$i' cpuset='$((CPU_START+i))'/>" >> /tmp/${VM_NAME}.xml
done

cat >> /tmp/${VM_NAME}.xml <<EOF
    <emulatorpin cpuset='0-1'/>
  </cputune>

  <os>
    <type arch='x86_64' machine='pc-q35-7.2'>hvm</type>
    <boot dev='hd'/>
  </os>

  <devices>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <driver queues='${VCPUS}' iothread='1'/>
    </controller>

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none' io='native' discard='unmap' iothread='1'/>
      <source file='/var/lib/libvirt/images/${VM_NAME}.qcow2'/>
      <target dev='sda' bus='scsi'/>
    </disk>

    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
      <driver name='vhost' queues='${VCPUS}' txmode='iothread'/>
    </interface>

    <console type='pty'/>
    <graphics type='vnc' port='-1'/>
  </devices>
</domain>
EOF

# ë””ìŠ¤í¬ ìƒì„±
qemu-img create -f qcow2 /var/lib/libvirt/images/${VM_NAME}.qcow2 ${DISK_SIZE_GB}G

# VM ì •ì˜ ë° ì‹œì‘
virsh define /tmp/${VM_NAME}.xml
virsh start ${VM_NAME}

echo "VM ${VM_NAME} created and started successfully!"
virsh vcpuinfo ${VM_NAME}
```

**ì‚¬ìš© ì˜ˆ:**
```bash
# NUMA 0ì— 8 vCPU, 16GB RAM, 100GB ë””ìŠ¤í¬ VM ìƒì„±
./create-optimized-vm.sh test-vm 8 16 100 0
```

### ì˜ˆì œ 2: Pythonìœ¼ë¡œ VM ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
#!/usr/bin/env python3
# vm-monitor.py

import libvirt
import time
import sys

def get_vm_stats(conn, vm_name):
    """VMì˜ CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬, ë„¤íŠ¸ì›Œí¬ í†µê³„ ìˆ˜ì§‘"""
    try:
        domain = conn.lookupByName(vm_name)
    except libvirt.libvirtError:
        print(f"VM {vm_name} not found")
        return None

    # CPU í†µê³„
    cpu_stats = domain.getCPUStats(True)[0]
    cpu_time = cpu_stats['cpu_time'] / 1e9  # nanoseconds to seconds

    # ë©”ëª¨ë¦¬ í†µê³„
    mem_stats = domain.memoryStats()
    mem_total = mem_stats.get('actual', 0) / 1024  # KB to MB
    mem_unused = mem_stats.get('unused', 0) / 1024
    mem_used = mem_total - mem_unused

    # ë¸”ë¡ ë””ìŠ¤í¬ í†µê³„
    tree = domain.blockStats('vda')  # ë˜ëŠ” 'sda'
    rd_bytes = tree[1]
    wr_bytes = tree[3]

    # ë„¤íŠ¸ì›Œí¬ í†µê³„
    iface_stats = domain.interfaceStats('vnet0')  # ì¸í„°í˜ì´ìŠ¤ ì´ë¦„
    rx_bytes = iface_stats[0]
    tx_bytes = iface_stats[4]

    return {
        'cpu_time': cpu_time,
        'mem_total': mem_total,
        'mem_used': mem_used,
        'disk_rd_bytes': rd_bytes,
        'disk_wr_bytes': wr_bytes,
        'net_rx_bytes': rx_bytes,
        'net_tx_bytes': tx_bytes
    }

def monitor_vm(vm_name, interval=5):
    """VM ì„±ëŠ¥ ì§€ì† ëª¨ë‹ˆí„°ë§"""
    conn = libvirt.open('qemu:///system')
    if not conn:
        print("Failed to connect to libvirt")
        sys.exit(1)

    prev_stats = None

    try:
        while True:
            stats = get_vm_stats(conn, vm_name)
            if not stats:
                break

            if prev_stats:
                # Delta ê³„ì‚°
                cpu_delta = stats['cpu_time'] - prev_stats['cpu_time']
                disk_rd_delta = (stats['disk_rd_bytes'] - prev_stats['disk_rd_bytes']) / 1024 / 1024  # MB
                disk_wr_delta = (stats['disk_wr_bytes'] - prev_stats['disk_wr_bytes']) / 1024 / 1024
                net_rx_delta = (stats['net_rx_bytes'] - prev_stats['net_rx_bytes']) / 1024 / 1024
                net_tx_delta = (stats['net_tx_bytes'] - prev_stats['net_tx_bytes']) / 1024 / 1024

                print(f"\n{'='*60}")
                print(f"VM: {vm_name} | Interval: {interval}s")
                print(f"{'='*60}")
                print(f"CPU Time:    {cpu_delta:.2f}s ({cpu_delta/interval*100:.1f}% util)")
                print(f"Memory:      {stats['mem_used']:.0f} / {stats['mem_total']:.0f} MB ({stats['mem_used']/stats['mem_total']*100:.1f}%)")
                print(f"Disk Read:   {disk_rd_delta:.2f} MB/s")
                print(f"Disk Write:  {disk_wr_delta:.2f} MB/s")
                print(f"Net RX:      {net_rx_delta:.2f} MB/s")
                print(f"Net TX:      {net_tx_delta:.2f} MB/s")

            prev_stats = stats
            time.sleep(interval)

    except KeyboardInterrupt:
        print("\nMonitoring stopped")
    finally:
        conn.close()

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <vm_name>")
        sys.exit(1)

    monitor_vm(sys.argv[1])
```

**ì‚¬ìš© ì˜ˆ:**
```bash
python3 vm-monitor.py test-vm
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
1. **KVM/QEMU**
   - [KVM Official Site](https://www.linux-kvm.org/)
   - [QEMU Documentation](https://www.qemu.org/documentation/)
   - [KVM Forum 2025](https://kvm-forum.qemu.org/2025/)

2. **Libvirt**
   - [Libvirt Domain XML Format](https://libvirt.org/formatdomain.html)
   - [Libvirt Performance Tuning](https://libvirt.org/kbase/launch_security_sev.html)

3. **Red Hat**
   - [Virtualization Tuning and Optimization Guide](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html-single/virtualization_tuning_and_optimization_guide/)

### 2025 Best Practices
1. [Optimizing KVM Disk Performance with Virtio-scsi and Virtio-blk](https://dohost.us/index.php/2025/09/10/optimizing-kvm-disk-performance-with-virtio-scsi-and-virtio-blk/)
2. [Performance Tuning Your KVM Virtual Machines](https://dohost.us/index.php/2025/09/09/performance-tuning-your-kvm-virtual-machines/)
3. [The KVM Memory Model: NUMA and HugePages Explained](https://dohost.us/index.php/2025/09/09/the-kvm-memory-model-numa-and-hugepages-explained/)
4. [Optimizing KVM Performance: Tips and Tricks for 2025](https://toxigon.com/optimizing-kvm-performance)

### ì»¤ë®¤ë‹ˆí‹°
1. **Proxmox VE Wiki**
   - [Performance Tweaks](https://pve.proxmox.com/wiki/Performance_Tweaks)
   - [Qemu/KVM Virtual Machines](https://pve.proxmox.com/wiki/Qemu/KVM_Virtual_Machines)

---

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ ê°œë…

- [ ] KVMê³¼ QEMUì˜ ì—­í•  ì°¨ì´ ì´í•´
- [ ] Intel VT-x / AMD-V í•˜ë“œì›¨ì–´ ê°€ìƒí™” í™•ì¥ ì´í•´
- [ ] EPT/NPT 2ë‹¨ê³„ í˜ì´ì§€ í…Œì´ë¸” ë™ì‘ ì›ë¦¬
- [ ] Libvirt ì•„í‚¤í…ì²˜ ë° XML êµ¬ì¡°

### CPU ê°€ìƒí™”

- [ ] vCPU ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ ì„¤ì •
- [ ] CPU í”¼ë‹ êµ¬ì„± (vcpupin, emulatorpin)
- [ ] CPU ëª¨ë¸ ì„ íƒ (host-passthrough vs host-model)
- [ ] CPU í† í´ë¡œì§€ ì„¤ì • (sockets, cores, threads)

### ë©”ëª¨ë¦¬ ê°€ìƒí™”

- [ ] Huge Pages ì„¤ì • (2MB, 1GB)
- [ ] Memory Ballooning ë™ì‘ ì´í•´
- [ ] KSM (Kernel Same-page Merging) ì¥ë‹¨ì 
- [ ] ë©”ëª¨ë¦¬ ì ê¸ˆ (locked) ì„¤ì •

### I/O ê°€ìƒí™”

- [ ] virtio-blk vs virtio-scsi ì„ íƒ ê¸°ì¤€
- [ ] ë””ìŠ¤í¬ ìºì‹œ ëª¨ë“œ (none, writethrough, writeback)
- [ ] virtio-net multi-queue ì„¤ì •
- [ ] VFIO/SR-IOV PCI íŒ¨ìŠ¤ìŠ¤ë£¨

### NUMA ìµœì í™”

- [ ] NUMA í† í´ë¡œì§€ í™•ì¸ (numactl, lstopo)
- [ ] vCPUì™€ ë©”ëª¨ë¦¬ì˜ NUMA ì •ë ¬
- [ ] NUMA í†µê³„ ëª¨ë‹ˆí„°ë§ (numastat)
- [ ] NICì™€ vCPUì˜ NUMA ì¼ì¹˜

### ì„±ëŠ¥ íŠœë‹

- [ ] ì¢…í•© íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìš©
- [ ] í˜¸ìŠ¤íŠ¸ ì»¤ë„ íŒŒë¼ë¯¸í„° ìµœì í™”
- [ ] ë²¤ì¹˜ë§ˆí‚¹ (sysbench, fio, iperf3)
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **[Ch3.ìŠ¤ì¼€ì¤„ë§.md](./Ch3.ìŠ¤ì¼€ì¤„ë§.md)**
   - ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ (CFS, Real-time)
   - ë¦¬ì†ŒìŠ¤ í• ë‹¹ ë° QoS
   - CPU/ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ì „ëµ

2. **ì‹¬í™” ì£¼ì œ**
   - **Nested Virtualization**: VM ì•ˆì—ì„œ VM ì‹¤í–‰
   - **Live Migration**: ë¬´ì¤‘ë‹¨ VM ë§ˆì´ê·¸ë ˆì´ì…˜
   - **GPU ê°€ìƒí™”**: vGPU, GPU íŒ¨ìŠ¤ìŠ¤ë£¨

3. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - ê³ ì„±ëŠ¥ DB ì„œë²„ìš© VM ìµœì í™”
   - OpenStack Novaì™€ KVM í†µí•©
   - VM ì„±ëŠ¥ ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-24
**ë‹¤ìŒ ì±•í„°:** [Ch3.ìŠ¤ì¼€ì¤„ë§.md](./Ch3.ìŠ¤ì¼€ì¤„ë§.md)
