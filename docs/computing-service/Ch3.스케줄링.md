# Ch3. ìŠ¤ì¼€ì¤„ë§ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

## ğŸ“‹ ê°œìš”

í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ í™œìš©ê³¼ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ì œê³µì„ ìœ„í•´ì„œëŠ” ì •êµí•œ ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ì „ëµì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë³¸ ì¥ì—ì„œëŠ” Linux ì»¤ë„ì˜ ìŠ¤ì¼€ì¤„ë§ ë©”ì»¤ë‹ˆì¦˜, cgroupsë¥¼ í†µí•œ ë¦¬ì†ŒìŠ¤ ê²©ë¦¬, OpenStack Novaì˜ ìŠ¤ì¼€ì¤„ë§ ì „ëµ, ê·¸ë¦¬ê³  CPU/ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ìµœì í™” ê¸°ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

2025ë…„ í˜„ì¬, Linux ì»¤ë„ì€ **EEVDF (Earliest Eligible Virtual Deadline First)** ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì±„íƒí•˜ì˜€ìœ¼ë©°, OpenShift Virtualizationì€ ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ì„ ìœ„í•œ **WASP (Workload-Aware Swap Provisioner)**ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

1. **Linux ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì´í•´**
   - CFSì—ì„œ EEVDFë¡œì˜ ì „í™˜ (Linux 6.6+)
   - Real-time ìŠ¤ì¼€ì¤„ëŸ¬ (SCHED_FIFO, SCHED_RR)
   - Deadline ìŠ¤ì¼€ì¤„ëŸ¬ (SCHED_DEADLINE)

2. **cgroupsë¥¼ í™œìš©í•œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**
   - CPU ì œì–´ (cpu, cpuset, cpuacct)
   - ë©”ëª¨ë¦¬ ì œì–´ (memory)
   - I/O ì œì–´ (blkio)
   - cgroups v2 ë§ˆì´ê·¸ë ˆì´ì…˜

3. **OpenStack Nova ìŠ¤ì¼€ì¤„ë§**
   - Filter Scheduler ì•„í‚¤í…ì²˜
   - í•„í„°ë§ (Filters) ë° ê°€ì¤‘ì¹˜ (Weighers)
   - Placement APIë¥¼ í†µí•œ ë¦¬ì†ŒìŠ¤ ì¶”ì 
   - Aggregates ë° Availability Zones

4. **ì˜¤ë²„ì»¤ë°‹ ì „ëµ ë° ìµœì í™”**
   - CPU ì˜¤ë²„ì»¤ë°‹ (1:1 ~ 5:1)
   - ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ë° SWAP ê´€ë¦¬
   - ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ ê²°ì • ê¸°ì¤€
   - ì„±ëŠ¥ vs íš¨ìœ¨ì„± íŠ¸ë ˆì´ë“œì˜¤í”„

5. **QoS (Quality of Service)**
   - CPU shares, quota, period
   - Memory limits ë° OOM Killer
   - I/O priorities ë° bandwidth ì œí•œ

6. **ì‹¤ì „ íŠœë‹ ë° ëª¨ë‹ˆí„°ë§**
   - ìŠ¤ì¼€ì¤„ë§ í†µê³„ ë¶„ì„
   - ë³‘ëª© ì§€ì  ì‹ë³„
   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

## Part 1: Linux ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜

### 1.1 EEVDF - ìµœì‹  ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ (Linux 6.6+)

**EEVDFë€?**
2025ë…„ í˜„ì¬, CFSë¥¼ ëŒ€ì²´í•˜ì—¬ Linux 6.6 (2023ë…„ 10ì›”)ë¶€í„° ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì±„íƒëœ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. Earliest Eligible Virtual Deadline Firstì˜ ì•½ìë¡œ, ë” ì •í™•í•œ ì§€ì—° ì‹œê°„ ë³´ì¥ê³¼ ê³µì •ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

**CFS vs EEVDF:**

| íŠ¹ì§• | CFS (Legacy) | EEVDF (Current) |
|------|--------------|-----------------|
| ê¸°ë³¸ ê°œë… | Virtual Runtime ê· í˜• | Virtual Deadline ê¸°ë°˜ |
| ì§€ì—° ì‹œê°„ | ì˜ˆì¸¡ ê°€ëŠ¥í•˜ì§€ë§Œ ë¶€ì •í™• | ë” ì •í™•í•œ ë³´ì¥ |
| ê³µì •ì„± | ì¢‹ìŒ | ë§¤ìš° ì¢‹ìŒ |
| ì˜¤ë²„í—¤ë“œ | ë‚®ìŒ | ì•½ê°„ ë” ë†’ìŒ |
| ì ìš© | Linux < 6.6 | Linux >= 6.6 |

**EEVDF ë™ì‘ ì›ë¦¬:**
```
1. ê° íƒœìŠ¤í¬ì— "ê°€ìƒ ë°ë“œë¼ì¸(virtual deadline)" í• ë‹¹
2. Eligible(ì‹¤í–‰ ê°€ëŠ¥) íƒœìŠ¤í¬ ì¤‘ ë°ë“œë¼ì¸ì´ ê°€ì¥ ì´ë¥¸ ê²ƒ ì„ íƒ
3. ì‹¤í–‰ í›„ ê°€ìƒ ëŸ°íƒ€ì„ ì—…ë°ì´íŠ¸
4. ìƒˆë¡œìš´ ë°ë“œë¼ì¸ ê³„ì‚° ë° ì¬ì‚½ì…
```

### 1.2 ìŠ¤ì¼€ì¤„ë§ í´ë˜ìŠ¤

**Linux ìŠ¤ì¼€ì¤„ë§ í´ë˜ìŠ¤ (ìš°ì„ ìˆœìœ„ ìˆœ):**
```
1. SCHED_DEADLINE (Deadline - ìµœê³  ìš°ì„ ìˆœìœ„)
   â””â”€ ì‹¤ì‹œê°„ ì‘ì—…, ì—„ê²©í•œ ë°ë“œë¼ì¸ ë³´ì¥

2. SCHED_FIFO (Real-time FIFO)
   â””â”€ ì„ ì í˜• ì‹¤ì‹œê°„, ìš°ì„ ìˆœìœ„ ê¸°ë°˜

3. SCHED_RR (Real-time Round-Robin)
   â””â”€ FIFO + Time slice

4. SCHED_OTHER (CFS/EEVDF - ê¸°ë³¸)
   â””â”€ ì¼ë°˜ íƒœìŠ¤í¬, ê³µì •í•œ CPU ì‹œê°„ ë¶„ë°°

5. SCHED_BATCH
   â””â”€ CPU ì§‘ì•½ì  ë°°ì¹˜ ì‘ì—…

6. SCHED_IDLE
   â””â”€ ê°€ì¥ ë‚®ì€ ìš°ì„ ìˆœìœ„
```

**ìŠ¤ì¼€ì¤„ë§ ì •ì±… í™•ì¸ ë° ë³€ê²½:**
```bash
# í”„ë¡œì„¸ìŠ¤ì˜ ìŠ¤ì¼€ì¤„ë§ ì •ì±… í™•ì¸
chrt -p <PID>

# ì˜ˆì‹œ ì¶œë ¥:
# pid 1234's current scheduling policy: SCHED_OTHER
# pid 1234's current scheduling priority: 0

# Real-time FIFOë¡œ ë³€ê²½ (priority 50)
sudo chrt -f -p 50 1234

# Deadline ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
# Runtime: 10ms, Deadline: 30ms, Period: 30ms
sudo chrt -d --sched-runtime 10000000 \
               --sched-deadline 30000000 \
               --sched-period 30000000 \
               <command>
```

### 1.3 Nice ê°’ê³¼ ìš°ì„ ìˆœìœ„

**Nice ê°’ (-20 ~ 19):**
```bash
# Nice ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„
# -20: ìµœê³  ìš°ì„ ìˆœìœ„
#   0: ê¸°ë³¸ê°’
#  19: ìµœì € ìš°ì„ ìˆœìœ„

# Nice ê°’ 10ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
nice -n 10 ./my_app

# ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì˜ nice ë³€ê²½
renice -n 5 -p 1234

# Nice ê°’ í™•ì¸
ps -eo pid,ni,comm | grep my_app
```

**Nice vs Priority ê´€ê³„:**
```
Static Priority (User Space): -20 ~ 19 (nice)
                    â”‚
                    â–¼ (mapping)
Kernel Priority:    0 ~ 139
                    â”œâ”€ 0-99: Real-time (SCHED_FIFO/RR)
                    â””â”€ 100-139: Normal (SCHED_OTHER)
                        â””â”€ 120: default (nice 0)
```

---

## Part 2: cgroups - ë¦¬ì†ŒìŠ¤ ê²©ë¦¬ ë° ì œí•œ

### 2.1 cgroups v1 vs v2

**cgroups (Control Groups):**
Linux ì»¤ë„ ê¸°ëŠ¥ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì„ ì œí•œ, ê²©ë¦¬, ì¸¡ì •í•©ë‹ˆë‹¤.

**cgroups v1 (Legacy):**

- ê° ë¦¬ì†ŒìŠ¤ ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ë…ë¦½ì ì¸ ê³„ì¸µ êµ¬ì¡°
- ë³µì¡í•œ ì„¤ì • ë° ê´€ë¦¬
- CPU, memory, blkio, net_cls ë“± ë¶„ë¦¬

**cgroups v2 (Unified Hierarchy - ê¶Œì¥):**

- ë‹¨ì¼ í†µí•© ê³„ì¸µ êµ¬ì¡°
- ê°„ì†Œí™”ëœ ì¸í„°í˜ì´ìŠ¤
- ë” ë‚˜ì€ ë¦¬ì†ŒìŠ¤ ê²©ë¦¬
- Linux 4.5+ ì§€ì›, ëŒ€ë¶€ë¶„ì˜ ë°°í¬íŒì´ ê¸°ë³¸ìœ¼ë¡œ ì±„íƒ (2025)

**í˜„ì¬ cgroups ë²„ì „ í™•ì¸:**
```bash
# cgroups v2 ë§ˆìš´íŠ¸ í™•ì¸
mount | grep cgroup2

# cgroups v2ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´:
# cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime)

# cgroups v1ì¸ ê²½ìš° ì—¬ëŸ¬ ê°œì˜ cgroup ë§ˆìš´íŠ¸ í‘œì‹œ:
# cgroup on /sys/fs/cgroup/cpu type cgroup (rw,...)
# cgroup on /sys/fs/cgroup/memory type cgroup (rw,...)
```

### 2.2 CPU ì œì–´

**CPU Shares (ìƒëŒ€ì  ê°€ì¤‘ì¹˜):**
```bash
# cgroups v1 ì˜ˆì‹œ
# CPU shares ì„¤ì • (ê¸°ë³¸ê°’: 1024)
mkdir -p /sys/fs/cgroup/cpu/high_priority
mkdir -p /sys/fs/cgroup/cpu/low_priority

echo 2048 > /sys/fs/cgroup/cpu/high_priority/cpu.shares  # 2ë°° ìš°ì„ ìˆœìœ„
echo 512  > /sys/fs/cgroup/cpu/low_priority/cpu.shares   # 0.5ë°° ìš°ì„ ìˆœìœ„

# í”„ë¡œì„¸ìŠ¤ë¥¼ cgroupì— í• ë‹¹
echo <PID> > /sys/fs/cgroup/cpu/high_priority/tasks
```

**CPU Quota (ì ˆëŒ€ì  ì œí•œ):**
```bash
# CPU ì‚¬ìš©ëŸ‰ì„ 50%ë¡œ ì œí•œ
# cpu.cfs_period_us: ê¸°ê°„ (ê¸°ë³¸ 100ms = 100000us)
# cpu.cfs_quota_us: í• ë‹¹ëŸ‰

echo 100000 > /sys/fs/cgroup/cpu/my_app/cpu.cfs_period_us
echo 50000  > /sys/fs/cgroup/cpu/my_app/cpu.cfs_quota_us  # 50% = 50000/100000

# 4 vCPU ì‹œìŠ¤í…œì—ì„œ 2 vCPUë¡œ ì œí•œ (200%)
echo 100000 > /sys/fs/cgroup/cpu/my_app/cpu.cfs_period_us
echo 200000 > /sys/fs/cgroup/cpu/my_app/cpu.cfs_quota_us  # 200%
```

**cpuset (CPU ì¹œí™”ì„±):**
```bash
# CPU 0-3ë²ˆë§Œ ì‚¬ìš©í•˜ë„ë¡ ì œí•œ
mkdir -p /sys/fs/cgroup/cpuset/database
echo "0-3" > /sys/fs/cgroup/cpuset/database/cpuset.cpus
echo "0"   > /sys/fs/cgroup/cpuset/database/cpuset.mems  # NUMA node 0

# í”„ë¡œì„¸ìŠ¤ í• ë‹¹
echo <PID> > /sys/fs/cgroup/cpuset/database/tasks
```

### 2.3 ë©”ëª¨ë¦¬ ì œì–´

**ë©”ëª¨ë¦¬ ì œí•œ:**
```bash
# ë©”ëª¨ë¦¬ë¥¼ 4GBë¡œ ì œí•œ
mkdir -p /sys/fs/cgroup/memory/my_app
echo 4294967296 > /sys/fs/cgroup/memory/my_app/memory.limit_in_bytes  # 4GB

# Swap í¬í•¨ ì´ ë©”ëª¨ë¦¬ ì œí•œ
echo 5368709120 > /sys/fs/cgroup/memory/my_app/memory.memsw.limit_in_bytes  # 5GB

# OOM Killer ë¹„í™œì„±í™” (ì‹ ì¤‘íˆ ì‚¬ìš©)
echo 1 > /sys/fs/cgroup/memory/my_app/memory.oom_control

# í”„ë¡œì„¸ìŠ¤ í• ë‹¹
echo <PID> > /sys/fs/cgroup/memory/my_app/tasks
```

**ë©”ëª¨ë¦¬ í†µê³„:**
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
cat /sys/fs/cgroup/memory/my_app/memory.usage_in_bytes
cat /sys/fs/cgroup/memory/my_app/memory.stat

# ì¶œë ¥ ì˜ˆì‹œ:
# cache 1234567890
# rss 2345678901
# mapped_file 123456789
# ...
```

### 2.4 I/O ì œì–´ (blkio)

**I/O ìš°ì„ ìˆœìœ„ ë° ëŒ€ì—­í­ ì œí•œ:**
```bash
# I/O weight (100-1000, ê¸°ë³¸ 500)
mkdir -p /sys/fs/cgroup/blkio/database
echo 800 > /sys/fs/cgroup/blkio/database/blkio.weight  # ë†’ì€ ìš°ì„ ìˆœìœ„

# íŠ¹ì • ë””ë°”ì´ìŠ¤ì— ëŒ€í•œ ì½ê¸° ëŒ€ì—­í­ ì œí•œ (BPS)
# í˜•ì‹: <major>:<minor> <bytes_per_second>
echo "8:0 10485760" > /sys/fs/cgroup/blkio/my_app/blkio.throttle.read_bps_device  # 10MB/s

# ì“°ê¸° ëŒ€ì—­í­ ì œí•œ
echo "8:0 20971520" > /sys/fs/cgroup/blkio/my_app/blkio.throttle.write_bps_device  # 20MB/s

# IOPS ì œí•œ
echo "8:0 1000" > /sys/fs/cgroup/blkio/my_app/blkio.throttle.read_iops_device  # 1000 IOPS
```

### 2.5 systemdì™€ cgroups í†µí•©

**systemdëŠ” cgroupsì˜ ì£¼ìš” ê´€ë¦¬ì:**
```bash
# systemd ì„œë¹„ìŠ¤ì˜ ë¦¬ì†ŒìŠ¤ ì œí•œ
cat > /etc/systemd/system/my_app.service <<EOF
[Unit]
Description=My Application

[Service]
ExecStart=/usr/bin/my_app
Restart=always

# CPU ì œí•œ (50%)
CPUQuota=50%

# ë©”ëª¨ë¦¬ ì œí•œ (4GB)
MemoryLimit=4G
MemoryHigh=3.5G  # Soft limit (throttle)

# I/O ê°€ì¤‘ì¹˜
IOWeight=500

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl start my_app

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
systemctl status my_app
```

**systemd-runìœ¼ë¡œ ì„ì‹œ cgroup ìƒì„±:**
```bash
# CPU 50%, Memory 2GB ì œí•œìœ¼ë¡œ ëª…ë ¹ ì‹¤í–‰
systemd-run --scope --unit=test-job \
  --property=CPUQuota=50% \
  --property=MemoryLimit=2G \
  stress-ng --cpu 4 --timeout 60s
```

---

## Part 3: OpenStack Nova ìŠ¤ì¼€ì¤„ë§

### 3.1 Nova Scheduler ì•„í‚¤í…ì²˜

**ìŠ¤ì¼€ì¤„ë§ í”Œë¡œìš°:**
```
1. API Request
   â””â”€> nova-api receives "boot instance" request

2. Conductor
   â””â”€> nova-conductor orchestrates the build

3. Scheduler (Filter + Weigh)
   â””â”€> nova-scheduler selects target host
       â”‚
       â”œâ”€ Step 1: Get available hosts from Placement
       â”œâ”€ Step 2: Apply Filters (í•„í„°ë§)
       â”‚   â””â”€> Eliminate unsuitable hosts
       â”œâ”€ Step 3: Apply Weighers (ê°€ì¤‘ì¹˜ ê³„ì‚°)
       â”‚   â””â”€> Rank remaining hosts
       â””â”€ Step 4: Select best host(s)

4. Compute
   â””â”€> nova-compute@selected_host provisions instance
```

### 3.2 Filters (í•„í„°ë§)

**ì£¼ìš” í•„í„° ëª©ë¡ (2025.1):**

| Filter | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ |
|--------|------|----------|
| **AvailabilityZoneFilter** | íŠ¹ì • AZì˜ í˜¸ìŠ¤íŠ¸ë§Œ ì„ íƒ | ì§€ì—­ ë¶„ì‚° |
| **ComputeFilter** | í™œì„± ìƒíƒœì˜ ì»´í“¨íŠ¸ ë…¸ë“œë§Œ | ê¸°ë³¸ |
| **ComputeCapabilitiesFilter** | CPU, RAM, Disk ìš©ëŸ‰ í™•ì¸ | ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ |
| **ImagePropertiesFilter** | ì´ë¯¸ì§€ ì†ì„±ê³¼ í˜¸ìŠ¤íŠ¸ capabilities ë§¤ì¹­ | CPU ê¸°ëŠ¥, Hypervisor |
| **ServerGroupAntiAffinityFilter** | ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„œë¡œ ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì— ë°°ì¹˜ | HA |
| **ServerGroupAffinityFilter** | ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°™ì€ í˜¸ìŠ¤íŠ¸ì— ë°°ì¹˜ | ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìµœì†Œí™” |
| **AggregateInstanceExtraSpecsFilter** | Aggregate ë©”íƒ€ë°ì´í„° ë§¤ì¹­ | SSD-only, GPU ë…¸ë“œ |
| **NUMATopologyFilter** | NUMA í† í´ë¡œì§€ ì •ë ¬ | ì„±ëŠ¥ ìµœì í™” |
| **PciPassthroughFilter** | PCI ì¥ì¹˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ | GPU, SR-IOV |

**ì„¤ì • (/etc/nova/nova.conf):**
```ini
[filter_scheduler]
enabled_filters = AvailabilityZoneFilter,
                  ComputeFilter,
                  ComputeCapabilitiesFilter,
                  ImagePropertiesFilter,
                  ServerGroupAntiAffinityFilter,
                  ServerGroupAffinityFilter,
                  NUMATopologyFilter,
                  AggregateInstanceExtraSpecsFilter

# ì‚¬ìš© ê°€ëŠ¥í•œ í˜¸ìŠ¤íŠ¸ê°€ ì´ ê°œìˆ˜ ë¯¸ë§Œì´ë©´ shuffle
shuffle_best_same_weighed_hosts = True
```

### 3.3 Weighers (ê°€ì¤‘ì¹˜)

**ê°€ì¤‘ì¹˜ ê³„ì‚° ê³µì‹:**
```
Weight = w1_multiplier Ã— norm(w1) + w2_multiplier Ã— norm(w2) + ...

norm(x) = (x - min) / (max - min)  # 0~1 ì •ê·œí™”
```

**ì£¼ìš” Weigher (2025.1):**

| Weigher | ì„¤ëª… | multiplier | íš¨ê³¼ |
|---------|------|------------|------|
| **RAMWeigher** | ê°€ìš© RAM ê¸°ë°˜ | ram_weight_multiplier=1.0 | ì–‘ìˆ˜: RAM ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ (spread)<br>ìŒìˆ˜: RAM ì ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ (pack) |
| **DiskWeigher** | ê°€ìš© Disk ê¸°ë°˜ | disk_weight_multiplier=1.0 | ì–‘ìˆ˜: Disk ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ |
| **CPUWeigher** | ê°€ìš© CPU ê¸°ë°˜ | cpu_weight_multiplier=1.0 | ì–‘ìˆ˜: CPU ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ |
| **IOOpsWeigher** | I/O operations ê¸°ë°˜ | io_ops_weight_multiplier=-1.0 | ìŒìˆ˜: I/O ì ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ |
| **PCIWeigher** | PCI ì¥ì¹˜ ìˆ˜ ê¸°ë°˜ | pci_weight_multiplier=1.0 | PCI ì¥ì¹˜ ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ |
| **ImagePropertiesWeigher** (New!) | ì´ë¯¸ì§€ ì†ì„± ì¼ì¹˜ë„ | image_props_weight_multiplier=0.0 | ê°™ì€ ì´ë¯¸ì§€ ì‚¬ìš© í˜¸ìŠ¤íŠ¸ ì„ í˜¸ |

**ì„¤ì • ì˜ˆì‹œ:**
```ini
[filter_scheduler]
# RAM ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ (Spread ì „ëµ)
ram_weight_multiplier = 1.0

# Disk ë§ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸
disk_weight_multiplier = 1.0

# I/O operations ì ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸
io_ops_weight_multiplier = -1.0

# CPU ì ì€ í˜¸ìŠ¤íŠ¸ ì„ í˜¸ (Pack ì „ëµ - ë¹„ìš© ì ˆê°)
cpu_weight_multiplier = -1.0
```

### 3.4 Host Aggregates ë° Availability Zones

**Host Aggregates:**
í˜¸ìŠ¤íŠ¸ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ë¶€ì—¬í•˜ì—¬ ìŠ¤ì¼€ì¤„ë§ì— í™œìš©í•©ë‹ˆë‹¤.

```bash
# Aggregate ìƒì„± (SSD ë…¸ë“œ)
openstack aggregate create ssd-nodes

# í˜¸ìŠ¤íŠ¸ ì¶”ê°€
openstack aggregate add host ssd-nodes compute01
openstack aggregate add host ssd-nodes compute02

# ë©”íƒ€ë°ì´í„° ì„¤ì •
openstack aggregate set --property ssd=true ssd-nodes

# Flavorì— ìš”êµ¬ì‚¬í•­ ì¶”ê°€
openstack flavor create --ram 8192 --disk 100 --vcpus 4 ssd-flavor
openstack flavor set ssd-flavor --property aggregate_instance_extra_specs:ssd=true

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ssd-nodesì—ë§Œ ìŠ¤ì¼€ì¤„ë§ë¨
openstack server create --flavor ssd-flavor --image ubuntu-22.04 my-vm
```

**Availability Zones:**
```bash
# AZ ìƒì„± (Aggregate + AZ name)
openstack aggregate create --zone az-korea az-korea-aggregate
openstack aggregate add host az-korea-aggregate compute-kr-01

# AZ ì§€ì •í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
openstack server create --flavor m1.small --image ubuntu --availability-zone az-korea my-vm
```

---

## Part 4: ì˜¤ë²„ì»¤ë°‹ ì „ëµ

### 4.1 CPU ì˜¤ë²„ì»¤ë°‹

**CPU ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨:**
```
vCPU ì˜¤ë²„ì»¤ë°‹ = (í• ë‹¹ëœ vCPU ì´í•©) / (ë¬¼ë¦¬ CPU ì½”ì–´ ìˆ˜)

ì•ˆì „í•œ ë²”ìœ„:
- í”„ë¡œë•ì…˜: 1:1 ~ 2:1
- ì¼ë°˜: 3:1 ~ 4:1
- VDI: 5:1 ~ 8:1 (ì‚¬ìš©ìê°€ ë™ì‹œì— ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
```

**2025 Best Practice (RedHat):**
> CPU ì˜¤ë²„ì»¤ë°‹ì€ í”„ë¡œì„¸ìŠ¤ê°€ ì¤‘ë‹¨(killed)ë˜ê±°ë‚˜ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ì˜ ìœ„í—˜ì´ ì—†ìŠµë‹ˆë‹¤. CPUê°€ ì˜¤ë²„ì»¤ë°‹ë˜ë©´ ì›Œí¬ë¡œë“œê°€ ëŠë ¤ì§ˆ ë¿ì…ë‹ˆë‹¤.
>
> ê° ê²ŒìŠ¤íŠ¸ê°€ ë‹¨ì¼ vCPUë§Œ ê°€ì§ˆ ë•Œ ê°€ìƒí™”ëœ CPUì˜ ì˜¤ë²„ì»¤ë°‹ì´ ê°€ì¥ ì˜ ì‘ë™í•˜ë©°, Linux ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ ìœ í˜•ì˜ ë¶€í•˜ì— ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤. **KVMì€ ë¶€í•˜ê°€ 100% ë¯¸ë§Œì¸ ê²ŒìŠ¤íŠ¸ë¥¼ 5:1 ë¹„ìœ¨ë¡œ ì•ˆì „í•˜ê²Œ ì§€ì›**í•´ì•¼ í•©ë‹ˆë‹¤.

**Nova ì„¤ì •:**
```ini
# /etc/nova/nova.conf
[DEFAULT]
# CPU ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ (ê¸°ë³¸ 16.0)
cpu_allocation_ratio = 4.0

# ì˜ˆì‹œ:
# ë¬¼ë¦¬ CPU: 32 cores
# cpu_allocation_ratio = 4.0
# ìµœëŒ€ í• ë‹¹ ê°€ëŠ¥ vCPU = 32 Ã— 4.0 = 128 vCPUs
```

### 4.2 ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹

**ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ì˜ ìœ„í—˜:**

- OOM (Out Of Memory) Killer ë°œë™ â†’ í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ
- SWAP ì‚¬ìš© â†’ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ (ë””ìŠ¤í¬ I/O)

**2025 OpenShift Virtualization ê¶Œì¥ì‚¬í•­:**
> OpenShift Virtualizationì€ **ìµœëŒ€ 25% ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹**ì„ ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œì—ì„œ ë§¤ìš° ì˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.

**ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ê³µì‹:**
```
Memory Overcommit % = (Total VM Memory - Reserved Memory) / Total VM Memory Ã— 100

ì˜ˆì‹œ:
- VM Memory: 9 GB
- Reserved (Request): 6 GB
- Overcommit: (9 - 6) / 9 Ã— 100 = 33%
```

**Nova ì„¤ì •:**
```ini
# /etc/nova/nova.conf
[DEFAULT]
# ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ (ê¸°ë³¸ 1.5)
ram_allocation_ratio = 1.2  # 20% ì˜¤ë²„ì»¤ë°‹ (ë³´ìˆ˜ì )

# ì˜ˆì‹œ:
# ë¬¼ë¦¬ RAM: 128 GB
# ram_allocation_ratio = 1.2
# ìµœëŒ€ í• ë‹¹ ê°€ëŠ¥ RAM = 128 Ã— 1.2 = 153.6 GB
```

### 4.3 SWAP ë° WASP (2025 ì‹ ê¸°ëŠ¥)

**WASP (Workload-Aware Swap Provisioner):**
OpenShift Virtualization 2025ì—ì„œ ë„ì…ëœ ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ì§€ì› ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

**WASP ë™ì‘:**
```
1. Worker ë…¸ë“œì— SWAP ë¦¬ì†ŒìŠ¤ í• ë‹¹
2. VMì— ì˜¤ë²„ì»¤ë°‹ëœ ë©”ëª¨ë¦¬ ì œê³µ
3. SWAP I/O íŠ¸ë˜í”½ ëª¨ë‹ˆí„°ë§
4. ë†’ì€ SWAP ì‚¬ìš© ì‹œ Pod eviction ê´€ë¦¬
```

**SWAP ì„¤ì • (KVM í˜¸ìŠ¤íŠ¸):**
```bash
# SWAP íŒŒì¼ ìƒì„± (16GB)
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# /etc/fstabì— ì¶”ê°€
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# SWAP í™•ì¸
free -h
swapon --show

# Swappiness ì¡°ì • (0-100, ë‚®ì„ìˆ˜ë¡ SWAP ì‚¬ìš© ì¤„ì„)
sudo sysctl vm.swappiness=10
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
```

### 4.4 ì˜¤ë²„ì»¤ë°‹ ëª¨ë‹ˆí„°ë§

**ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í™•ì¸:**
```bash
# Nova ì „ì²´ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
openstack hypervisor stats show

# ì¶œë ¥:
# +----------------------+-------+
# | Field                | Value |
# +----------------------+-------+
# | count                | 10    |
# | current_workload     | 42    |
# | disk_available_least | 5000  |
# | free_disk_gb         | 8000  |
# | free_ram_mb          | 102400|
# | local_gb             | 10000 |
# | local_gb_used        | 2000  |
# | memory_mb            | 204800|
# | memory_mb_used       | 102400|
# | running_vms          | 156   |
# | vcpus                | 320   |
# | vcpus_used           | 624   | # 1.95:1 ì˜¤ë²„ì»¤ë°‹
# +----------------------+-------+

# ê°œë³„ Hypervisor
openstack hypervisor show compute01 -f yaml
```

---

## Part 5: QoS (Quality of Service)

### 5.1 CPU QoS

**CPU Shares (ìƒëŒ€ì ):**
```bash
# Systemd ì„œë¹„ìŠ¤
# High priority: 2048 shares
# Normal: 1024 shares (ê¸°ë³¸)
# Low: 512 shares

cat > /etc/systemd/system/database.service <<EOF
[Service]
CPUShares=2048
EOF

systemctl daemon-reload
systemctl restart database
```

**CPU Quota (ì ˆëŒ€ì ):**
```bash
# 50% CPU ì œí•œ
cat > /etc/systemd/system/batch_job.service <<EOF
[Service]
CPUQuota=50%
EOF
```

### 5.2 Memory QoS

**Memory Limits:**
```bash
# Soft limit (MemoryHigh): Throttle but don't kill
# Hard limit (MemoryMax/MemoryLimit): Kill if exceeded

cat > /etc/systemd/system/web_server.service <<EOF
[Service]
MemoryHigh=3G    # Soft limit - ì´ˆê³¼ ì‹œ throttle
MemoryMax=4G     # Hard limit - ì´ˆê³¼ ì‹œ OOM kill
EOF
```

**OOM Score ì¡°ì •:**
```bash
# OOM Killer ìš°ì„ ìˆœìœ„ ì¡°ì • (-1000 ~ 1000)
# ê°’ì´ ë†’ì„ìˆ˜ë¡ ë¨¼ì € ì¢…ë£Œë¨

# ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚®ì€ ê°’
echo -500 > /proc/<PID>/oom_score_adj

# ëœ ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ë†’ì€ ê°’
echo 500 > /proc/<PID>/oom_score_adj
```

### 5.3 I/O QoS

**ionice (I/O ìš°ì„ ìˆœìœ„):**
```bash
# I/O í´ë˜ìŠ¤:
# 0: None (ê¸°ë³¸)
# 1: Real-time (ìµœê³  ìš°ì„ ìˆœìœ„)
# 2: Best-effort (ê¸°ë³¸)
# 3: Idle (ê°€ì¥ ë‚®ìŒ)

# Real-time í´ë˜ìŠ¤, ìš°ì„ ìˆœìœ„ 0 (ê°€ì¥ ë†’ìŒ)
ionice -c 1 -n 0 -p <PID>

# Best-effort í´ë˜ìŠ¤, ìš°ì„ ìˆœìœ„ 7 (ê°€ì¥ ë‚®ìŒ)
ionice -c 2 -n 7 ./batch_job

# Idle í´ë˜ìŠ¤
ionice -c 3 ./background_task
```

**blkio cgroupì„ í†µí•œ I/O ì œí•œ:**
```bash
# ì½ê¸° ëŒ€ì—­í­ 10MB/s ì œí•œ
echo "8:0 10485760" > /sys/fs/cgroup/blkio/my_app/blkio.throttle.read_bps_device

# ì“°ê¸° IOPS 1000 ì œí•œ
echo "8:0 1000" > /sys/fs/cgroup/blkio/my_app/blkio.throttle.write_iops_device
```

---

## Part 6: ìŠ¤ì¼€ì¤„ë§ ëª¨ë‹ˆí„°ë§ ë° íŠœë‹

### 6.1 ìŠ¤ì¼€ì¤„ë§ í†µê³„

**CPU ìŠ¤ì¼€ì¤„ë§ í†µê³„:**
```bash
# í”„ë¡œì„¸ìŠ¤ë³„ CPU ì‚¬ìš© ì‹œê°„
cat /proc/<PID>/schedstat

# ì¶œë ¥: <runtime_ns> <wait_time_ns> <nr_switches>
# 987654321 123456789 1234

# ì‹œìŠ¤í…œ ì „ì²´ ìŠ¤ì¼€ì¤„ë§ í†µê³„
cat /proc/schedstat

# CPUë³„ ëŸ°í í†µê³„
cat /proc/sched_debug
```

**Context Switch í™•ì¸:**
```bash
# ì´ˆë‹¹ context switch ìˆ˜
vmstat 1

# ì¶œë ¥:
# procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
#  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
#  2  0      0 123456   1234  56789    0    0     1     2  345 6789 10 5 85  0  0
#                                                          cs = context switches

# ë†’ì€ cs ê°’ (>10000)ì€ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë²„í—¤ë“œë¥¼ ì˜ë¯¸
```

### 6.2 ëŸ°í ë¶„ì„

```bash
# í˜„ì¬ ì‹¤í–‰ ëŒ€ê¸° ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ìˆ˜
cat /proc/loadavg

# ì¶œë ¥: 1.23 2.34 3.45 2/567 12345
#       1ë¶„  5ë¶„  15ë¶„ í‰ê·  load
#       ì‹¤í–‰ì¤‘/ì „ì²´ í”„ë¡œì„¸ìŠ¤
#       ë§ˆì§€ë§‰ PID

# Load average í•´ì„:
# Load < CPU cores: ì—¬ìœ 
# Load = CPU cores: í¬í™”
# Load > CPU cores: ê³¼ë¶€í•˜
```

**ëŸ°í ì‹œê°í™” (mpstat):**
```bash
# CPUë³„ ì‚¬ìš©ë¥ 
mpstat -P ALL 1

# ì¶œë ¥ í•´ì„:
# %idleì´ 0ì— ê°€ê¹Œìš°ë©´ CPU í¬í™”
# %iowaitì´ ë†’ìœ¼ë©´ I/O ë³‘ëª©
# %stealì´ ë†’ìœ¼ë©´ Hypervisorê°€ CPUë¥¼ ë¹¼ì•—ì•„ê° (ì˜¤ë²„ì»¤ë°‹)
```

### 6.3 ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

**perfë¥¼ í†µí•œ ìŠ¤ì¼€ì¤„ë§ ë¶„ì„:**
```bash
# ìŠ¤ì¼€ì¤„ë§ ì´ë²¤íŠ¸ ì¶”ì 
sudo perf sched record -- sleep 10

# ë¶„ì„
sudo perf sched latency

# ì¶œë ¥:
# Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms |
# migration/0           |      0.123 ms |        5 |    avg: 0.010 ms | max: 0.050 ms |
# ksoftirqd/0           |      1.234 ms |       12 |    avg: 0.020 ms | max: 0.100 ms |
# ...

# ìŠ¤ì¼€ì¤„ë§ ë§µ ì‹œê°í™”
sudo perf sched map
```

**Flame Graph:**
```bash
# CPU í”„ë¡œíŒŒì¼ë§ (60ì´ˆ)
sudo perf record -F 99 -a -g -- sleep 60

# Flame graph ìƒì„±
git clone https://github.com/brendangregg/FlameGraph
sudo perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > flamegraph.svg

# ë¸Œë¼ìš°ì €ì—ì„œ flamegraph.svg ì—´ê¸°
```

---

## ğŸ› ï¸ ì‹¤ìŠµ ê°€ì´ë“œ

### ì‹¤ìŠµ 1: cgroupsë¥¼ í™œìš©í•œ CPU ì œí•œ

**ì‹œë‚˜ë¦¬ì˜¤:** CPU ì§‘ì•½ì  ì‘ì—… 2ê°œë¥¼ ê²©ë¦¬í•˜ê³  ë¦¬ì†ŒìŠ¤ ë¶„ë°°

```bash
# 1. cgroup ìƒì„±
sudo mkdir -p /sys/fs/cgroup/cpu/high_priority
sudo mkdir -p /sys/fs/cgroup/cpu/low_priority

# 2. CPU shares ì„¤ì •
echo 2048 > /sys/fs/cgroup/cpu/high_priority/cpu.shares  # 2x
echo 512  > /sys/fs/cgroup/cpu/low_priority/cpu.shares   # 0.5x

# 3. CPU stress í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰
# High priority
stress-ng --cpu 4 --timeout 60s &
HIGH_PID=$!
echo $HIGH_PID > /sys/fs/cgroup/cpu/high_priority/tasks

# Low priority
stress-ng --cpu 4 --timeout 60s &
LOW_PID=$!
echo $LOW_PID > /sys/fs/cgroup/cpu/low_priority/tasks

# 4. CPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
top -p $HIGH_PID,$LOW_PID

# ê²°ê³¼: high_priorityê°€ low_priorityë³´ë‹¤ ì•½ 4ë°° ë§ì€ CPU ì‹œê°„ í• ë‹¹
# (2048 / 512 = 4)
```

### ì‹¤ìŠµ 2: ë©”ëª¨ë¦¬ ì œí•œ ë° OOM í…ŒìŠ¤íŠ¸

```bash
# 1. cgroup ìƒì„± ë° ë©”ëª¨ë¦¬ ì œí•œ (1GB)
sudo mkdir -p /sys/fs/cgroup/memory/limited
echo 1073741824 > /sys/fs/cgroup/memory/limited/memory.limit_in_bytes

# 2. OOM notification ì„¤ì •
echo 1 > /sys/fs/cgroup/memory/limited/memory.oom_control

# 3. ë©”ëª¨ë¦¬ í• ë‹¹ í”„ë¡œê·¸ë¨ ì‹¤í–‰ (2GB í• ë‹¹ ì‹œë„)
cat > mem_hog.py <<'EOF'
import time
data = []
for i in range(20):  # 20 * 100MB = 2GB
    data.append(' ' * (100 * 1024 * 1024))
    print(f"Allocated {(i+1) * 100} MB")
    time.sleep(1)
EOF

python3 mem_hog.py &
PID=$!
echo $PID > /sys/fs/cgroup/memory/limited/tasks

# 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 "cat /sys/fs/cgroup/memory/limited/memory.usage_in_bytes && \
            cat /proc/$PID/status | grep VmRSS"

# ê²°ê³¼: 1GB ë„ë‹¬ ì‹œ OOM Killer ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
```

### ì‹¤ìŠµ 3: Nova Scheduler ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# 1. Custom Aggregate ìƒì„± (GPU ë…¸ë“œ)
openstack aggregate create gpu-nodes --zone gpu-zone
openstack aggregate add host gpu-nodes compute-gpu-01
openstack aggregate set --property gpu=true gpu-nodes
openstack aggregate set --property gpu_type=nvidia-a100 gpu-nodes

# 2. GPU Flavor ìƒì„±
openstack flavor create --ram 32768 --disk 200 --vcpus 16 gpu.large
openstack flavor set gpu.large \
  --property aggregate_instance_extra_specs:gpu=true \
  --property aggregate_instance_extra_specs:gpu_type=nvidia-a100 \
  --property "pci_passthrough:alias"="a100:1"

# 3. Anti-affinity Server Group ìƒì„± (HA)
openstack server group create --policy anti-affinity web-servers-ha

# 4. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Server Group ì‚¬ìš©)
for i in {1..3}; do
  openstack server create \
    --flavor m1.large \
    --image ubuntu-22.04 \
    --hint group=$(openstack server group show web-servers-ha -f value -c id) \
    web-server-$i
done

# ê²°ê³¼: 3ê°œ ì¸ìŠ¤í„´ìŠ¤ê°€ ì„œë¡œ ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì— ë°°ì¹˜ë¨

# 5. Scheduler í†µê³„ í™•ì¸
openstack hypervisor stats show
```

### ì‹¤ìŠµ 4: CPU ì˜¤ë²„ì»¤ë°‹ ì˜í–¥ ì¸¡ì •

```bash
# 1. ì˜¤ë²„ì»¤ë°‹ ì—†ìŒ (1:1)
# 4 vCPU VM 1ê°œ ìƒì„± (ë¬¼ë¦¬ CPU 4 cores)
openstack server create --flavor m1.xlarge --image ubuntu cpu-test-1

# VM ë‚´ë¶€ì—ì„œ CPU ë²¤ì¹˜ë§ˆí¬
ssh ubuntu@<VM_IP>
sysbench cpu --cpu-max-prime=20000 --threads=4 run

# ê²°ê³¼ ê¸°ë¡ (Baseline)

# 2. 2:1 ì˜¤ë²„ì»¤ë°‹
# 4 vCPU VM 2ê°œ ìƒì„± (ë¬¼ë¦¬ CPU 4 cores, ì´ 8 vCPUs)
openstack server create --flavor m1.xlarge --image ubuntu cpu-test-2

# ë‘ VMì—ì„œ ë™ì‹œì— ë²¤ì¹˜ë§ˆí¬
# ê²°ê³¼: ê° VMì´ ì•½ 50% ì„±ëŠ¥

# 3. 4:1 ì˜¤ë²„ì»¤ë°‹
# 4 vCPU VM 4ê°œ ìƒì„±
# ê²°ê³¼: ê° VMì´ ì•½ 25% ì„±ëŠ¥

# ì„±ëŠ¥ ì €í•˜ìœ¨ = (Baseline - Current) / Baseline Ã— 100
```

---

## ğŸ’» ì˜ˆì œ ì½”ë“œ

### ì˜ˆì œ 1: cgroup ê´€ë¦¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# cgroup-manager.sh

set -e

CGROUP_NAME=$1
CPU_SHARES=$2
MEMORY_LIMIT_GB=$3
COMMAND=$4

if [ $# -ne 4 ]; then
  echo "Usage: $0 <cgroup_name> <cpu_shares> <memory_limit_gb> <command>"
  exit 1
fi

# cgroup ìƒì„±
CPU_CGROUP="/sys/fs/cgroup/cpu/${CGROUP_NAME}"
MEMORY_CGROUP="/sys/fs/cgroup/memory/${CGROUP_NAME}"

sudo mkdir -p "$CPU_CGROUP"
sudo mkdir -p "$MEMORY_CGROUP"

# CPU ì„¤ì •
echo "$CPU_SHARES" | sudo tee "$CPU_CGROUP/cpu.shares"

# ë©”ëª¨ë¦¬ ì„¤ì • (GB â†’ Bytes)
MEMORY_BYTES=$((MEMORY_LIMIT_GB * 1024 * 1024 * 1024))
echo "$MEMORY_BYTES" | sudo tee "$MEMORY_CGROUP/memory.limit_in_bytes"

# ëª…ë ¹ ì‹¤í–‰
echo "Starting: $COMMAND"
echo "CPU Shares: $CPU_SHARES"
echo "Memory Limit: ${MEMORY_LIMIT_GB}GB"

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
eval "$COMMAND" &
PID=$!

# cgroupì— í• ë‹¹
echo "$PID" | sudo tee "$CPU_CGROUP/tasks"
echo "$PID" | sudo tee "$MEMORY_CGROUP/tasks"

echo "Process $PID added to cgroup: $CGROUP_NAME"

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
while kill -0 $PID 2>/dev/null; do
  CPU_USAGE=$(cat "$CPU_CGROUP/cpuacct.usage")
  MEM_USAGE=$(cat "$MEMORY_CGROUP/memory.usage_in_bytes")
  MEM_USAGE_MB=$((MEM_USAGE / 1024 / 1024))

  echo "$(date): CPU=${CPU_USAGE}ns, Memory=${MEM_USAGE_MB}MB"
  sleep 5
done

echo "Process completed"
```

**ì‚¬ìš© ì˜ˆ:**
```bash
# CPU shares 1024, Memory 2GBë¡œ stress í…ŒìŠ¤íŠ¸ ì‹¤í–‰
./cgroup-manager.sh my_test 1024 2 "stress-ng --cpu 4 --vm 2 --vm-bytes 1G --timeout 60s"
```

### ì˜ˆì œ 2: Pythonìœ¼ë¡œ Nova Scheduler ì‹œë®¬ë ˆì´ì…˜

```python
#!/usr/bin/env python3
# nova-scheduler-sim.py

class Host:
    def __init__(self, name, vcpus, ram_mb, disk_gb):
        self.name = name
        self.vcpus = vcpus
        self.vcpus_used = 0
        self.ram_mb = ram_mb
        self.ram_mb_used = 0
        self.disk_gb = disk_gb
        self.disk_gb_used = 0
        self.instances = []

    def available_vcpus(self):
        return self.vcpus - self.vcpus_used

    def available_ram(self):
        return self.ram_mb - self.ram_mb_used

    def available_disk(self):
        return self.disk_gb - self.disk_gb_used

    def can_fit(self, vcpus, ram_mb, disk_gb):
        return (self.available_vcpus() >= vcpus and
                self.available_ram() >= ram_mb and
                self.available_disk() >= disk_gb)

    def allocate(self, instance_name, vcpus, ram_mb, disk_gb):
        if not self.can_fit(vcpus, ram_mb, disk_gb):
            return False

        self.vcpus_used += vcpus
        self.ram_mb_used += ram_mb
        self.disk_gb_used += disk_gb
        self.instances.append(instance_name)
        return True

    def __repr__(self):
        return (f"Host({self.name}: vCPU={self.vcpus_used}/{self.vcpus}, "
                f"RAM={self.ram_mb_used}/{self.ram_mb}MB, "
                f"Disk={self.disk_gb_used}/{self.disk_gb}GB, "
                f"instances={len(self.instances)})")


class NovaScheduler:
    def __init__(self, hosts):
        self.hosts = hosts

    def filter(self, vcpus, ram_mb, disk_gb):
        """Filter: ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” í˜¸ìŠ¤íŠ¸ë§Œ"""
        return [h for h in self.hosts if h.can_fit(vcpus, ram_mb, disk_gb)]

    def weigh(self, hosts, strategy='spread'):
        """Weigh: ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì •ë ¬"""
        if strategy == 'spread':
            # RAM ë§ì€ í˜¸ìŠ¤íŠ¸ ìš°ì„  (spread)
            return sorted(hosts, key=lambda h: h.available_ram(), reverse=True)
        elif strategy == 'pack':
            # RAM ì ì€ í˜¸ìŠ¤íŠ¸ ìš°ì„  (pack)
            return sorted(hosts, key=lambda h: h.available_ram())
        else:
            return hosts

    def schedule(self, instance_name, vcpus, ram_mb, disk_gb, strategy='spread'):
        """ìŠ¤ì¼€ì¤„ë§: Filter â†’ Weigh â†’ Select"""
        # Step 1: Filter
        candidates = self.filter(vcpus, ram_mb, disk_gb)
        if not candidates:
            print(f"âŒ No suitable host for {instance_name}")
            return None

        # Step 2: Weigh
        ranked = self.weigh(candidates, strategy)

        # Step 3: Select best host
        selected = ranked[0]

        # Step 4: Allocate
        if selected.allocate(instance_name, vcpus, ram_mb, disk_gb):
            print(f"âœ… {instance_name} scheduled on {selected.name}")
            return selected
        else:
            print(f"âŒ Failed to allocate {instance_name}")
            return None


# ì‹œë®¬ë ˆì´ì…˜
if __name__ == '__main__':
    # í˜¸ìŠ¤íŠ¸ ìƒì„±
    hosts = [
        Host('compute-01', vcpus=32, ram_mb=131072, disk_gb=1000),
        Host('compute-02', vcpus=32, ram_mb=131072, disk_gb=1000),
        Host('compute-03', vcpus=32, ram_mb=131072, disk_gb=1000),
    ]

    scheduler = NovaScheduler(hosts)

    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ìš”ì²­
    instances = [
        ('web-1', 4, 8192, 50),
        ('web-2', 4, 8192, 50),
        ('db-1', 8, 32768, 200),
        ('db-2', 8, 32768, 200),
        ('cache-1', 2, 4096, 20),
    ]

    print("=== Spread Strategy ===")
    for name, vcpus, ram, disk in instances:
        scheduler.schedule(name, vcpus, ram, disk, strategy='spread')

    print("\n=== Host Status ===")
    for host in hosts:
        print(host)

    # ë¦¬ì…‹ í›„ Pack ì „ëµ í…ŒìŠ¤íŠ¸
    hosts = [
        Host('compute-01', vcpus=32, ram_mb=131072, disk_gb=1000),
        Host('compute-02', vcpus=32, ram_mb=131072, disk_gb=1000),
        Host('compute-03', vcpus=32, ram_mb=131072, disk_gb=1000),
    ]
    scheduler = NovaScheduler(hosts)

    print("\n=== Pack Strategy ===")
    for name, vcpus, ram, disk in instances:
        scheduler.schedule(name, vcpus, ram, disk, strategy='pack')

    print("\n=== Host Status ===")
    for host in hosts:
        print(host)
```

**ì‹¤í–‰ ê²°ê³¼:**
```
=== Spread Strategy ===
âœ… web-1 scheduled on compute-01
âœ… web-2 scheduled on compute-02
âœ… db-1 scheduled on compute-03
âœ… db-2 scheduled on compute-01
âœ… cache-1 scheduled on compute-02

=== Host Status ===
Host(compute-01: vCPU=12/32, RAM=40960/131072MB, Disk=250/1000GB, instances=2)
Host(compute-02: vCPU=6/32, RAM=12288/131072MB, Disk=70/1000GB, instances=2)
Host(compute-03: vCPU=8/32, RAM=32768/131072MB, Disk=200/1000GB, instances=1)

=== Pack Strategy ===
âœ… web-1 scheduled on compute-01
âœ… web-2 scheduled on compute-01
âœ… db-1 scheduled on compute-01
âœ… db-2 scheduled on compute-01
âœ… cache-1 scheduled on compute-01

=== Host Status ===
Host(compute-01: vCPU=26/32, RAM=85504/131072MB, Disk=520/1000GB, instances=5)
Host(compute-02: vCPU=0/32, RAM=0/131072MB, Disk=0/1000GB, instances=0)
Host(compute-03: vCPU=0/32, RAM=0/131072MB, Disk=0/1000GB, instances=0)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
1. **Linux Kernel Scheduler**
   - [CFS Scheduler Documentation](https://docs.kernel.org/scheduler/sched-design-CFS.html)
   - [CFS Bandwidth Control](https://docs.kernel.org/scheduler/sched-bwc.html)
   - [CFS Group Scheduling](https://blogs.oracle.com/linux/post/cfs-group-scheduling)

2. **cgroups**
   - [Red Hat Resource Management Guide](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/)
   - [Control Groups v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html)

3. **OpenStack Nova Scheduler**
   - [Compute Schedulers](https://docs.openstack.org/nova/latest/admin/scheduling.html)
   - [Filter Scheduler](https://docs.openstack.org/nova/rocky/user/filter-scheduler.html)
   - [2025.1 Release Notes](https://docs.openstack.org/releasenotes/nova/2025.1.html)

### 2025 Best Practices
1. [Evaluating Memory Overcommitment in OpenShift Virtualization](https://developers.redhat.com/articles/2025/04/24/evaluating-memory-overcommitment-openshift-virtualization)
2. [Memory Management in OpenShift Virtualization](https://developers.redhat.com/blog/2025/01/31/memory-management-openshift-virtualization)
3. [Boost OpenShift Database VM Density with Memory Overcommit](https://developers.redhat.com/articles/2025/04/28/boost-openshift-database-vm-density-memory-overcommit)
4. [Optimizing Memory Overcommitment and Swap in OpenShift](https://access.redhat.com/articles/7104984)

### ì—°êµ¬ ë…¼ë¬¸
1. [Mitigating Context Switching in Densely Packed Linux Clusters (2025)](https://arxiv.org/html/2508.15703v1)

---

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ ê°œë…
- [ ] CFSì—ì„œ EEVDFë¡œì˜ ì „í™˜ ì´í•´ (Linux 6.6+)
- [ ] ìŠ¤ì¼€ì¤„ë§ í´ë˜ìŠ¤ (SCHED_DEADLINE, FIFO, RR, OTHER)
- [ ] Nice ê°’ê³¼ ì»¤ë„ ìš°ì„ ìˆœìœ„ ë§¤í•‘
- [ ] Context switching ê°œë… ë° ë¹„ìš©

### cgroups
- [ ] cgroups v1 vs v2 ì°¨ì´ì 
- [ ] CPU ì œì–´ (shares, quota, cpuset)
- [ ] ë©”ëª¨ë¦¬ ì œì–´ (limit, OOM control)
- [ ] I/O ì œì–´ (blkio weight, throttle)
- [ ] systemdì™€ cgroups í†µí•©

### OpenStack Nova Scheduler
- [ ] Filter Scheduler ì•„í‚¤í…ì²˜
- [ ] ì£¼ìš” í•„í„° (Availability Zone, NUMA, PCI ë“±)
- [ ] Weigher ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
- [ ] Host Aggregates ë° ë©”íƒ€ë°ì´í„° í™œìš©
- [ ] Placement API ì—­í• 

### ì˜¤ë²„ì»¤ë°‹
- [ ] CPU ì˜¤ë²„ì»¤ë°‹ ë¹„ìœ¨ ê²°ì • (1:1 ~ 5:1)
- [ ] ë©”ëª¨ë¦¬ ì˜¤ë²„ì»¤ë°‹ ìœ„í—˜ì„± ì´í•´
- [ ] SWAP ë° WASP ì„¤ì •
- [ ] ì˜¤ë²„ì»¤ë°‹ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •

### QoS
- [ ] CPU QoS (shares, quota)
- [ ] Memory QoS (limits, OOM score)
- [ ] I/O QoS (ionice, blkio)
- [ ] ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

Computing Serviceì˜ ì ˆë°˜ ì´ìƒì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤ (3/7)! ë‹¤ìŒì€:

1. **[Ch4.Cloud_Native_ì»´í“¨íŒ….md](./Ch4.Cloud_Native_ì»´í“¨íŒ….md)**
   - Kubernetes Operator íŒ¨í„´
   - CRD ë° Custom Controller
   - ì„ ì–¸ì  API ì„¤ê³„

2. **ì‹¬í™” ì£¼ì œ**
   - **BPF (Berkeley Packet Filter)**: ì»¤ë„ ë ˆë²¨ ëª¨ë‹ˆí„°ë§
   - **Latency-Aware Scheduling**: ì§€ì—° ì‹œê°„ ìµœì í™”
   - **NUMA-aware Scheduling**: ë©€í‹°ì†Œì¼“ ìµœì í™”

3. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - Nova Scheduler ì»¤ìŠ¤í…€ í•„í„° ê°œë°œ
   - cgroups ê¸°ë°˜ ë©€í‹°í…Œë„ŒíŠ¸ ë¦¬ì†ŒìŠ¤ ê²©ë¦¬
   - ì˜¤ë²„ì»¤ë°‹ ìë™ ì¡°ì • ì‹œìŠ¤í…œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-24
**ë‹¤ìŒ ì±•í„°:** [Ch4.Cloud_Native_ì»´í“¨íŒ….md](./Ch4.Cloud_Native_ì»´í“¨íŒ….md)
